@article{takahashi2020a,
abstract = {Dataset shifts are present in many real-world applications, since data generation is not always fully controlled and is subject to noise, degradation, and other natural variations. In machine learning, the lack of regularity in data can degrade performance by breaching error constraints. Different methods have been proposed to solve shifting problems; however, shifts in off-line learning mode are not as well examined. Off-line shifts consist of problems where drifts occur only with unlabeled data. Most methods aimed at dataset shifts consider that new labeled data can be received after training, which is not always the case. Here, a review on dataset shift characteristics and causes is presented as a tool for the analysis and implementation of machine learning methods targeting off-line mode dataset shift problems. In this context, a relationship between statistical learning risk functions and error degradation due to variation in data distribution was straightforwardly derived. Moreover, this paper provides a consistent survey of recent popular machine learning methods that address off-line mode dataset shift problems, focusing on the main characteristics of unlabeled data shifts.},
author = {Takahashi, Carla C. and Braga, Antonio P.},
doi = {10.1109/MCI.2020.2998231},
file = {::},
issn = {1556-603X},
journal = {IEEE Computational Intelligence Magazine},
month = {aug},
number = {3},
pages = {16--27},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{A Review of Off-Line Mode Dataset Shifts}},
url = {https://ieeexplore.ieee.org/document/9141463/},
volume = {15},
year = {2020}
}
@article{liu2017a,
author = {Liu, Bing},
doi = {10.1007/s11704-016-6903-6},
file = {:home/andrea/Zotero/storage/JL4BVZBI/Liu - 2017 - Lifelong machine learning a paradigm for continuo.pdf:pdf},
issn = {2095-2236},
journal = {Frontiers of Computer Science},
number = {3},
pages = {359--361},
shorttitle = {Lifelong machine learning},
title = {{Lifelong machine learning: a paradigm for continuous learning}},
url = {https://doi.org/10.1007/s11704-016-6903-6},
volume = {11},
year = {2017}
}
@article{ditzler2015a,
abstract = {The prevalence of mobile phones, the internet-of-things technology, and networks of sensors has led to an enormous and ever increasing amount of data that are now more commonly available in a streaming fashion [1]-[5]. Often, it is assumed - either implicitly or explicitly - that the process generating such a stream of data is stationary, that is, the data are drawn from a fixed, albeit unknown probability distribution. In many real-world scenarios, however, such an assumption is simply not true, and the underlying process generating the data stream is characterized by an intrinsic nonstationary (or evolving or drifting) phenomenon. The nonstationarity can be due, for example, to seasonality or periodicity effects, changes in the users' habits or preferences, hardware or software faults affecting a cyber-physical system, thermal drifts or aging effects in sensors. In such nonstationary environments, where the probabilistic properties of the data change over time, a non-adaptive model trained under the false stationarity assumption is bound to become obsolete in time, and perform sub-optimally at best, or fail catastrophically at worst.},
author = {Ditzler, Gregory and Roveri, Manuel and Alippi, Cesare and Polikar, Robi},
doi = {10.1109/MCI.2015.2471196},
file = {:home/andrea/Zotero/storage/ZMFTIQ2V/Ditzler et al. - 2015 - Learning in Nonstationary Environments A Survey.pdf:pdf},
issn = {1556-603X},
journal = {IEEE Computational Intelligence Magazine},
language = {en},
month = {nov},
number = {4},
pages = {12--25},
shorttitle = {Learning in Nonstationary Environments},
title = {{Learning in Nonstationary Environments: A Survey}},
url = {http://ieeexplore.ieee.org/document/7296710/},
volume = {10},
year = {2015}
}
@article{Mitchell2015a,
abstract = {Whereas people learn many different types of knowledge from diverse experiences over many years, and become better learners over time, most current machine learning systems are much more narrow, learning just a single func-tion or data model based on statistical analysis of a single data set. We suggest that people learn better than comput-ers precisely because of this difference, and we suggest a key direction for machine learning research is to develop software architectures that enable intelligent agents to also learn many types of knowledge, continuously over many years, and to become better learners over time. In this paper we defi ne more precisely this never-ending learning paradigm for machine learning, and we present one case study: the Never-Ending Language Learner (NELL), which achieves a number of the desired properties of a never-end-ing learner. NELL has been learning to read the Web 24hrs/ day since January 2010, and so far has acquired a knowledge base with 120mn diverse, confi dence-weighted beliefs (e.g., servedWith(tea,biscuits)), while learning thousands of interrelated functions that continually improve its reading competence over time. NELL has also learned to reason over its knowledge base to infer new beliefs it has not yet read from those it has, and NELL is inventing new relational pred-icates to extend the ontology it uses to represent beliefs. We describe the design of NELL, experimental results illustrat-ing its behavior, and discuss both its successes and short-comings as a case study in never-ending learning. NELL can be tracked online at http://rtw.ml.cmu.edu, and followed on Twitter at @CMUNELL. 1. INTRODUCTION Machine learning is a highly successful branch of Artifi cial Intelligence (AI), and is now widely used for tasks from spam fi ltering, to speech recognition, to credit card fraud detec-tion, to face recognition. Despite these successes, the ways in which computers learn today remain surprisingly narrow when compared to human learning. This paper explores an alternative paradigm for machine learning that more closely models the diversity, competence and cumulative nature of human learning. We call this alternative paradigm never-ending learning. To illustrate, note that in each of the above machine learning applications, the computer learns only a single function to perform a single task in isolation, usually from human labeled training examples of inputs and outputs of that function. In spam fi ltering, for instance, training examples consist of specifi c emails and spam or not-spam labels for each. This style of learning is often called super-vised function approximation, because the abstract learning problem is to approximate some unknown function f : X → Y},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Mitchell, Tom and Cohen, William W and Hruschka, E and Talukdar, Partha P and Yang, B and Betteridge, Justin and Carlson, Andrew and Dalvi, B and Gardner, Matt and Kisiel, Bryan and Krishnamurthy, J and Lao, Ni and Mazaitis, K and Mohamed, T and Nakashole, N and Platanios, E and Ritter, A and Samadi, M and Settles, B and Wang, R and Wijaya, D and Gupta, A and Chen, X and Saparov, A and Greaves, M and Welling, J},
doi = {10.1145/3191513},
eprint = {arXiv:1011.1669v3},
isbn = {9781577357018},
issn = {0001-0782},
journal = {Communications of the Acm},
keywords = {NLP and Machine Learning Track},
number = {1},
pages = {2302--2310},
pmid = {15003161},
title = {{Never-Ending Learning}},
url = {https://dl.acm.org/doi/10.1145/3191513},
volume = {61},
year = {2015}
}
@article{Losing2018a,
abstract = {Recently, incremental and on-line learning gained more attention especially in the context of big data and learning from data streams, conflicting with the traditional assumption of complete data availability. Even though a variety of different methods are available, it often remains unclear which of them is suitable for a specific task and how they perform in comparison to each other. We analyze the key properties of eight popular incremental methods representing different algorithm classes. Thereby, we evaluate them with regards to their on-line classification error as well as to their behavior in the limit. Further, we discuss the often neglected issue of hyperparameter optimization specifically for each method and test how robustly it can be done based on a small set of examples. Our extensive evaluation on data sets with different characteristics gives an overview of the performance with respect to accuracy, convergence speed as well as model complexity, facilitating the choice of the best method for a given application.},
author = {Losing, Viktor and Hammer, Barbara and Wersing, Heiko},
doi = {10.1016/j.neucom.2017.06.084},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Data streams,Hyperparameter optimization,Incremental learning,Model selection,On-line learning},
pages = {1261--1274},
publisher = {Elsevier B.V.},
title = {{Incremental on-line learning: A review and comparison of state of the art algorithms}},
url = {https://doi.org/10.1016/j.neucom.2017.06.084},
volume = {275},
year = {2018}
}
@article{Lesort2019a,
abstract = {Which generative model is the most suitable for Continual Learning? This paper aims at evaluating and comparing generative models on disjoint sequential image generation tasks. We investigate how several models learn and forget, considering various strategies: rehearsal, regularization, generative replay and fine-tuning. We used two quantitative metrics to estimate the generation quality and memory ability. We experiment with sequential tasks on three commonly used benchmarks for Continual Learning (MNIST, Fashion MNIST and CIFAR10). We found that among all models, the original GAN performs best and among Continual Learning strategies, generative replay outperforms all other methods. Even if we found satisfactory combinations on MNIST and Fashion MNIST, training generative models sequentially on CIFAR10 is particularly instable, and remains a challenge. Our code is available online $\backslash$footnote{\{}$\backslash$url{\{}https://github.com/TLESORT/Generative$\backslash${\_}Continual$\backslash${\_}Learning{\}}{\}}.},
archivePrefix = {arXiv},
arxivId = {1812.09111},
author = {Lesort, Timoth{\'{e}}e and Caselles-Dupr{\'{e}}, Hugo and Garcia-Ortiz, Michael and Stoian, Andrei and Filliat, David},
doi = {10.1109/IJCNN.2019.8851986},
eprint = {1812.09111},
isbn = {9781728119854},
journal = {Proceedings of the International Joint Conference on Neural Networks},
keywords = {[cifar],[generative],[mnist]},
mendeley-tags = {[cifar],[generative],[mnist]},
month = {dec},
title = {{Generative Models from the perspective of Continual Learning}},
url = {http://arxiv.org/abs/1812.09111},
year = {2018}
}
@article{Hasselmo2017,
abstract = {Humans regularly perform new learning without losing memory for previous information, but neural network models suffer from the phenomenon of catastrophic forgetting in which new learning impairs prior function. A recent article presents an algorithm that spares learning at synapses important for previously learned function, reducing catastrophic forgetting.},
author = {Hasselmo, Michael E.},
doi = {10.1016/j.tics.2017.04.001},
file = {::},
issn = {13646613},
journal = {Trends in Cognitive Sciences},
keywords = {hasselmo2017a},
month = {jun},
number = {6},
pages = {407--408},
publisher = {Elsevier Ltd},
title = {{Avoiding Catastrophic Forgetting}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661317300736},
volume = {21},
year = {2017}
}
@inproceedings{awasthi2019a,
abstract = {Continual learning broadly refers to the algorithms which aim to learn continuously over time across varying domains, tasks or data distributions. This is in contrast to algorithms restricted to learning a fixed number of tasks in a given domain, assuming a static data distribution. In this survey we aim to discuss a wide breadth of challenges faced in a continual learning setup and review existing work in the area. We discuss parameter regularization techniques to avoid catastrophic forgetting in neural networks followed by memory based approaches and the role of generative models in assisting continual learning algorithms. We discuss how dynamic neural networks assist continual learning by endowing neural networks with a new capacity to learn further. We conclude by discussing possible future directions.},
author = {Awasthi, Abhijeet and Sarawagi, Sunita},
booktitle = {Proceedings of the ACM India Joint International Conference on Data Science and Management of Data},
doi = {10.1145},
pages = {362--365},
title = {{Continual Learning with Neural Networks: A Review}},
url = {https://dl.acm.org/doi/pdf/10.1145/3297001.3297062},
year = {2019}
}
@article{DeLange2020a,
abstract = {Artificial neural networks thrive in solving the classification problem for a particular rigid task, acquiring knowledge through generalized learning behaviour from a distinct training phase. The resulting network resembles a static entity of knowledge, with endeavours to extend this knowledge without targeting the original task resulting in a catastrophic forgetting. Continual learning shifts this paradigm towards networks that can continually accumulate knowledge over different tasks without the need to retrain from scratch. We focus on task incremental classification, where tasks arrive sequentially and are delineated by clear boundaries. Our main contributions concern 1) a taxonomy and extensive overview of the state-of-the-art, 2) a novel framework to continually determine the stability-plasticity trade-off of the continual learner, 3) a comprehensive experimental comparison of 11 state-of-the-art continual learning methods and 4 baselines. We empirically scrutinize method strengths and weaknesses on three benchmarks, considering Tiny Imagenet and large-scale unbalanced iNaturalist and a sequence of recognition datasets. We study the influence of model capacity, weight decay and dropout regularization, and the order in which the tasks are presented, and qualitatively compare methods in terms of required memory, computation time, and storage.},
archivePrefix = {arXiv},
arxivId = {1909.08383},
author = {{De Lange}, Matthias and Aljundi, Rahaf and Masana, Marc and Parisot, Sarah and Jia, Xu and Leonardis, Ales and Slabaugh, Gregory and Tuytelaars, Tinne},
eprint = {1909.08383},
file = {::},
journal = {arXiv},
keywords = {Index Terms-Continual Learning,[framework],catastrophic forgetting,classification,lifelong learning,neural networks ✦,task incremental learning},
mendeley-tags = {[framework]},
month = {sep},
title = {{A continual learning survey: Defying forgetting in classification tasks}},
url = {http://arxiv.org/abs/1909.08383},
year = {2019}
}
@inproceedings{Pfulb2018a,
abstract = {We present a large-scale empirical study of catastrophic forgetting (CF) in modern Deep Neural Network (DNN) models that perform sequential (or: incremen-tal) learning. A new experimental protocol is proposed that enforces typical constraints encountered in application scenarios. As the investigation is empirical, we evaluate CF behavior on the hitherto largest number of visual classification datasets, from each of which we construct a representative number of Sequential Learning Tasks (SLTs) in close alignment to previous works on CF. Our results clearly indicate that there is no model that avoids CF for all investigated datasets and SLTs under application conditions. We conclude with a discussion of potential solutions and workarounds to CF, notably for the EWC and IMM models.},
author = {Pf{\"{u}}lb, B and Gepperth, A},
booktitle = {ICLR},
file = {::},
keywords = {[fashion],[mnist]},
mendeley-tags = {[fashion],[mnist]},
month = {sep},
title = {{A comprehensive, application-oriented study of catastrophic forgetting in DNNs}},
url = {https://openreview.net/pdf?id=BkloRs0qK7},
year = {2018}
}
@article{Robins1995a,
abstract = {This paper reviews the problem of catastrophic forgetting (the loss or disruption of previously learned information when new information is learned) in neural networks, and explores rehearsal mechanisms (the retraining of some of the previously learned information as the new information is added) as a potential solution. W e replicate some of the experiments described by Ratcliff (1990), including those relating to a simple `recency' based rehearsal regime. We then develop further rehearsal regimes which are more effective than recency rehearsal. In particular, `sweep rehearsal' is very successful at minimizing catastrophic forgetting. One possible limitation of rehearsal in general, however, is that previously learned information may not be available for retraining. W e describe a solution to this problem, `pseudorehearsal' , a method which provides the advantages of rehearsal without actually requiring any access to the previously learned information (the original training population) itself. We then suggest an interpretation of these rehearsal mechanisms in the context of a function approximation based account of neural network learning. Both rehearsal and pseudorehearsal may have practical applications, allowing new information to be integrated into an existing network with minimum disruption of old informa tion.},
annote = {An in-depth overview of rehearsal techniques, including pseudo-rehearsal. Useful to combine these ideas in dual systems, in which one system learns a task while the other provides rehearsal of previous patterns.},
author = {Robins, Anthony},
doi = {10.1080/09540099550039318},
issn = {0954-0091, 1360-0494},
journal = {Connection Science},
keywords = {[dual]},
language = {en},
mendeley-tags = {[dual]},
month = {jun},
number = {2},
pages = {123--146},
title = {{Catastrophic Forgetting; Catastrophic Interference; Stability; Plasticity; Rehearsal.}},
url = {http://www.tandfonline.com/doi/abs/10.1080/09540099550039318},
volume = {7},
year = {1995}
}
@inproceedings{Kemker2018a,
abstract = {Deep neural networks are used in many state-of-the-art systems for machine perception. Once a network is trained to do a specific task, e.g., bird classification, it cannot easily be trained to do new tasks, e.g., incrementally learning to recognize additional bird species or learning an entirely different task such as flower recognition. When new tasks are added, typical deep neural networks are prone to catastrophically forgetting previous tasks. Networks that are capable of assimilating new information incrementally, much like how humans form new memories over time, will be more efficient than re-training the model from scratch each time a new task needs to be learned. There have been multiple attempts to develop schemes that mitigate catastrophic forgetting, but these methods have not been directly compared, the tests used to evaluate them vary considerably, and these methods have only been evaluated on small-scale problems (e.g., MNIST). In this paper, we introduce new metrics and benchmarks for directly comparing five different mechanisms designed to mitigate catastrophic forgetting in neural networks: regularization, ensembling, rehearsal, dual-memory, and sparse-coding. Our experiments on real-world images and sounds show that the mechanism(s) that are critical for optimal performance vary based on the incremental training paradigm and type of data being used, but they all demonstrate that the catastrophic forgetting problem is not yet solved.},
author = {Kemker, Ronald and McClure, Marc and Abitino, Angelina and Hayes, Tyler L and Kanan, Christopher},
booktitle = {Thirty-Second AAAI Conference on Artificial Intelligence},
keywords = {[mnist],audioset,kemker,review,survey},
language = {en},
mendeley-tags = {[mnist]},
month = {apr},
title = {{Measuring Catastrophic Forgetting in Neural Networks}},
url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16410},
year = {2018}
}
@article{Lesort2020c,
abstract = {Continual learning (CL) is a particular machine learning paradigm where the data distribution and learning objective change through time, or where all the training data and objective criteria are never available at once. The evolution of the learning process is modeled by a sequence of learning experiences where the goal is to be able to learn new skills all along the sequence without forgetting what has been previously learned. CL can be seen as an online learning where knowledge fusion needs to take place in order to learn from streams of data presented sequentially in time. Continual learning also aims at the same time at optimizing the memory, the computation power and the speed during the learning process. An important challenge for machine learning is not necessarily finding solutions that work in the real world but rather finding stable algorithms that can learn in real world. Hence, the ideal approach would be tackling the real world in a embodied platform: an autonomous agent. Continual learning would then be effective in an autonomous agent or robot, which would learn autonomously through time about the external world, and incrementally develop a set of complex skills and knowledge.Robotic agents have to learn to adapt and interact with their environment using a continuous stream of observations. Some recent approaches aim at tackling continual learning for robotics, but most recent papers on continual learning only experiment approaches in simulation or with static datasets. Unfortunately, the evaluation of those algorithms does not provide insights on whether their solutions may help continual learning in the context of robotics. This paper aims at reviewing the existing state of the art of continual learning, summarizing existing benchmarks and metrics, and proposing a framework for presenting and evaluating both robotics and non robotics approaches in a way that makes transfer between both fields easier. We put light on continual learning in the context of robotics to create connections between fields and normalize approaches.},
annote = {Overview of a CL framework for applications in robotic, together with discussion of existing CL strategies and techniques.},
author = {Lesort, Timoth{\'{e}}e and Lomonaco, Vincenzo and Stoian, Andrei and Maltoni, Davide and Filliat, David and D{\'{i}}az-Rodr{\'{i}}guez, Natalia},
doi = {10.1016/j.inffus.2019.12.004},
issn = {1566-2535},
journal = {Information Fusion},
keywords = {Catastrophic Forgetting,Continual Learning,Deep Learning,Lifelong Learning,Reinforcement Learning,Robotics,[framework]},
language = {en},
mendeley-tags = {[framework]},
month = {jun},
pages = {52--68},
shorttitle = {Continual learning for robotics},
title = {{Continual learning for robotics: Definition, framework, learning strategies, opportunities and challenges}},
url = {http://www.sciencedirect.com/science/article/pii/S1566253519307377},
volume = {58},
year = {2020}
}
@article{Parisi2019a,
abstract = {Humans and animals have the ability to continually acquire, fine-tune, and transfer knowledge and skills throughout their lifespan. This ability, referred to as lifelong learning, is mediated by a rich set of neurocognitive mechanisms that together contribute to the development and specialization of our sensorimotor skills as well as to long-term memory consolidation and retrieval. Consequently, lifelong learning capabilities are crucial for computational learning systems and autonomous agents interacting in the real world and processing continuous streams of information. However, lifelong learning remains a long-standing challenge for machine learning and neural network models since the continual acquisition of incrementally available information from non-stationary data distributions generally leads to catastrophic forgetting or interference. This limitation represents a major drawback for state-of-the-art deep neural network models that typically learn representations from stationary batches of training data, thus without accounting for situations in which information becomes incrementally available over time. In this review, we critically summarize the main challenges linked to lifelong learning for artificial learning systems and compare existing neural network approaches that alleviate, to different extents, catastrophic forgetting. Although significant advances have been made in domain-specific learning with neural networks, extensive research efforts are required for the development of robust lifelong learning on autonomous agents and robots. We discuss well-established and emerging research motivated by lifelong learning factors in biological systems such as structural plasticity, memory replay, curriculum and transfer learning, intrinsic motivation, and multisensory integration.},
annote = {A general survey of the most important and used CL techniques, with a characterisation of the catastrophic forgetting phenomenon.},
author = {Parisi, German I and Kemker, Ronald and Part, Jose L and Kanan, Christopher and Wermter, Stefan},
doi = {10.1016/j.neunet.2019.01.012},
issn = {0893-6080},
journal = {Neural Networks},
keywords = {Catastrophic forgetting,Continual learning,Developmental systems,Lifelong learning,Memory consolidation,[framework]},
language = {en},
mendeley-tags = {[framework]},
month = {may},
pages = {54--71},
shorttitle = {Continual lifelong learning with neural networks},
title = {{Continual lifelong learning with neural networks: A review}},
url = {http://www.sciencedirect.com/science/article/pii/S0893608019300231},
volume = {113},
year = {2019}
}
