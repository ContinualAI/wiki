

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Research &mdash; ContinualAI Wiki  documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Industry" href="industry.html" />
    <link rel="prev" title="Introduction" href="introduction.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html">
          

          
            
            <img src="_static/logo_inline_wiki.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Introduction</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Research</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#publications">Publications</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#classics">Classics</a></li>
<li class="toctree-l3"><a class="reference internal" href="#review-papers-and-books">Review Papers and Books</a></li>
<li class="toctree-l3"><a class="reference internal" href="#catastrophic-forgetting-studies">Catastrophic Forgetting Studies</a></li>
<li class="toctree-l3"><a class="reference internal" href="#benchmarks">Benchmarks</a></li>
<li class="toctree-l3"><a class="reference internal" href="#architectural-methods">Architectural Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="#regularization-methods">Regularization Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="#rehearsal-methods">Rehearsal Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="#generative-replay-methods">Generative Replay Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="#hybrid-methods">Hybrid Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="#metrics-and-evaluations">Metrics and Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="#bioinspired-methods">Bioinspired Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="#meta-continual-learning">Meta Continual Learning</a></li>
<li class="toctree-l3"><a class="reference internal" href="#continual-meta-learning">Continual Meta Learning</a></li>
<li class="toctree-l3"><a class="reference internal" href="#continual-reinforcement-learning">Continual Reinforcement Learning</a></li>
<li class="toctree-l3"><a class="reference internal" href="#continual-sequential-learning">Continual Sequential Learning</a></li>
<li class="toctree-l3"><a class="reference internal" href="#dissertation-and-theses">Dissertation and Theses</a></li>
<li class="toctree-l3"><a class="reference internal" href="#applications">Applications</a></li>
<li class="toctree-l3"><a class="reference internal" href="#neuroscience">Neuroscience</a></li>
<li class="toctree-l3"><a class="reference internal" href="#robotics">Robotics</a></li>
<li class="toctree-l3"><a class="reference internal" href="#others">Others</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#conference-workshops">Conference Workshops</a></li>
<li class="toctree-l2"><a class="reference internal" href="#research-programs">Research Programs</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="industry.html">Industry</a></li>
<li class="toctree-l1"><a class="reference internal" href="software_and_data.html">Software and Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials.html">Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="news_and_media.html">Media Articles</a></li>
<li class="toctree-l1"><a class="reference internal" href="about_us.html">About Us</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">ContinualAI Wiki</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Research</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="research">
<h1>Research<a class="headerlink" href="#research" title="Permalink to this headline">¶</a></h1>
<p>In the last few years we have witnessed a renewed and steadily growing interest
in the ability to learn continuously from high-dimensional data. In this page,
we will keep track of recent <strong>Continual/Lifelong</strong> Learning developments in
the research community.</p>
<div class="section" id="publications">
<h2>Publications<a class="headerlink" href="#publications" title="Permalink to this headline">¶</a></h2>
<p>In this section we maintain an updated list of publications related to Continual Learning.
This references list is automatically generated by a single bibtex file maintained
by the ContinualAI community through an open Mendeley group! Join our group <a class="reference external" href="https://www.mendeley.com/community/continual-learning-papers/?__cf_chl_captcha_tk__=d4a16b2e7ba082bc24fbb7fb7cbba3149969ff33-1589287156-0-Aa1Wr5LQkCQwqaFz3Ho_5lc1NnR1Dn6bDEe8fZlbjwIKIQy-b28wKYYcbcdksrP0zP2e8x1BfyD3V0eiZWMVdFQ0AqGzm8qHQYklAGUPz0COhkQec_hu0O1_XFh7PtHXNKfIiyBb9TppP05KlSNIIxJk2u7lNAlGw1pWscPNhIvk_4p-5XDf-YFu3HpCDYN1IQ7bQgkGqMRYAdYtZS7gq1C_w6iykd2sA6IawsIbaCtdW08H77e-7T7rEdo91HndXMIJgV5UQBnJSwRHOl-g-8EKrUWUDHBdGQgLhiJli4y16AAGu979jkOyhtS7onFfRNXdUELb3pOiD0YS5zCnmHM6PURblRyb6HA2ma7f0JIC8DIjmK2xCcRlYqgiNrWVS3oEbS6uqn63IdxYgoSLq6vo68mS1e_Or8LGRpOE8uemjJfbVnPR4RI3mqevN5OxbgWz-CYkElgLAXeaEFqVitVCsaEmDygdit6flohhCpCd5vVs6gv1t_ALu6Q7nZIbFc386zRcqDb-MhIV7BpRIOA">here</a>
to add a reference to your paper! Please, remember to follow the (very simple) <a class="reference external" href="https://github.com/ContinualAI/wiki#how-to-contribute-to-the-continualai-database-of-publications">contributions guidelines</a> when adding new papers.</p>
<p><strong>Search among 130 papers!</strong></p>
<p><strong>Filter list by keyword:</strong> <input type="text" id="myInput" onkeyup="keyword_filter()" placeholder="Insert keywords here..."><br>
<strong>Filter list by regex:</strong> <input type="text" id="myInputreg" onkeyup="regex_filter()" placeholder="Insert regex here..." style="margin-left:22px"></p>
<div class="section" id="classics">
<h3>Classics<a class="headerlink" href="#classics" title="Permalink to this headline">¶</a></h3>
<p>In this section you’ll find pioneering and classic continual learning papers. We recommend to read all the papers in this section for a good background on current continual deep learning developments.</p>
<ul class="simple">
<li><p>The Organization of Behavior: A Neuropsychological Theory by  and D O Hebb. <em>Lawrence Erlbaum</em>, 2002. <span style='background-color:#99C68E; padding: 2px; border-radius:4px; border: 1px solid black;'>[hebbian]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Hebb2005aClassicsFunction()" id="Hebb2005aClassics_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Hebb2005aClassicsFunction2()" id="Hebb2005aClassics_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Hebb2005aClassicsFunction3()" id="Hebb2005aClassics_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Hebb2005aClassics_more" style="display: none">
    @book{Hebb2005a,<br> author = {Hebb, D O},<br> booktitle = {Lawrence Erlbaum},<br> isbn = {978-1-135-63191-8},<br> keywords = {Psychology / Cognitive Psychology & Cognition,Psychology / General,Psychology / Neuropsychology,Psychology / Physiological Psychology,[hebbian]},<br> language = {en},<br> mendeley-tags = {[hebbian]},<br> month = {apr},<br> publisher = {Psychology Press},<br> shorttitle = {The Organization of Behavior},<br> title = {The Organization of Behavior: A Neuropsychological Theory},<br> year = {2002}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Hebb2005aClassics_more2" style="display: none">
    Since its publication in 1949, D.O. Hebb's, The Organization of Behavior has been one of the most influential books in the fields of psychology and neuroscience. However, the original edition has been unavailable since 1966, ensuring that Hebb's comment that a classic normally means "cited but not read" is true in his case. This new edition rectifies a long-standing problem for behavioral neuroscientists–the inability to obtain one of the most cited publications in the field. The Organization of Behavior played a significant part in stimulating the investigation of the neural foundations of behavior and continues to be inspiring because it provides a general framework for relating behavior to synaptic organization through the dynamics of neural networks. D.O. Hebb was also the first to examine the mechanisms by which environment and experience can influence brain structure and function, and his ideas formed the basis for work on enriched environments as stimulants for behavioral development. References to Hebb, the Hebbian cell assembly, the Hebb synapse, and the Hebb rule increase each year. These forceful ideas of 1949 are now applied in engineering, robotics, and computer science, as well as neurophysiology, neuroscience, and psychology–a tribute to Hebb's foresight in developing a foundational neuropsychological theory of the organization of behavior.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Hebb2005aClassics_more3" style="display: none">
    N.A.
</span></p>
<script>
    function Hebb2005aClassicsFunction() {
      var moreText = document.getElementById("Hebb2005aClassics_more");
      var moreText2 = document.getElementById("Hebb2005aClassics_more2");
      var moreText3 = document.getElementById("Hebb2005aClassics_more3");
      var btnText = document.getElementById("Hebb2005aClassics_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Hebb2005aClassicsFunction2() {
      var moreText = document.getElementById("Hebb2005aClassics_more2");
      var moreText1 = document.getElementById("Hebb2005aClassics_more");
      var moreText3 = document.getElementById("Hebb2005aClassics_more3");
      var btnText = document.getElementById("Hebb2005aClassics_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Hebb2005aClassicsFunction3() {
      var moreText = document.getElementById("Hebb2005aClassics_more3");
      var moreText1 = document.getElementById("Hebb2005aClassics_more");
      var moreText2 = document.getElementById("Hebb2005aClassics_more2");
      var btnText = document.getElementById("Hebb2005aClassics_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="http://www.tandfonline.com/doi/abs/10.1080/095400997116595">Pseudo-recurrent Connectionist Networks: An Approach to the ‘Sensitivity-Stability’ Dilemma</a> by  and Robert French. <em>Connection Science</em>, 353–380, 1997. <span style='background-color:green; padding: 2px; border-radius:4px; border: 1px solid black;'>[dual]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="French1997aClassicsFunction()" id="French1997aClassics_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="French1997aClassicsFunction2()" id="French1997aClassics_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="French1997aClassicsFunction3()" id="French1997aClassics_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="French1997aClassics_more" style="display: none">
    @article{French1997a,<br> annote = {In this seminal paper the author introduces many different forms of rehearsal in order to mitigate the catastrophic forgetting phenomenon},<br> author = {French, Robert},<br> doi = {10.1080/095400997116595},<br> issn = {0954-0091, 1360-0494},<br> journal = {Connection Science},<br> keywords = {Catastrophic Interference,Dual Memory,Keywords: Pseudopatterns,Semi-distributed Representations,Sensitivity-stability Transfer,[dual],dilemma,plasticity,stability},<br> language = {en},<br> mendeley-tags = {[dual]},<br> month = {dec},<br> number = {4},<br> pages = {353--380},<br> shorttitle = {Pseudo-recurrent Connectionist Networks},<br> title = {Pseudo-recurrent Connectionist Networks: An Approach to the 'Sensitivity-Stability' Dilemma},<br> url = {http://www.tandfonline.com/doi/abs/10.1080/095400997116595},<br> volume = {9},<br> year = {1997}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="French1997aClassics_more2" style="display: none">
    In order to solve the “sensitivity-stability” problem — and its immediate correlate, the problem of sequential learning — it is crucial to develop connectionist architectures that are simultaneously sensitive to, but not excessively disrupted by, new input. French (1992) suggested that to alleviate a particularly severe form of this disruption, catastrophic forgetting, it was necessary for networks to dynamically separate their internal representations during learning. McClelland, McNaughton, & O'Reilly (1995) went even further. They suggested that nature's way of implementing this obligatory separation was the evolution of two separate areas of the brain, the hippocampus and the neocortex. In keeping with this idea of radical separation, a “pseudo-recurrent” memory model is presented here that partitions a connectionist network into two functionally distinct, but continually interacting areas. One area serves as a final-storage area for representations; the other is an early-processing area where new representations are first learned by the system. The final-storage area continually supplies internally generated patterns (pseudopatterns, Robins (1995)), which are approximations of its content, to the early-processing area, where they are interleaved with the new patterns to be learned. Transfer of the new learning is done either by weight-copying from the early-processing area to the final-storage area or by pseudopattern transfer. A number of experiments are presented that demonstrate the effectiveness of this approach, allowing, in particular, effective sequential learning with gradual forgetting in the presence of new input. Finally, it is shown that the two interacting areas automatically produce representational compaction and it is suggested that similar representational streamlining may exist in the brain.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="French1997aClassics_more3" style="display: none">
    In this seminal paper the author introduces many different forms of rehearsal in order to mitigate the catastrophic forgetting phenomenon
</span></p>
<script>
    function French1997aClassicsFunction() {
      var moreText = document.getElementById("French1997aClassics_more");
      var moreText2 = document.getElementById("French1997aClassics_more2");
      var moreText3 = document.getElementById("French1997aClassics_more3");
      var btnText = document.getElementById("French1997aClassics_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function French1997aClassicsFunction2() {
      var moreText = document.getElementById("French1997aClassics_more2");
      var moreText1 = document.getElementById("French1997aClassics_more");
      var moreText3 = document.getElementById("French1997aClassics_more3");
      var btnText = document.getElementById("French1997aClassics_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function French1997aClassicsFunction3() {
      var moreText = document.getElementById("French1997aClassics_more3");
      var moreText1 = document.getElementById("French1997aClassics_more");
      var moreText2 = document.getElementById("French1997aClassics_more2");
      var btnText = document.getElementById("French1997aClassics_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="https://doi.org/10.1023/A:1007331723572">CHILD: A First Step Towards Continual Learning</a> by  and Mark B Ring. <em>Machine Learning</em>, 77–104, 1997. <span style='background-color:AliceBlue; padding: 2px; border-radius:4px; border: 1px solid black;'>[rnn]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Ring1997aClassicsFunction()" id="Ring1997aClassics_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Ring1997aClassicsFunction2()" id="Ring1997aClassics_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Ring1997aClassicsFunction3()" id="Ring1997aClassics_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Ring1997aClassics_more" style="display: none">
    @article{Ring1997a,<br> author = {Ring, Mark B},<br> doi = {10.1023/A:1007331723572},<br> issn = {1573-0565},<br> journal = {Machine Learning},<br> keywords = {Continual learning,[rnn],cl,continual learner,definition,hierarchical neural networks,reinforcement learning,sequence learning,transfer},<br> language = {en},<br> mendeley-tags = {[rnn]},<br> month = {jul},<br> number = {1},<br> pages = {77--104},<br> shorttitle = {CHILD},<br> title = {CHILD: A First Step Towards Continual Learning},<br> url = {https://doi.org/10.1023/A:1007331723572},<br> volume = {28},<br> year = {1997}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Ring1997aClassics_more2" style="display: none">
    Continual learning is the constant development of increasingly complex behaviors; the process of building more complicated skills on top of those already developed. A continual-learning agent should therefore learn incrementally and hierarchically. This paper describes CHILD, an agent capable of Continual, Hierarchical, Incremental Learning and Development. CHILD can quickly solve complicated non-Markovian reinforcement-learning tasks and can then transfer its skills to similar but even more complicated tasks, learning these faster still.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Ring1997aClassics_more3" style="display: none">
    N.A.
</span></p>
<script>
    function Ring1997aClassicsFunction() {
      var moreText = document.getElementById("Ring1997aClassics_more");
      var moreText2 = document.getElementById("Ring1997aClassics_more2");
      var moreText3 = document.getElementById("Ring1997aClassics_more3");
      var btnText = document.getElementById("Ring1997aClassics_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Ring1997aClassicsFunction2() {
      var moreText = document.getElementById("Ring1997aClassics_more2");
      var moreText1 = document.getElementById("Ring1997aClassics_more");
      var moreText3 = document.getElementById("Ring1997aClassics_more3");
      var btnText = document.getElementById("Ring1997aClassics_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Ring1997aClassicsFunction3() {
      var moreText = document.getElementById("Ring1997aClassics_more3");
      var moreText1 = document.getElementById("Ring1997aClassics_more");
      var moreText2 = document.getElementById("Ring1997aClassics_more2");
      var btnText = document.getElementById("Ring1997aClassics_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="http://papers.nips.cc/paper/1034-is-learning-the-n-th-thing-any-easier-than-learning-the-first.pdf">Is Learning The n-th Thing Any Easier Than Learning The First?</a> by  and Sebastian Thrun. <em>Advances in Neural Information Processing Systems 8</em>, 640–646, 1996. <span style='background-color:#E5E4E2; padding: 2px; border-radius:4px; border: 1px solid black;'>[vision]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Thrun1996aClassicsFunction()" id="Thrun1996aClassics_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Thrun1996aClassicsFunction2()" id="Thrun1996aClassics_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Thrun1996aClassicsFunction3()" id="Thrun1996aClassics_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Thrun1996aClassics_more" style="display: none">
    @inproceedings{Thrun1996a,<br> author = {Thrun, Sebastian},<br> booktitle = {Advances in Neural Information Processing Systems 8},<br> editor = {Touretzky, D S and Mozer, M C and Hasselmo, M E},<br> keywords = {[vision],lifelong,lifelong learning},<br> mendeley-tags = {[vision]},<br> pages = {640--646},<br> publisher = {MIT Press},<br> title = {Is Learning The n-th Thing Any Easier Than Learning The First?},<br> url = {http://papers.nips.cc/paper/1034-is-learning-the-n-th-thing-any-easier-than-learning-the-first.pdf},<br> year = {1996}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Thrun1996aClassics_more2" style="display: none">
    N.A.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Thrun1996aClassics_more3" style="display: none">
    N.A.
</span></p>
<script>
    function Thrun1996aClassicsFunction() {
      var moreText = document.getElementById("Thrun1996aClassics_more");
      var moreText2 = document.getElementById("Thrun1996aClassics_more2");
      var moreText3 = document.getElementById("Thrun1996aClassics_more3");
      var btnText = document.getElementById("Thrun1996aClassics_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Thrun1996aClassicsFunction2() {
      var moreText = document.getElementById("Thrun1996aClassics_more2");
      var moreText1 = document.getElementById("Thrun1996aClassics_more");
      var moreText3 = document.getElementById("Thrun1996aClassics_more3");
      var btnText = document.getElementById("Thrun1996aClassics_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Thrun1996aClassicsFunction3() {
      var moreText = document.getElementById("Thrun1996aClassics_more3");
      var moreText1 = document.getElementById("Thrun1996aClassics_more");
      var moreText2 = document.getElementById("Thrun1996aClassics_more2");
      var btnText = document.getElementById("Thrun1996aClassics_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="http://www.tandfonline.com/doi/abs/10.1080/09540099550039318">Catastrophic Forgetting; Catastrophic Interference; Stability; Plasticity; Rehearsal.</a> by  and Anthony Robins. <em>Connection Science</em>, 123–146, 1995. <span style='background-color:green; padding: 2px; border-radius:4px; border: 1px solid black;'>[dual]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Robins1995aClassicsFunction()" id="Robins1995aClassics_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Robins1995aClassicsFunction2()" id="Robins1995aClassics_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Robins1995aClassicsFunction3()" id="Robins1995aClassics_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Robins1995aClassics_more" style="display: none">
    @article{Robins1995a,<br> annote = {An in-depth overview of rehearsal techniques, including pseudo-rehearsal. Useful to combine these ideas in dual systems, in which one system learns a task while the other provides rehearsal of previous patterns.},<br> author = {Robins, Anthony},<br> doi = {10.1080/09540099550039318},<br> issn = {0954-0091, 1360-0494},<br> journal = {Connection Science},<br> keywords = {[dual]},<br> language = {en},<br> mendeley-tags = {[dual]},<br> month = {jun},<br> number = {2},<br> pages = {123--146},<br> title = {Catastrophic Forgetting; Catastrophic Interference; Stability; Plasticity; Rehearsal.},<br> url = {http://www.tandfonline.com/doi/abs/10.1080/09540099550039318},<br> volume = {7},<br> year = {1995}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Robins1995aClassics_more2" style="display: none">
    This paper reviews the problem of catastrophic forgetting (the loss or disruption of previously learned information when new information is learned) in neural networks, and explores rehearsal mechanisms (the retraining of some of the previously learned information as the new information is added) as a potential solution. W e replicate some of the experiments described by Ratcliff (1990), including those relating to a simple `recency' based rehearsal regime. We then develop further rehearsal regimes which are more effective than recency rehearsal. In particular, `sweep rehearsal' is very successful at minimizing catastrophic forgetting. One possible limitation of rehearsal in general, however, is that previously learned information may not be available for retraining. W e describe a solution to this problem, `pseudorehearsal' , a method which provides the advantages of rehearsal without actually requiring any access to the previously learned information (the original training population) itself. We then suggest an interpretation of these rehearsal mechanisms in the context of a function approximation based account of neural network learning. Both rehearsal and pseudorehearsal may have practical applications, allowing new information to be integrated into an existing network with minimum disruption of old informa tion.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Robins1995aClassics_more3" style="display: none">
    An in-depth overview of rehearsal techniques, including pseudo-rehearsal. Useful to combine these ideas in dual systems, in which one system learns a task while the other provides rehearsal of previous patterns.
</span></p>
<script>
    function Robins1995aClassicsFunction() {
      var moreText = document.getElementById("Robins1995aClassics_more");
      var moreText2 = document.getElementById("Robins1995aClassics_more2");
      var moreText3 = document.getElementById("Robins1995aClassics_more3");
      var btnText = document.getElementById("Robins1995aClassics_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Robins1995aClassicsFunction2() {
      var moreText = document.getElementById("Robins1995aClassics_more2");
      var moreText1 = document.getElementById("Robins1995aClassics_more");
      var moreText3 = document.getElementById("Robins1995aClassics_more3");
      var btnText = document.getElementById("Robins1995aClassics_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Robins1995aClassicsFunction3() {
      var moreText = document.getElementById("Robins1995aClassics_more3");
      var moreText1 = document.getElementById("Robins1995aClassics_more");
      var moreText2 = document.getElementById("Robins1995aClassics_more2");
      var btnText = document.getElementById("Robins1995aClassics_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p>Using Semi-Distributed Representations to Overcome Catastrophic Forgetting in Connectionist Networks by  and Robert M French. <em>In Proceedings of the 13th Annual Cognitive Science Society Conference</em>, 173–178, 1991. <span style='background-color:DarkSalmon; padding: 2px; border-radius:4px; border: 1px solid black;'>[sparsity]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="French1991aClassicsFunction()" id="French1991aClassics_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="French1991aClassicsFunction2()" id="French1991aClassics_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="French1991aClassicsFunction3()" id="French1991aClassics_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="French1991aClassics_more" style="display: none">
    @inproceedings{French1991a,<br> author = {French, Robert M},<br> booktitle = {In Proceedings of the 13th Annual Cognitive Science Society Conference},<br> keywords = {[sparsity],activation sharpening},<br> mendeley-tags = {[sparsity]},<br> pages = {173--178},<br> publisher = {Erlbaum},<br> title = {Using Semi-Distributed Representations to Overcome Catastrophic Forgetting in Connectionist Networks},<br> year = {1991}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="French1991aClassics_more2" style="display: none">
    In connectionist networks, newly-learned information destroys previously-learned information unless the network is continually retrained on the old information. This behavior, known as catastrophic forgetting, is unacceptable both for practical purposes and as a model of mind. This paper advances the claim that catastrophic forgetting is a direct consequence of the overlap of the system's distributed representations and can be reduced by reducing this overlap. A simple algorithm is presented that allows a standard feedforward backpropagation network to develop semi-distributed representations, thereby significantly reducing the problem of catastrophic forgetting. 1 Introduction Catastrophic forgetting is the inability of a neural network to retain old information in the presence of new. New information destroys old unless the old information is continually relearned by the net. McCloskey & Cohen [1990] and Ratcliff [1989] have demonstrated that this is a serious problem with c...
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="French1991aClassics_more3" style="display: none">
    N.A.
</span></p>
<script>
    function French1991aClassicsFunction() {
      var moreText = document.getElementById("French1991aClassics_more");
      var moreText2 = document.getElementById("French1991aClassics_more2");
      var moreText3 = document.getElementById("French1991aClassics_more3");
      var btnText = document.getElementById("French1991aClassics_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function French1991aClassicsFunction2() {
      var moreText = document.getElementById("French1991aClassics_more2");
      var moreText1 = document.getElementById("French1991aClassics_more");
      var moreText3 = document.getElementById("French1991aClassics_more3");
      var btnText = document.getElementById("French1991aClassics_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function French1991aClassicsFunction3() {
      var moreText = document.getElementById("French1991aClassics_more3");
      var moreText1 = document.getElementById("French1991aClassics_more");
      var moreText2 = document.getElementById("French1991aClassics_more2");
      var btnText = document.getElementById("French1991aClassics_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
</ul>
</div>
<div class="section" id="review-papers-and-books">
<h3>Review Papers and Books<a class="headerlink" href="#review-papers-and-books" title="Permalink to this headline">¶</a></h3>
<p>In this section we collect all the main review papers and books on the subject. These may constitute a solid starting point for continual learning newcomers.</p>
<ul class="simple">
<li><p>A continual learning survey: Defying forgetting in classification tasks by Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Aleš Leonardis, Gregory Slabaugh and Tinne Tuytelaars. <em>arXiv</em>, 2020. <span style='background-color:Tomato; padding: 2px; border-radius:4px; border: 1px solid black;'>[framework]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="DeLange2020aReview_Papers_and_BooksFunction()" id="DeLange2020aReview_Papers_and_Books_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="DeLange2020aReview_Papers_and_BooksFunction2()" id="DeLange2020aReview_Papers_and_Books_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="DeLange2020aReview_Papers_and_BooksFunction3()" id="DeLange2020aReview_Papers_and_Books_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="DeLange2020aReview_Papers_and_Books_more" style="display: none">
    @article{DeLange2020a,<br> archiveprefix = {arXiv},<br> arxivid = {1909.08383v2},<br> author = {De Lange, Matthias and Aljundi, Rahaf and Masana, Marc and Parisot, Sarah and Jia, Xu and Leonardis, Aleš and Slabaugh, Gregory and Tuytelaars, Tinne},<br> eprint = {1909.08383v2},<br> file = {::},<br> journal = {arXiv},<br> keywords = {Index Terms-Continual Learning,[framework],catastrophic forgetting,classification,lifelong learning,neural networks ✦,task incremental learning},<br> mendeley-tags = {[framework]},<br> title = {A continual learning survey: Defying forgetting in classification tasks},<br> year = {2020}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="DeLange2020aReview_Papers_and_Books_more2" style="display: none">
    Artificial neural networks thrive in solving the classification problem for a particular rigid task, acquiring knowledge through generalized learning behaviour from a distinct training phase. The resulting network resembles a static entity of knowledge, with endeavours to extend this knowledge without targeting the original task resulting in a catastrophic forgetting. Continual learning shifts this paradigm towards networks that can continually accumulate knowledge over different tasks without the need to retrain from scratch. We focus on task incremental classification, where tasks arrive sequentially and are delineated by clear boundaries. Our main contributions concern 1) a taxonomy and extensive overview of the state-of-the-art, 2) a novel framework to continually determine the stability-plasticity trade-off of the continual learner, 3) a comprehensive experimental comparison of 11 state-of-the-art continual learning methods and 4 baselines. We empirically scrutinize method strengths and weaknesses on three benchmarks, considering Tiny Imagenet and large-scale unbalanced iNaturalist and a sequence of recognition datasets. We study the influence of model capacity, weight decay and dropout regularization, and the order in which the tasks are presented, and qualitatively compare methods in terms of required memory, computation time and storage. Code will be made publicly available upon acceptance of this paper.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="DeLange2020aReview_Papers_and_Books_more3" style="display: none">
    N.A.
</span></p>
<script>
    function DeLange2020aReview_Papers_and_BooksFunction() {
      var moreText = document.getElementById("DeLange2020aReview_Papers_and_Books_more");
      var moreText2 = document.getElementById("DeLange2020aReview_Papers_and_Books_more2");
      var moreText3 = document.getElementById("DeLange2020aReview_Papers_and_Books_more3");
      var btnText = document.getElementById("DeLange2020aReview_Papers_and_Books_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function DeLange2020aReview_Papers_and_BooksFunction2() {
      var moreText = document.getElementById("DeLange2020aReview_Papers_and_Books_more2");
      var moreText1 = document.getElementById("DeLange2020aReview_Papers_and_Books_more");
      var moreText3 = document.getElementById("DeLange2020aReview_Papers_and_Books_more3");
      var btnText = document.getElementById("DeLange2020aReview_Papers_and_Books_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function DeLange2020aReview_Papers_and_BooksFunction3() {
      var moreText = document.getElementById("DeLange2020aReview_Papers_and_Books_more3");
      var moreText1 = document.getElementById("DeLange2020aReview_Papers_and_Books_more");
      var moreText2 = document.getElementById("DeLange2020aReview_Papers_and_Books_more2");
      var btnText = document.getElementById("DeLange2020aReview_Papers_and_Books_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="http://www.sciencedirect.com/science/article/pii/S1566253519307377">Continual learning for robotics: Definition, framework, learning strategies, opportunities and challenges</a> by Timothée Lesort, Vincenzo Lomonaco, Andrei Stoian, Davide Maltoni, David Filliat and Natalia Díaz-Rodrǵuez. <em>Information Fusion</em>, 52–68, 2020. <span style='background-color:Tomato; padding: 2px; border-radius:4px; border: 1px solid black;'>[framework]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Lesort2020aReview_Papers_and_BooksFunction()" id="Lesort2020aReview_Papers_and_Books_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Lesort2020aReview_Papers_and_BooksFunction2()" id="Lesort2020aReview_Papers_and_Books_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Lesort2020aReview_Papers_and_BooksFunction3()" id="Lesort2020aReview_Papers_and_Books_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Lesort2020aReview_Papers_and_Books_more" style="display: none">
    @article{Lesort2020a,<br> annote = {Overview of a CL framework for applications in robotic, together with discussion of existing CL strategies and techniques.},<br> author = {Lesort, Timothée and Lomonaco, Vincenzo and Stoian, Andrei and Maltoni, Davide and Filliat, David and Díaz-Rodr\ǵuez, Natalia},<br> doi = {10.1016/j.inffus.2019.12.004},<br> issn = {1566-2535},<br> journal = {Information Fusion},<br> keywords = {Catastrophic Forgetting,Continual Learning,Deep Learning,Lifelong Learning,Reinforcement Learning,Robotics,[framework]},<br> language = {en},<br> mendeley-tags = {[framework]},<br> month = {jun},<br> pages = {52--68},<br> shorttitle = {Continual learning for robotics},<br> title = {Continual learning for robotics: Definition, framework, learning strategies, opportunities and challenges},<br> url = {http://www.sciencedirect.com/science/article/pii/S1566253519307377},<br> volume = {58},<br> year = {2020}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Lesort2020aReview_Papers_and_Books_more2" style="display: none">
    Continual learning (CL) is a particular machine learning paradigm where the data distribution and learning objective change through time, or where all the training data and objective criteria are never available at once. The evolution of the learning process is modeled by a sequence of learning experiences where the goal is to be able to learn new skills all along the sequence without forgetting what has been previously learned. CL can be seen as an online learning where knowledge fusion needs to take place in order to learn from streams of data presented sequentially in time. Continual learning also aims at the same time at optimizing the memory, the computation power and the speed during the learning process. An important challenge for machine learning is not necessarily finding solutions that work in the real world but rather finding stable algorithms that can learn in real world. Hence, the ideal approach would be tackling the real world in a embodied platform: an autonomous agent. Continual learning would then be effective in an autonomous agent or robot, which would learn autonomously through time about the external world, and incrementally develop a set of complex skills and knowledge.Robotic agents have to learn to adapt and interact with their environment using a continuous stream of observations. Some recent approaches aim at tackling continual learning for robotics, but most recent papers on continual learning only experiment approaches in simulation or with static datasets. Unfortunately, the evaluation of those algorithms does not provide insights on whether their solutions may help continual learning in the context of robotics. This paper aims at reviewing the existing state of the art of continual learning, summarizing existing benchmarks and metrics, and proposing a framework for presenting and evaluating both robotics and non robotics approaches in a way that makes transfer between both fields easier. We put light on continual learning in the context of robotics to create connections between fields and normalize approaches.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Lesort2020aReview_Papers_and_Books_more3" style="display: none">
    Overview of a CL framework for applications in robotic, together with discussion of existing CL strategies and techniques.
</span></p>
<script>
    function Lesort2020aReview_Papers_and_BooksFunction() {
      var moreText = document.getElementById("Lesort2020aReview_Papers_and_Books_more");
      var moreText2 = document.getElementById("Lesort2020aReview_Papers_and_Books_more2");
      var moreText3 = document.getElementById("Lesort2020aReview_Papers_and_Books_more3");
      var btnText = document.getElementById("Lesort2020aReview_Papers_and_Books_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Lesort2020aReview_Papers_and_BooksFunction2() {
      var moreText = document.getElementById("Lesort2020aReview_Papers_and_Books_more2");
      var moreText1 = document.getElementById("Lesort2020aReview_Papers_and_Books_more");
      var moreText3 = document.getElementById("Lesort2020aReview_Papers_and_Books_more3");
      var btnText = document.getElementById("Lesort2020aReview_Papers_and_Books_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Lesort2020aReview_Papers_and_BooksFunction3() {
      var moreText = document.getElementById("Lesort2020aReview_Papers_and_Books_more3");
      var moreText1 = document.getElementById("Lesort2020aReview_Papers_and_Books_more");
      var moreText2 = document.getElementById("Lesort2020aReview_Papers_and_Books_more2");
      var btnText = document.getElementById("Lesort2020aReview_Papers_and_Books_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="https://dl.acm.org/doi/pdf/10.1145/3297001.3297062">Continual Learning with Neural Networks: A Review</a> by Abhijeet Awasthi and Sunita Sarawagi. <em>Proceedings of the ACM India Joint International Conference on Data Science and Management of Data</em>, 362–365, 2019.  <br>
<button style="font-size:75%; line-height:15px" onclick="awasthi2019aReview_Papers_and_BooksFunction()" id="awasthi2019aReview_Papers_and_Books_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="awasthi2019aReview_Papers_and_BooksFunction2()" id="awasthi2019aReview_Papers_and_Books_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="awasthi2019aReview_Papers_and_BooksFunction3()" id="awasthi2019aReview_Papers_and_Books_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="awasthi2019aReview_Papers_and_Books_more" style="display: none">
    @inproceedings{awasthi2019a,<br> author = {Awasthi, Abhijeet and Sarawagi, Sunita},<br> booktitle = {Proceedings of the ACM India Joint International Conference on Data Science and Management of Data},<br> doi = {10.1145},<br> pages = {362--365},<br> title = {Continual Learning with Neural Networks: A Review},<br> url = {https://dl.acm.org/doi/pdf/10.1145/3297001.3297062},<br> year = {2019}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="awasthi2019aReview_Papers_and_Books_more2" style="display: none">
    Continual learning broadly refers to the algorithms which aim to learn continuously over time across varying domains, tasks or data distributions. This is in contrast to algorithms restricted to learning a fixed number of tasks in a given domain, assuming a static data distribution. In this survey we aim to discuss a wide breadth of challenges faced in a continual learning setup and review existing work in the area. We discuss parameter regularization techniques to avoid catastrophic forgetting in neural networks followed by memory based approaches and the role of generative models in assisting continual learning algorithms. We discuss how dynamic neural networks assist continual learning by endowing neural networks with a new capacity to learn further. We conclude by discussing possible future directions.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="awasthi2019aReview_Papers_and_Books_more3" style="display: none">
    N.A.
</span></p>
<script>
    function awasthi2019aReview_Papers_and_BooksFunction() {
      var moreText = document.getElementById("awasthi2019aReview_Papers_and_Books_more");
      var moreText2 = document.getElementById("awasthi2019aReview_Papers_and_Books_more2");
      var moreText3 = document.getElementById("awasthi2019aReview_Papers_and_Books_more3");
      var btnText = document.getElementById("awasthi2019aReview_Papers_and_Books_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function awasthi2019aReview_Papers_and_BooksFunction2() {
      var moreText = document.getElementById("awasthi2019aReview_Papers_and_Books_more2");
      var moreText1 = document.getElementById("awasthi2019aReview_Papers_and_Books_more");
      var moreText3 = document.getElementById("awasthi2019aReview_Papers_and_Books_more3");
      var btnText = document.getElementById("awasthi2019aReview_Papers_and_Books_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function awasthi2019aReview_Papers_and_BooksFunction3() {
      var moreText = document.getElementById("awasthi2019aReview_Papers_and_Books_more3");
      var moreText1 = document.getElementById("awasthi2019aReview_Papers_and_Books_more");
      var moreText2 = document.getElementById("awasthi2019aReview_Papers_and_Books_more2");
      var btnText = document.getElementById("awasthi2019aReview_Papers_and_Books_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="http://www.sciencedirect.com/science/article/pii/S0893608019300231">Continual lifelong learning with neural networks: A review</a> by German I Parisi, Ronald Kemker, Jose L Part, Christopher Kanan and Stefan Wermter. <em>Neural Networks</em>, 54–71, 2019. <span style='background-color:Tomato; padding: 2px; border-radius:4px; border: 1px solid black;'>[framework]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Parisi2019aReview_Papers_and_BooksFunction()" id="Parisi2019aReview_Papers_and_Books_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Parisi2019aReview_Papers_and_BooksFunction2()" id="Parisi2019aReview_Papers_and_Books_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Parisi2019aReview_Papers_and_BooksFunction3()" id="Parisi2019aReview_Papers_and_Books_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Parisi2019aReview_Papers_and_Books_more" style="display: none">
    @article{Parisi2019a,<br> annote = {A general survey of the most important and used CL techniques, with a characterisation of the catastrophic forgetting phenomenon.},<br> author = {Parisi, German I and Kemker, Ronald and Part, Jose L and Kanan, Christopher and Wermter, Stefan},<br> doi = {10.1016/j.neunet.2019.01.012},<br> issn = {0893-6080},<br> journal = {Neural Networks},<br> keywords = {Catastrophic forgetting,Continual learning,Developmental systems,Lifelong learning,Memory consolidation,[framework]},<br> language = {en},<br> mendeley-tags = {[framework]},<br> month = {may},<br> pages = {54--71},<br> shorttitle = {Continual lifelong learning with neural networks},<br> title = {Continual lifelong learning with neural networks: A review},<br> url = {http://www.sciencedirect.com/science/article/pii/S0893608019300231},<br> volume = {113},<br> year = {2019}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Parisi2019aReview_Papers_and_Books_more2" style="display: none">
    Humans and animals have the ability to continually acquire, fine-tune, and transfer knowledge and skills throughout their lifespan. This ability, referred to as lifelong learning, is mediated by a rich set of neurocognitive mechanisms that together contribute to the development and specialization of our sensorimotor skills as well as to long-term memory consolidation and retrieval. Consequently, lifelong learning capabilities are crucial for computational learning systems and autonomous agents interacting in the real world and processing continuous streams of information. However, lifelong learning remains a long-standing challenge for machine learning and neural network models since the continual acquisition of incrementally available information from non-stationary data distributions generally leads to catastrophic forgetting or interference. This limitation represents a major drawback for state-of-the-art deep neural network models that typically learn representations from stationary batches of training data, thus without accounting for situations in which information becomes incrementally available over time. In this review, we critically summarize the main challenges linked to lifelong learning for artificial learning systems and compare existing neural network approaches that alleviate, to different extents, catastrophic forgetting. Although significant advances have been made in domain-specific learning with neural networks, extensive research efforts are required for the development of robust lifelong learning on autonomous agents and robots. We discuss well-established and emerging research motivated by lifelong learning factors in biological systems such as structural plasticity, memory replay, curriculum and transfer learning, intrinsic motivation, and multisensory integration.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Parisi2019aReview_Papers_and_Books_more3" style="display: none">
    A general survey of the most important and used CL techniques, with a characterisation of the catastrophic forgetting phenomenon.
</span></p>
<script>
    function Parisi2019aReview_Papers_and_BooksFunction() {
      var moreText = document.getElementById("Parisi2019aReview_Papers_and_Books_more");
      var moreText2 = document.getElementById("Parisi2019aReview_Papers_and_Books_more2");
      var moreText3 = document.getElementById("Parisi2019aReview_Papers_and_Books_more3");
      var btnText = document.getElementById("Parisi2019aReview_Papers_and_Books_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Parisi2019aReview_Papers_and_BooksFunction2() {
      var moreText = document.getElementById("Parisi2019aReview_Papers_and_Books_more2");
      var moreText1 = document.getElementById("Parisi2019aReview_Papers_and_Books_more");
      var moreText3 = document.getElementById("Parisi2019aReview_Papers_and_Books_more3");
      var btnText = document.getElementById("Parisi2019aReview_Papers_and_Books_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Parisi2019aReview_Papers_and_BooksFunction3() {
      var moreText = document.getElementById("Parisi2019aReview_Papers_and_Books_more3");
      var moreText1 = document.getElementById("Parisi2019aReview_Papers_and_Books_more");
      var moreText2 = document.getElementById("Parisi2019aReview_Papers_and_Books_more2");
      var btnText = document.getElementById("Parisi2019aReview_Papers_and_Books_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16410">Measuring Catastrophic Forgetting in Neural Networks</a> by Ronald Kemker, Marc McClure, Angelina Abitino, Tyler L Hayes and Christopher Kanan. <em>Thirty-Second AAAI Conference on Artificial Intelligence</em>, 2018. <span style='background-color:MediumAquaMarine; padding: 2px; border-radius:4px; border: 1px solid black;'>[mnist]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Kemker2018aReview_Papers_and_BooksFunction()" id="Kemker2018aReview_Papers_and_Books_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Kemker2018aReview_Papers_and_BooksFunction2()" id="Kemker2018aReview_Papers_and_Books_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Kemker2018aReview_Papers_and_BooksFunction3()" id="Kemker2018aReview_Papers_and_Books_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Kemker2018aReview_Papers_and_Books_more" style="display: none">
    @inproceedings{Kemker2018a,<br> author = {Kemker, Ronald and McClure, Marc and Abitino, Angelina and Hayes, Tyler L and Kanan, Christopher},<br> booktitle = {Thirty-Second AAAI Conference on Artificial Intelligence},<br> keywords = {[mnist],audioset,kemker,review,survey},<br> language = {en},<br> mendeley-tags = {[mnist]},<br> month = {apr},<br> title = {Measuring Catastrophic Forgetting in Neural Networks},<br> url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16410},<br> year = {2018}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Kemker2018aReview_Papers_and_Books_more2" style="display: none">
    Deep neural networks are used in many state-of-the-art systems for machine perception. Once a network is trained to do a specific task, e.g., bird classification, it cannot easily be trained to do new tasks, e.g., incrementally learning to recognize additional bird species or learning an entirely different task such as flower recognition. When new tasks are added, typical deep neural networks are prone to catastrophically forgetting previous tasks. Networks that are capable of assimilating new information incrementally, much like how humans form new memories over time, will be more efficient than re-training the model from scratch each time a new task needs to be learned. There have been multiple attempts to develop schemes that mitigate catastrophic forgetting, but these methods have not been directly compared, the tests used to evaluate them vary considerably, and these methods have only been evaluated on small-scale problems (e.g., MNIST). In this paper, we introduce new metrics and benchmarks for directly comparing five different mechanisms designed to mitigate catastrophic forgetting in neural networks: regularization, ensembling, rehearsal, dual-memory, and sparse-coding. Our experiments on real-world images and sounds show that the mechanism(s) that are critical for optimal performance vary based on the incremental training paradigm and type of data being used, but they all demonstrate that the catastrophic forgetting problem is not yet solved.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Kemker2018aReview_Papers_and_Books_more3" style="display: none">
    N.A.
</span></p>
<script>
    function Kemker2018aReview_Papers_and_BooksFunction() {
      var moreText = document.getElementById("Kemker2018aReview_Papers_and_Books_more");
      var moreText2 = document.getElementById("Kemker2018aReview_Papers_and_Books_more2");
      var moreText3 = document.getElementById("Kemker2018aReview_Papers_and_Books_more3");
      var btnText = document.getElementById("Kemker2018aReview_Papers_and_Books_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Kemker2018aReview_Papers_and_BooksFunction2() {
      var moreText = document.getElementById("Kemker2018aReview_Papers_and_Books_more2");
      var moreText1 = document.getElementById("Kemker2018aReview_Papers_and_Books_more");
      var moreText3 = document.getElementById("Kemker2018aReview_Papers_and_Books_more3");
      var btnText = document.getElementById("Kemker2018aReview_Papers_and_Books_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Kemker2018aReview_Papers_and_BooksFunction3() {
      var moreText = document.getElementById("Kemker2018aReview_Papers_and_Books_more3");
      var moreText1 = document.getElementById("Kemker2018aReview_Papers_and_Books_more");
      var moreText2 = document.getElementById("Kemker2018aReview_Papers_and_Books_more2");
      var btnText = document.getElementById("Kemker2018aReview_Papers_and_Books_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="https://gitlab.informatik.hs-fulda.de/ML-Projects/CF_in_DNNs">A comprehensive, application-oriented study of catastrophic forgetting in DNNs</a> by B Pfülb and A Gepperth. <em>ICLR</em>, 2018. <span style='background-color:LightGray; padding: 2px; border-radius:4px; border: 1px solid black;'>[fashion]</span> <span style='background-color:MediumAquaMarine; padding: 2px; border-radius:4px; border: 1px solid black;'>[mnist]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Pfulb2018aReview_Papers_and_BooksFunction()" id="Pfulb2018aReview_Papers_and_Books_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Pfulb2018aReview_Papers_and_BooksFunction2()" id="Pfulb2018aReview_Papers_and_Books_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Pfulb2018aReview_Papers_and_BooksFunction3()" id="Pfulb2018aReview_Papers_and_Books_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Pfulb2018aReview_Papers_and_Books_more" style="display: none">
    @inproceedings{Pfulb2018a,<br> author = {Pfülb, B and Gepperth, A},<br> booktitle = {ICLR},<br> file = {:home/andrea/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pfülb, Gepperth - 2018 - A COMPREHENSIVE, APPLICATION-ORIENTED STUDY OF CATASTROPHIC FORGETTING IN DNNS.pdf:pdf},<br> keywords = {[fashion],[mnist]},<br> mendeley-tags = {[fashion],[mnist]},<br> month = {sep},<br> title = {A comprehensive, application-oriented study of catastrophic forgetting in DNNs},<br> url = {https://gitlab.informatik.hs-fulda.de/ML-Projects/CF_in_DNNs},<br> year = {2018}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Pfulb2018aReview_Papers_and_Books_more2" style="display: none">
    We present a large-scale empirical study of catastrophic forgetting (CF) in modern Deep Neural Network (DNN) models that perform sequential (or: incremen-tal) learning. A new experimental protocol is proposed that enforces typical constraints encountered in application scenarios. As the investigation is empirical, we evaluate CF behavior on the hitherto largest number of visual classification datasets, from each of which we construct a representative number of Sequential Learning Tasks (SLTs) in close alignment to previous works on CF. Our results clearly indicate that there is no model that avoids CF for all investigated datasets and SLTs under application conditions. We conclude with a discussion of potential solutions and workarounds to CF, notably for the EWC and IMM models.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Pfulb2018aReview_Papers_and_Books_more3" style="display: none">
    N.A.
</span></p>
<script>
    function Pfulb2018aReview_Papers_and_BooksFunction() {
      var moreText = document.getElementById("Pfulb2018aReview_Papers_and_Books_more");
      var moreText2 = document.getElementById("Pfulb2018aReview_Papers_and_Books_more2");
      var moreText3 = document.getElementById("Pfulb2018aReview_Papers_and_Books_more3");
      var btnText = document.getElementById("Pfulb2018aReview_Papers_and_Books_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Pfulb2018aReview_Papers_and_BooksFunction2() {
      var moreText = document.getElementById("Pfulb2018aReview_Papers_and_Books_more2");
      var moreText1 = document.getElementById("Pfulb2018aReview_Papers_and_Books_more");
      var moreText3 = document.getElementById("Pfulb2018aReview_Papers_and_Books_more3");
      var btnText = document.getElementById("Pfulb2018aReview_Papers_and_Books_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Pfulb2018aReview_Papers_and_BooksFunction3() {
      var moreText = document.getElementById("Pfulb2018aReview_Papers_and_Books_more3");
      var moreText1 = document.getElementById("Pfulb2018aReview_Papers_and_Books_more");
      var moreText2 = document.getElementById("Pfulb2018aReview_Papers_and_Books_more2");
      var btnText = document.getElementById("Pfulb2018aReview_Papers_and_Books_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p>Avoiding Catastrophic Forgetting by  and Michael E. Hasselmo. <em>Trends in Cognitive Sciences</em>, 407–408, 2017.  <br>
<button style="font-size:75%; line-height:15px" onclick="Hasselmo2017Review_Papers_and_BooksFunction()" id="Hasselmo2017Review_Papers_and_Books_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Hasselmo2017Review_Papers_and_BooksFunction2()" id="Hasselmo2017Review_Papers_and_Books_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Hasselmo2017Review_Papers_and_BooksFunction3()" id="Hasselmo2017Review_Papers_and_Books_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Hasselmo2017Review_Papers_and_Books_more" style="display: none">
    @article{Hasselmo2017,<br> author = {Hasselmo, Michael E.},<br> doi = {10.1016/j.tics.2017.04.001},<br> file = {::},<br> issn = {1879307X},<br> journal = {Trends in Cognitive Sciences},<br> keywords = {hasselmo2017a},<br> month = {jun},<br> number = {6},<br> pages = {407--408},<br> publisher = {Elsevier Ltd},<br> title = {Avoiding Catastrophic Forgetting},<br> volume = {21},<br> year = {2017}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Hasselmo2017Review_Papers_and_Books_more2" style="display: none">
    Humans regularly perform new learning without losing memory for previous information, but neural network models suffer from the phenomenon of catastrophic forgetting in which new learning impairs prior function. A recent article presents an algorithm that spares learning at synapses important for previously learned function, reducing catastrophic forgetting.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Hasselmo2017Review_Papers_and_Books_more3" style="display: none">
    N.A.
</span></p>
<script>
    function Hasselmo2017Review_Papers_and_BooksFunction() {
      var moreText = document.getElementById("Hasselmo2017Review_Papers_and_Books_more");
      var moreText2 = document.getElementById("Hasselmo2017Review_Papers_and_Books_more2");
      var moreText3 = document.getElementById("Hasselmo2017Review_Papers_and_Books_more3");
      var btnText = document.getElementById("Hasselmo2017Review_Papers_and_Books_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Hasselmo2017Review_Papers_and_BooksFunction2() {
      var moreText = document.getElementById("Hasselmo2017Review_Papers_and_Books_more2");
      var moreText1 = document.getElementById("Hasselmo2017Review_Papers_and_Books_more");
      var moreText3 = document.getElementById("Hasselmo2017Review_Papers_and_Books_more3");
      var btnText = document.getElementById("Hasselmo2017Review_Papers_and_Books_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Hasselmo2017Review_Papers_and_BooksFunction3() {
      var moreText = document.getElementById("Hasselmo2017Review_Papers_and_Books_more3");
      var moreText1 = document.getElementById("Hasselmo2017Review_Papers_and_Books_more");
      var moreText2 = document.getElementById("Hasselmo2017Review_Papers_and_Books_more2");
      var btnText = document.getElementById("Hasselmo2017Review_Papers_and_Books_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="http://www.tandfonline.com/doi/abs/10.1080/09540099550039318">Catastrophic Forgetting; Catastrophic Interference; Stability; Plasticity; Rehearsal.</a> by  and Anthony Robins. <em>Connection Science</em>, 123–146, 1995. <span style='background-color:green; padding: 2px; border-radius:4px; border: 1px solid black;'>[dual]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Robins1995aReview_Papers_and_BooksFunction()" id="Robins1995aReview_Papers_and_Books_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Robins1995aReview_Papers_and_BooksFunction2()" id="Robins1995aReview_Papers_and_Books_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Robins1995aReview_Papers_and_BooksFunction3()" id="Robins1995aReview_Papers_and_Books_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Robins1995aReview_Papers_and_Books_more" style="display: none">
    @article{Robins1995a,<br> annote = {An in-depth overview of rehearsal techniques, including pseudo-rehearsal. Useful to combine these ideas in dual systems, in which one system learns a task while the other provides rehearsal of previous patterns.},<br> author = {Robins, Anthony},<br> doi = {10.1080/09540099550039318},<br> issn = {0954-0091, 1360-0494},<br> journal = {Connection Science},<br> keywords = {[dual]},<br> language = {en},<br> mendeley-tags = {[dual]},<br> month = {jun},<br> number = {2},<br> pages = {123--146},<br> title = {Catastrophic Forgetting; Catastrophic Interference; Stability; Plasticity; Rehearsal.},<br> url = {http://www.tandfonline.com/doi/abs/10.1080/09540099550039318},<br> volume = {7},<br> year = {1995}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Robins1995aReview_Papers_and_Books_more2" style="display: none">
    This paper reviews the problem of catastrophic forgetting (the loss or disruption of previously learned information when new information is learned) in neural networks, and explores rehearsal mechanisms (the retraining of some of the previously learned information as the new information is added) as a potential solution. W e replicate some of the experiments described by Ratcliff (1990), including those relating to a simple `recency' based rehearsal regime. We then develop further rehearsal regimes which are more effective than recency rehearsal. In particular, `sweep rehearsal' is very successful at minimizing catastrophic forgetting. One possible limitation of rehearsal in general, however, is that previously learned information may not be available for retraining. W e describe a solution to this problem, `pseudorehearsal' , a method which provides the advantages of rehearsal without actually requiring any access to the previously learned information (the original training population) itself. We then suggest an interpretation of these rehearsal mechanisms in the context of a function approximation based account of neural network learning. Both rehearsal and pseudorehearsal may have practical applications, allowing new information to be integrated into an existing network with minimum disruption of old informa tion.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Robins1995aReview_Papers_and_Books_more3" style="display: none">
    An in-depth overview of rehearsal techniques, including pseudo-rehearsal. Useful to combine these ideas in dual systems, in which one system learns a task while the other provides rehearsal of previous patterns.
</span></p>
<script>
    function Robins1995aReview_Papers_and_BooksFunction() {
      var moreText = document.getElementById("Robins1995aReview_Papers_and_Books_more");
      var moreText2 = document.getElementById("Robins1995aReview_Papers_and_Books_more2");
      var moreText3 = document.getElementById("Robins1995aReview_Papers_and_Books_more3");
      var btnText = document.getElementById("Robins1995aReview_Papers_and_Books_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Robins1995aReview_Papers_and_BooksFunction2() {
      var moreText = document.getElementById("Robins1995aReview_Papers_and_Books_more2");
      var moreText1 = document.getElementById("Robins1995aReview_Papers_and_Books_more");
      var moreText3 = document.getElementById("Robins1995aReview_Papers_and_Books_more3");
      var btnText = document.getElementById("Robins1995aReview_Papers_and_Books_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Robins1995aReview_Papers_and_BooksFunction3() {
      var moreText = document.getElementById("Robins1995aReview_Papers_and_Books_more3");
      var moreText1 = document.getElementById("Robins1995aReview_Papers_and_Books_more");
      var moreText2 = document.getElementById("Robins1995aReview_Papers_and_Books_more2");
      var btnText = document.getElementById("Robins1995aReview_Papers_and_Books_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
</ul>
</div>
<div class="section" id="catastrophic-forgetting-studies">
<h3>Catastrophic Forgetting Studies<a class="headerlink" href="#catastrophic-forgetting-studies" title="Permalink to this headline">¶</a></h3>
<p>In this section we list all the major contributions trying to understand catastrophic forgetting and its implication in machines that learn continually.</p>
<ul class="simple">
<li><p><a class="reference external" href="http://arxiv.org/abs/2001.01578">Dissecting Catastrophic Forgetting in Continual Learning by Deep Visualization</a> by Giang Nguyen, Shuan Chen, Thao Do, Tae Joon Jun, Ho-Jin Choi and Daeyoung Kim. <em>arXiv</em>, 2020. <span style='background-color:#E5E4E2; padding: 2px; border-radius:4px; border: 1px solid black;'>[vision]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Nguyen2020aCatastrophic_Forgetting_StudiesFunction()" id="Nguyen2020aCatastrophic_Forgetting_Studies_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Nguyen2020aCatastrophic_Forgetting_StudiesFunction2()" id="Nguyen2020aCatastrophic_Forgetting_Studies_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Nguyen2020aCatastrophic_Forgetting_StudiesFunction3()" id="Nguyen2020aCatastrophic_Forgetting_Studies_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Nguyen2020aCatastrophic_Forgetting_Studies_more" style="display: none">
    @article{Nguyen2020a,<br> archiveprefix = {arXiv},<br> arxivid = {2001.01578},<br> author = {Nguyen, Giang and Chen, Shuan and Do, Thao and Jun, Tae Joon and Choi, Ho-Jin and Kim, Daeyoung},<br> eprint = {2001.01578},<br> journal = {arXiv},<br> keywords = {[vision]},<br> mendeley-tags = {[vision]},<br> title = {Dissecting Catastrophic Forgetting in Continual Learning by Deep Visualization},<br> url = {http://arxiv.org/abs/2001.01578},<br> year = {2020}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Nguyen2020aCatastrophic_Forgetting_Studies_more2" style="display: none">
    Interpreting the behaviors of Deep Neural Networks (usually considered as a black box) is critical especially when they are now being widely adopted over diverse aspects of human life. Taking the advancements from Explainable Artificial Intelligent, this paper proposes a novel technique called Auto DeepVis to dissect catastrophic forgetting in continual learning. A new method to deal with catastrophic forgetting named critical freezing is also introduced upon investigating the dilemma by Auto DeepVis. Experiments on a captioning model meticulously present how catastrophic forgetting happens, particularly showing which components are forgetting or changing. The effectiveness of our technique is then assessed; and more precisely, critical freezing claims the best performance on both previous and coming tasks over baselines, proving the capability of the investigation. Our techniques could not only be supplementary to existing solutions for completely eradicating catastrophic forgetting for life-long learning but also explainable.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Nguyen2020aCatastrophic_Forgetting_Studies_more3" style="display: none">
    N.A.
</span></p>
<script>
    function Nguyen2020aCatastrophic_Forgetting_StudiesFunction() {
      var moreText = document.getElementById("Nguyen2020aCatastrophic_Forgetting_Studies_more");
      var moreText2 = document.getElementById("Nguyen2020aCatastrophic_Forgetting_Studies_more2");
      var moreText3 = document.getElementById("Nguyen2020aCatastrophic_Forgetting_Studies_more3");
      var btnText = document.getElementById("Nguyen2020aCatastrophic_Forgetting_Studies_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Nguyen2020aCatastrophic_Forgetting_StudiesFunction2() {
      var moreText = document.getElementById("Nguyen2020aCatastrophic_Forgetting_Studies_more2");
      var moreText1 = document.getElementById("Nguyen2020aCatastrophic_Forgetting_Studies_more");
      var moreText3 = document.getElementById("Nguyen2020aCatastrophic_Forgetting_Studies_more3");
      var btnText = document.getElementById("Nguyen2020aCatastrophic_Forgetting_Studies_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Nguyen2020aCatastrophic_Forgetting_StudiesFunction3() {
      var moreText = document.getElementById("Nguyen2020aCatastrophic_Forgetting_Studies_more3");
      var moreText1 = document.getElementById("Nguyen2020aCatastrophic_Forgetting_Studies_more");
      var moreText2 = document.getElementById("Nguyen2020aCatastrophic_Forgetting_Studies_more2");
      var btnText = document.getElementById("Nguyen2020aCatastrophic_Forgetting_Studies_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="https://openreview.net/forum?id=BJlxm30cKm">An Empirical Study of Example Forgetting during Deep Neural Network Learning</a> by Mariya Toneva, Alessandro Sordoni, Remi Tachet des Combes, Adam Trischler, Yoshua Bengio and Geoffrey J Gordon. <em>International Conference on Learning Representations</em>, 2019. <span style='background-color:DarkCyan; padding: 2px; border-radius:4px; border: 1px solid black;'>[cifar]</span> <span style='background-color:MediumAquaMarine; padding: 2px; border-radius:4px; border: 1px solid black;'>[mnist]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Toneva2018aCatastrophic_Forgetting_StudiesFunction()" id="Toneva2018aCatastrophic_Forgetting_Studies_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Toneva2018aCatastrophic_Forgetting_StudiesFunction2()" id="Toneva2018aCatastrophic_Forgetting_Studies_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Toneva2018aCatastrophic_Forgetting_StudiesFunction3()" id="Toneva2018aCatastrophic_Forgetting_Studies_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Toneva2018aCatastrophic_Forgetting_Studies_more" style="display: none">
    @inproceedings{Toneva2018a,<br> annote = {An interesting aspect of this paper is related to the study of unforgettable patterns and how they influence performance in terms of forgetting.},<br> author = {Toneva, Mariya and Sordoni, Alessandro and des Combes, Remi Tachet and Trischler, Adam and Bengio, Yoshua and Gordon, Geoffrey J},<br> booktitle = {International Conference on Learning Representations},<br> keywords = {[cifar],[mnist]},<br> mendeley-tags = {[cifar],[mnist]},<br> month = {sep},<br> title = {An Empirical Study of Example Forgetting during Deep Neural Network Learning},<br> url = {https://openreview.net/forum?id=BJlxm30cKm},<br> year = {2019}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Toneva2018aCatastrophic_Forgetting_Studies_more2" style="display: none">
    Inspired by the phenomenon of catastrophic forgetting, we investigate the learning dynamics of neural networks as they train on single classification tasks. Our goal is to understand whether a...
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Toneva2018aCatastrophic_Forgetting_Studies_more3" style="display: none">
    An interesting aspect of this paper is related to the study of unforgettable patterns and how they influence performance in terms of forgetting.
</span></p>
<script>
    function Toneva2018aCatastrophic_Forgetting_StudiesFunction() {
      var moreText = document.getElementById("Toneva2018aCatastrophic_Forgetting_Studies_more");
      var moreText2 = document.getElementById("Toneva2018aCatastrophic_Forgetting_Studies_more2");
      var moreText3 = document.getElementById("Toneva2018aCatastrophic_Forgetting_Studies_more3");
      var btnText = document.getElementById("Toneva2018aCatastrophic_Forgetting_Studies_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Toneva2018aCatastrophic_Forgetting_StudiesFunction2() {
      var moreText = document.getElementById("Toneva2018aCatastrophic_Forgetting_Studies_more2");
      var moreText1 = document.getElementById("Toneva2018aCatastrophic_Forgetting_Studies_more");
      var moreText3 = document.getElementById("Toneva2018aCatastrophic_Forgetting_Studies_more3");
      var btnText = document.getElementById("Toneva2018aCatastrophic_Forgetting_Studies_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Toneva2018aCatastrophic_Forgetting_StudiesFunction3() {
      var moreText = document.getElementById("Toneva2018aCatastrophic_Forgetting_Studies_more3");
      var moreText1 = document.getElementById("Toneva2018aCatastrophic_Forgetting_Studies_more");
      var moreText2 = document.getElementById("Toneva2018aCatastrophic_Forgetting_Studies_more2");
      var btnText = document.getElementById("Toneva2018aCatastrophic_Forgetting_Studies_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="http://arxiv.org/abs/1810.13166">Don’t forget, there is more than forgetting: new metrics for Continual Learning</a> by Natalia Díaz-Rodrǵuez, Vincenzo Lomonaco, David Filliat and Davide Maltoni. <em>arXiv</em>, 2018. <span style='background-color:DarkCyan; padding: 2px; border-radius:4px; border: 1px solid black;'>[cifar]</span> <span style='background-color:Tomato; padding: 2px; border-radius:4px; border: 1px solid black;'>[framework]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="DiazRodriguez2018aCatastrophic_Forgetting_StudiesFunction()" id="DiazRodriguez2018aCatastrophic_Forgetting_Studies_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="DiazRodriguez2018aCatastrophic_Forgetting_StudiesFunction2()" id="DiazRodriguez2018aCatastrophic_Forgetting_Studies_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="DiazRodriguez2018aCatastrophic_Forgetting_StudiesFunction3()" id="DiazRodriguez2018aCatastrophic_Forgetting_Studies_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="DiazRodriguez2018aCatastrophic_Forgetting_Studies_more" style="display: none">
    @article{Diaz-Rodriguez2018a,<br> annote = {arXiv: 1810.13166},<br> author = {Díaz-Rodr\ǵuez, Natalia and Lomonaco, Vincenzo and Filliat, David and Maltoni, Davide},<br> journal = {arXiv},<br> keywords = {68T05,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Rec,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computi,[cifar],[framework],cs.AI,cs.CV,cs.LG,cs.NE,stat.ML},<br> mendeley-tags = {[cifar],[framework]},<br> month = {oct},<br> shorttitle = {Don't forget, there is more than forgetting},<br> title = {Don't forget, there is more than forgetting: new metrics for Continual Learning},<br> url = {http://arxiv.org/abs/1810.13166},<br> year = {2018}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="DiazRodriguez2018aCatastrophic_Forgetting_Studies_more2" style="display: none">
    Continual learning consists of algorithms that learn from a stream of data/tasks continuously and adaptively thought time, enabling the incremental development of ever more complex knowledge and skills. The lack of consensus in evaluating continual learning algorithms and the almost exclusive focus on forgetting motivate us to propose a more comprehensive set of implementation independent metrics accounting for several factors we believe have practical implications worth considering in the deployment of real AI systems that learn continually: accuracy or performance over time, backward and forward knowledge transfer, memory overhead as well as computational efficiency. Drawing inspiration from the standard Multi-Attribute Value Theory (MAVT) we further propose to fuse these metrics into a single score for ranking purposes and we evaluate our proposal with five continual learning strategies on the iCIFAR-100 continual learning benchmark.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="DiazRodriguez2018aCatastrophic_Forgetting_Studies_more3" style="display: none">
    arXiv: 1810.13166
</span></p>
<script>
    function DiazRodriguez2018aCatastrophic_Forgetting_StudiesFunction() {
      var moreText = document.getElementById("DiazRodriguez2018aCatastrophic_Forgetting_Studies_more");
      var moreText2 = document.getElementById("DiazRodriguez2018aCatastrophic_Forgetting_Studies_more2");
      var moreText3 = document.getElementById("DiazRodriguez2018aCatastrophic_Forgetting_Studies_more3");
      var btnText = document.getElementById("DiazRodriguez2018aCatastrophic_Forgetting_Studies_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function DiazRodriguez2018aCatastrophic_Forgetting_StudiesFunction2() {
      var moreText = document.getElementById("DiazRodriguez2018aCatastrophic_Forgetting_Studies_more2");
      var moreText1 = document.getElementById("DiazRodriguez2018aCatastrophic_Forgetting_Studies_more");
      var moreText3 = document.getElementById("DiazRodriguez2018aCatastrophic_Forgetting_Studies_more3");
      var btnText = document.getElementById("DiazRodriguez2018aCatastrophic_Forgetting_Studies_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function DiazRodriguez2018aCatastrophic_Forgetting_StudiesFunction3() {
      var moreText = document.getElementById("DiazRodriguez2018aCatastrophic_Forgetting_Studies_more3");
      var moreText1 = document.getElementById("DiazRodriguez2018aCatastrophic_Forgetting_Studies_more");
      var moreText2 = document.getElementById("DiazRodriguez2018aCatastrophic_Forgetting_Studies_more2");
      var btnText = document.getElementById("DiazRodriguez2018aCatastrophic_Forgetting_Studies_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16410">Measuring Catastrophic Forgetting in Neural Networks</a> by Ronald Kemker, Marc McClure, Angelina Abitino, Tyler L Hayes and Christopher Kanan. <em>Thirty-Second AAAI Conference on Artificial Intelligence</em>, 2018. <span style='background-color:MediumAquaMarine; padding: 2px; border-radius:4px; border: 1px solid black;'>[mnist]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Kemker2018aCatastrophic_Forgetting_StudiesFunction()" id="Kemker2018aCatastrophic_Forgetting_Studies_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Kemker2018aCatastrophic_Forgetting_StudiesFunction2()" id="Kemker2018aCatastrophic_Forgetting_Studies_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Kemker2018aCatastrophic_Forgetting_StudiesFunction3()" id="Kemker2018aCatastrophic_Forgetting_Studies_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Kemker2018aCatastrophic_Forgetting_Studies_more" style="display: none">
    @inproceedings{Kemker2018a,<br> author = {Kemker, Ronald and McClure, Marc and Abitino, Angelina and Hayes, Tyler L and Kanan, Christopher},<br> booktitle = {Thirty-Second AAAI Conference on Artificial Intelligence},<br> keywords = {[mnist],audioset,kemker,review,survey},<br> language = {en},<br> mendeley-tags = {[mnist]},<br> month = {apr},<br> title = {Measuring Catastrophic Forgetting in Neural Networks},<br> url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16410},<br> year = {2018}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Kemker2018aCatastrophic_Forgetting_Studies_more2" style="display: none">
    Deep neural networks are used in many state-of-the-art systems for machine perception. Once a network is trained to do a specific task, e.g., bird classification, it cannot easily be trained to do new tasks, e.g., incrementally learning to recognize additional bird species or learning an entirely different task such as flower recognition. When new tasks are added, typical deep neural networks are prone to catastrophically forgetting previous tasks. Networks that are capable of assimilating new information incrementally, much like how humans form new memories over time, will be more efficient than re-training the model from scratch each time a new task needs to be learned. There have been multiple attempts to develop schemes that mitigate catastrophic forgetting, but these methods have not been directly compared, the tests used to evaluate them vary considerably, and these methods have only been evaluated on small-scale problems (e.g., MNIST). In this paper, we introduce new metrics and benchmarks for directly comparing five different mechanisms designed to mitigate catastrophic forgetting in neural networks: regularization, ensembling, rehearsal, dual-memory, and sparse-coding. Our experiments on real-world images and sounds show that the mechanism(s) that are critical for optimal performance vary based on the incremental training paradigm and type of data being used, but they all demonstrate that the catastrophic forgetting problem is not yet solved.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Kemker2018aCatastrophic_Forgetting_Studies_more3" style="display: none">
    N.A.
</span></p>
<script>
    function Kemker2018aCatastrophic_Forgetting_StudiesFunction() {
      var moreText = document.getElementById("Kemker2018aCatastrophic_Forgetting_Studies_more");
      var moreText2 = document.getElementById("Kemker2018aCatastrophic_Forgetting_Studies_more2");
      var moreText3 = document.getElementById("Kemker2018aCatastrophic_Forgetting_Studies_more3");
      var btnText = document.getElementById("Kemker2018aCatastrophic_Forgetting_Studies_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Kemker2018aCatastrophic_Forgetting_StudiesFunction2() {
      var moreText = document.getElementById("Kemker2018aCatastrophic_Forgetting_Studies_more2");
      var moreText1 = document.getElementById("Kemker2018aCatastrophic_Forgetting_Studies_more");
      var moreText3 = document.getElementById("Kemker2018aCatastrophic_Forgetting_Studies_more3");
      var btnText = document.getElementById("Kemker2018aCatastrophic_Forgetting_Studies_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Kemker2018aCatastrophic_Forgetting_StudiesFunction3() {
      var moreText = document.getElementById("Kemker2018aCatastrophic_Forgetting_Studies_more3");
      var moreText1 = document.getElementById("Kemker2018aCatastrophic_Forgetting_Studies_more");
      var moreText2 = document.getElementById("Kemker2018aCatastrophic_Forgetting_Studies_more2");
      var btnText = document.getElementById("Kemker2018aCatastrophic_Forgetting_Studies_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="https://gitlab.informatik.hs-fulda.de/ML-Projects/CF_in_DNNs">A comprehensive, application-oriented study of catastrophic forgetting in DNNs</a> by B Pfülb and A Gepperth. <em>ICLR</em>, 2018. <span style='background-color:LightGray; padding: 2px; border-radius:4px; border: 1px solid black;'>[fashion]</span> <span style='background-color:MediumAquaMarine; padding: 2px; border-radius:4px; border: 1px solid black;'>[mnist]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Pfulb2018aCatastrophic_Forgetting_StudiesFunction()" id="Pfulb2018aCatastrophic_Forgetting_Studies_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Pfulb2018aCatastrophic_Forgetting_StudiesFunction2()" id="Pfulb2018aCatastrophic_Forgetting_Studies_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Pfulb2018aCatastrophic_Forgetting_StudiesFunction3()" id="Pfulb2018aCatastrophic_Forgetting_Studies_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Pfulb2018aCatastrophic_Forgetting_Studies_more" style="display: none">
    @inproceedings{Pfulb2018a,<br> author = {Pfülb, B and Gepperth, A},<br> booktitle = {ICLR},<br> file = {:home/andrea/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pfülb, Gepperth - 2018 - A COMPREHENSIVE, APPLICATION-ORIENTED STUDY OF CATASTROPHIC FORGETTING IN DNNS.pdf:pdf},<br> keywords = {[fashion],[mnist]},<br> mendeley-tags = {[fashion],[mnist]},<br> month = {sep},<br> title = {A comprehensive, application-oriented study of catastrophic forgetting in DNNs},<br> url = {https://gitlab.informatik.hs-fulda.de/ML-Projects/CF_in_DNNs},<br> year = {2018}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Pfulb2018aCatastrophic_Forgetting_Studies_more2" style="display: none">
    We present a large-scale empirical study of catastrophic forgetting (CF) in modern Deep Neural Network (DNN) models that perform sequential (or: incremen-tal) learning. A new experimental protocol is proposed that enforces typical constraints encountered in application scenarios. As the investigation is empirical, we evaluate CF behavior on the hitherto largest number of visual classification datasets, from each of which we construct a representative number of Sequential Learning Tasks (SLTs) in close alignment to previous works on CF. Our results clearly indicate that there is no model that avoids CF for all investigated datasets and SLTs under application conditions. We conclude with a discussion of potential solutions and workarounds to CF, notably for the EWC and IMM models.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Pfulb2018aCatastrophic_Forgetting_Studies_more3" style="display: none">
    N.A.
</span></p>
<script>
    function Pfulb2018aCatastrophic_Forgetting_StudiesFunction() {
      var moreText = document.getElementById("Pfulb2018aCatastrophic_Forgetting_Studies_more");
      var moreText2 = document.getElementById("Pfulb2018aCatastrophic_Forgetting_Studies_more2");
      var moreText3 = document.getElementById("Pfulb2018aCatastrophic_Forgetting_Studies_more3");
      var btnText = document.getElementById("Pfulb2018aCatastrophic_Forgetting_Studies_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Pfulb2018aCatastrophic_Forgetting_StudiesFunction2() {
      var moreText = document.getElementById("Pfulb2018aCatastrophic_Forgetting_Studies_more2");
      var moreText1 = document.getElementById("Pfulb2018aCatastrophic_Forgetting_Studies_more");
      var moreText3 = document.getElementById("Pfulb2018aCatastrophic_Forgetting_Studies_more3");
      var btnText = document.getElementById("Pfulb2018aCatastrophic_Forgetting_Studies_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Pfulb2018aCatastrophic_Forgetting_StudiesFunction3() {
      var moreText = document.getElementById("Pfulb2018aCatastrophic_Forgetting_Studies_more3");
      var moreText1 = document.getElementById("Pfulb2018aCatastrophic_Forgetting_Studies_more");
      var moreText2 = document.getElementById("Pfulb2018aCatastrophic_Forgetting_Studies_more2");
      var btnText = document.getElementById("Pfulb2018aCatastrophic_Forgetting_Studies_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(99)01294-2">Catastrophic forgetting in connectionist networks</a> by  and Robert French. <em>Trends in Cognitive Sciences</em>, 128–135, 1999. <span style='background-color:DarkSalmon; padding: 2px; border-radius:4px; border: 1px solid black;'>[sparsity]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="French1999aCatastrophic_Forgetting_StudiesFunction()" id="French1999aCatastrophic_Forgetting_Studies_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="French1999aCatastrophic_Forgetting_StudiesFunction2()" id="French1999aCatastrophic_Forgetting_Studies_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="French1999aCatastrophic_Forgetting_StudiesFunction3()" id="French1999aCatastrophic_Forgetting_Studies_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="French1999aCatastrophic_Forgetting_Studies_more" style="display: none">
    @article{French1999a,<br> author = {French, Robert},<br> doi = {10.1016/S1364-6613(99)01294-2},<br> issn = {1364-6613, 1879-307X},<br> journal = {Trends in Cognitive Sciences},<br> keywords = {Catastrophic forgetting,Connectionism,Connectionist networks,Interference,Learning,Memory,Neuroscience,[sparsity],biology},<br> language = {English},<br> mendeley-tags = {[sparsity]},<br> month = {apr},<br> number = {4},<br> pages = {128--135},<br> pmid = {10322466},<br> title = {Catastrophic forgetting in connectionist networks},<br> url = {https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(99)01294-2},<br> volume = {3},<br> year = {1999}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="French1999aCatastrophic_Forgetting_Studies_more2" style="display: none">
    N.A.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="French1999aCatastrophic_Forgetting_Studies_more3" style="display: none">
    N.A.
</span></p>
<script>
    function French1999aCatastrophic_Forgetting_StudiesFunction() {
      var moreText = document.getElementById("French1999aCatastrophic_Forgetting_Studies_more");
      var moreText2 = document.getElementById("French1999aCatastrophic_Forgetting_Studies_more2");
      var moreText3 = document.getElementById("French1999aCatastrophic_Forgetting_Studies_more3");
      var btnText = document.getElementById("French1999aCatastrophic_Forgetting_Studies_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function French1999aCatastrophic_Forgetting_StudiesFunction2() {
      var moreText = document.getElementById("French1999aCatastrophic_Forgetting_Studies_more2");
      var moreText1 = document.getElementById("French1999aCatastrophic_Forgetting_Studies_more");
      var moreText3 = document.getElementById("French1999aCatastrophic_Forgetting_Studies_more3");
      var btnText = document.getElementById("French1999aCatastrophic_Forgetting_Studies_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function French1999aCatastrophic_Forgetting_StudiesFunction3() {
      var moreText = document.getElementById("French1999aCatastrophic_Forgetting_Studies_more3");
      var moreText1 = document.getElementById("French1999aCatastrophic_Forgetting_Studies_more");
      var moreText2 = document.getElementById("French1999aCatastrophic_Forgetting_Studies_more2");
      var btnText = document.getElementById("French1999aCatastrophic_Forgetting_Studies_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="http://www.tandfonline.com/doi/abs/10.1080/09540099550039318">Catastrophic Forgetting; Catastrophic Interference; Stability; Plasticity; Rehearsal.</a> by  and Anthony Robins. <em>Connection Science</em>, 123–146, 1995. <span style='background-color:green; padding: 2px; border-radius:4px; border: 1px solid black;'>[dual]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Robins1995aCatastrophic_Forgetting_StudiesFunction()" id="Robins1995aCatastrophic_Forgetting_Studies_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Robins1995aCatastrophic_Forgetting_StudiesFunction2()" id="Robins1995aCatastrophic_Forgetting_Studies_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Robins1995aCatastrophic_Forgetting_StudiesFunction3()" id="Robins1995aCatastrophic_Forgetting_Studies_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Robins1995aCatastrophic_Forgetting_Studies_more" style="display: none">
    @article{Robins1995a,<br> annote = {An in-depth overview of rehearsal techniques, including pseudo-rehearsal. Useful to combine these ideas in dual systems, in which one system learns a task while the other provides rehearsal of previous patterns.},<br> author = {Robins, Anthony},<br> doi = {10.1080/09540099550039318},<br> issn = {0954-0091, 1360-0494},<br> journal = {Connection Science},<br> keywords = {[dual]},<br> language = {en},<br> mendeley-tags = {[dual]},<br> month = {jun},<br> number = {2},<br> pages = {123--146},<br> title = {Catastrophic Forgetting; Catastrophic Interference; Stability; Plasticity; Rehearsal.},<br> url = {http://www.tandfonline.com/doi/abs/10.1080/09540099550039318},<br> volume = {7},<br> year = {1995}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Robins1995aCatastrophic_Forgetting_Studies_more2" style="display: none">
    This paper reviews the problem of catastrophic forgetting (the loss or disruption of previously learned information when new information is learned) in neural networks, and explores rehearsal mechanisms (the retraining of some of the previously learned information as the new information is added) as a potential solution. W e replicate some of the experiments described by Ratcliff (1990), including those relating to a simple `recency' based rehearsal regime. We then develop further rehearsal regimes which are more effective than recency rehearsal. In particular, `sweep rehearsal' is very successful at minimizing catastrophic forgetting. One possible limitation of rehearsal in general, however, is that previously learned information may not be available for retraining. W e describe a solution to this problem, `pseudorehearsal' , a method which provides the advantages of rehearsal without actually requiring any access to the previously learned information (the original training population) itself. We then suggest an interpretation of these rehearsal mechanisms in the context of a function approximation based account of neural network learning. Both rehearsal and pseudorehearsal may have practical applications, allowing new information to be integrated into an existing network with minimum disruption of old informa tion.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Robins1995aCatastrophic_Forgetting_Studies_more3" style="display: none">
    An in-depth overview of rehearsal techniques, including pseudo-rehearsal. Useful to combine these ideas in dual systems, in which one system learns a task while the other provides rehearsal of previous patterns.
</span></p>
<script>
    function Robins1995aCatastrophic_Forgetting_StudiesFunction() {
      var moreText = document.getElementById("Robins1995aCatastrophic_Forgetting_Studies_more");
      var moreText2 = document.getElementById("Robins1995aCatastrophic_Forgetting_Studies_more2");
      var moreText3 = document.getElementById("Robins1995aCatastrophic_Forgetting_Studies_more3");
      var btnText = document.getElementById("Robins1995aCatastrophic_Forgetting_Studies_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Robins1995aCatastrophic_Forgetting_StudiesFunction2() {
      var moreText = document.getElementById("Robins1995aCatastrophic_Forgetting_Studies_more2");
      var moreText1 = document.getElementById("Robins1995aCatastrophic_Forgetting_Studies_more");
      var moreText3 = document.getElementById("Robins1995aCatastrophic_Forgetting_Studies_more3");
      var btnText = document.getElementById("Robins1995aCatastrophic_Forgetting_Studies_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Robins1995aCatastrophic_Forgetting_StudiesFunction3() {
      var moreText = document.getElementById("Robins1995aCatastrophic_Forgetting_Studies_more3");
      var moreText1 = document.getElementById("Robins1995aCatastrophic_Forgetting_Studies_more");
      var moreText2 = document.getElementById("Robins1995aCatastrophic_Forgetting_Studies_more2");
      var btnText = document.getElementById("Robins1995aCatastrophic_Forgetting_Studies_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
</ul>
</div>
<div class="section" id="benchmarks">
<h3>Benchmarks<a class="headerlink" href="#benchmarks" title="Permalink to this headline">¶</a></h3>
<p>In this section we list all the papers related to new benchmarks proposals for continual learning and related topics.</p>
<ul class="simple">
<li><p><a class="reference external" href="http://proceedings.mlr.press/v78/lomonaco17a.html">CORe50: a New Dataset and Benchmark for Continuous Object Recognition</a> by Vincenzo Lomonaco and Davide Maltoni. <em>Proceedings of the 1st Annual Conference on Robot Learning</em>, 17–26, 2017. <span style='background-color:#E5E4E2; padding: 2px; border-radius:4px; border: 1px solid black;'>[vision]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="lomonaco2017aBenchmarksFunction()" id="lomonaco2017aBenchmarks_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="lomonaco2017aBenchmarksFunction2()" id="lomonaco2017aBenchmarks_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="lomonaco2017aBenchmarksFunction3()" id="lomonaco2017aBenchmarks_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="lomonaco2017aBenchmarks_more" style="display: none">
    @inproceedings{lomonaco2017a,<br> author = {Lomonaco, Vincenzo and Maltoni, Davide},<br> booktitle = {Proceedings of the 1st Annual Conference on Robot Learning},<br> editor = {Levine, Sergey and Vanhoucke, Vincent and Goldberg, Ken},<br> keywords = {[vision]},<br> mendeley-tags = {[vision]},<br> month = {may},<br> pages = {17--26},<br> publisher = {PMLR},<br> series = {Proceedings of Machine Learning Research},<br> title = {CORe50: a New Dataset and Benchmark for Continuous Object Recognition},<br> url = {http://proceedings.mlr.press/v78/lomonaco17a.html},<br> volume = {78},<br> year = {2017}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="lomonaco2017aBenchmarks_more2" style="display: none">
    Continuous/Lifelong learning of high-dimensional data streams is a challenging research problem. In fact, fully retraining models each time new data become available is infeasible, due to computational and storage issues, while näive incremental strategies have been shown to suffer from catastrophic forgetting. In the context of real-world object recognition applications (e.g., robotic vision), where continuous learning is crucial, very few datasets and benchmarks are available to evaluate and compare emerging techniques. In this work we propose a new dataset and benchmark CORe50, specifically designed for continuous object recognition, and introduce baseline approaches for different continuous learning scenarios.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="lomonaco2017aBenchmarks_more3" style="display: none">
    N.A.
</span></p>
<script>
    function lomonaco2017aBenchmarksFunction() {
      var moreText = document.getElementById("lomonaco2017aBenchmarks_more");
      var moreText2 = document.getElementById("lomonaco2017aBenchmarks_more2");
      var moreText3 = document.getElementById("lomonaco2017aBenchmarks_more3");
      var btnText = document.getElementById("lomonaco2017aBenchmarks_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function lomonaco2017aBenchmarksFunction2() {
      var moreText = document.getElementById("lomonaco2017aBenchmarks_more2");
      var moreText1 = document.getElementById("lomonaco2017aBenchmarks_more");
      var moreText3 = document.getElementById("lomonaco2017aBenchmarks_more3");
      var btnText = document.getElementById("lomonaco2017aBenchmarks_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function lomonaco2017aBenchmarksFunction3() {
      var moreText = document.getElementById("lomonaco2017aBenchmarks_more3");
      var moreText1 = document.getElementById("lomonaco2017aBenchmarks_more");
      var moreText2 = document.getElementById("lomonaco2017aBenchmarks_more2");
      var btnText = document.getElementById("lomonaco2017aBenchmarks_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
</ul>
</div>
<div class="section" id="architectural-methods">
<h3>Architectural Methods<a class="headerlink" href="#architectural-methods" title="Permalink to this headline">¶</a></h3>
<p>In this section we collect all the papers introducing a continual learning strategy employing some architectural methods.</p>
<ul class="simple">
<li><p><a class="reference external" href="http://arxiv.org/abs/2004.04077">Continual Learning with Gated Incremental Memories for sequential data processing</a> by Andrea Cossu, Antonio Carta and Davide Bacciu. <em>Proceedings of the 2020 International Joint Conference on Neural Networks (IJCNN 2020)</em>, 2020. <span style='background-color:MediumAquaMarine; padding: 2px; border-radius:4px; border: 1px solid black;'>[mnist]</span> <span style='background-color:AliceBlue; padding: 2px; border-radius:4px; border: 1px solid black;'>[rnn]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Cossu2020aArchitectural_MethodsFunction()" id="Cossu2020aArchitectural_Methods_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Cossu2020aArchitectural_MethodsFunction2()" id="Cossu2020aArchitectural_Methods_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Cossu2020aArchitectural_MethodsFunction3()" id="Cossu2020aArchitectural_Methods_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Cossu2020aArchitectural_Methods_more" style="display: none">
    @inproceedings{Cossu2020a,<br> annote = {An evaluation of RNNs (LSTM and LMN) inspired by Progressive networks, leading to the Gated Incremental Memory approach to overcome catastrophic forgetting.},<br> author = {Cossu, Andrea and Carta, Antonio and Bacciu, Davide},<br> booktitle = {Proceedings of the 2020 International Joint Conference on Neural Networks (IJCNN 2020)},<br> keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computi,Statistics - Machine Learning,[mnist],[rnn]},<br> mendeley-tags = {[mnist],[rnn]},<br> month = {apr},<br> title = {Continual Learning with Gated Incremental Memories for sequential data processing},<br> url = {http://arxiv.org/abs/2004.04077},<br> year = {2020}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Cossu2020aArchitectural_Methods_more2" style="display: none">
    The ability to learn in dynamic, nonstationary environments without forgetting previous knowledge, also known as Continual Learning (CL), is a key enabler for scalable and trustworthy deployments of adaptive solutions. While the importance of continual learning is largely acknowledged in machine vision and reinforcement learning problems, this is mostly under-documented for sequence processing tasks. This work proposes a Recurrent Neural Network (RNN) model for CL that is able to deal with concept drift in input distribution without forgetting previously acquired knowledge. We also implement and test a popular CL approach, Elastic Weight Consolidation (EWC), on top of two different types of RNNs. Finally, we compare the performances of our enhanced architecture against EWC and RNNs on a set of standard CL benchmarks, adapted to the sequential data processing scenario. Results show the superior performance of our architecture and highlight the need for special solutions designed to address CL in RNNs.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Cossu2020aArchitectural_Methods_more3" style="display: none">
    An evaluation of RNNs (LSTM and LMN) inspired by Progressive networks, leading to the Gated Incremental Memory approach to overcome catastrophic forgetting.
</span></p>
<script>
    function Cossu2020aArchitectural_MethodsFunction() {
      var moreText = document.getElementById("Cossu2020aArchitectural_Methods_more");
      var moreText2 = document.getElementById("Cossu2020aArchitectural_Methods_more2");
      var moreText3 = document.getElementById("Cossu2020aArchitectural_Methods_more3");
      var btnText = document.getElementById("Cossu2020aArchitectural_Methods_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Cossu2020aArchitectural_MethodsFunction2() {
      var moreText = document.getElementById("Cossu2020aArchitectural_Methods_more2");
      var moreText1 = document.getElementById("Cossu2020aArchitectural_Methods_more");
      var moreText3 = document.getElementById("Cossu2020aArchitectural_Methods_more3");
      var btnText = document.getElementById("Cossu2020aArchitectural_Methods_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Cossu2020aArchitectural_MethodsFunction3() {
      var moreText = document.getElementById("Cossu2020aArchitectural_Methods_more3");
      var moreText1 = document.getElementById("Cossu2020aArchitectural_Methods_more");
      var moreText2 = document.getElementById("Cossu2020aArchitectural_Methods_more2");
      var btnText = document.getElementById("Cossu2020aArchitectural_Methods_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="http://arxiv.org/abs/2004.10098">Bayesian Nonparametric Weight Factorization for Continual Learning</a> by Nikhil Mehta, Kevin J Liang and Lawrence Carin. <em>arXiv</em>, 1–17, 2020. <span style='background-color:Violet; padding: 2px; border-radius:4px; border: 1px solid black;'>[bayes]</span> <span style='background-color:DarkCyan; padding: 2px; border-radius:4px; border: 1px solid black;'>[cifar]</span> <span style='background-color:MediumAquaMarine; padding: 2px; border-radius:4px; border: 1px solid black;'>[mnist]</span> <span style='background-color:DarkSalmon; padding: 2px; border-radius:4px; border: 1px solid black;'>[sparsity]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Mehta2020aArchitectural_MethodsFunction()" id="Mehta2020aArchitectural_Methods_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Mehta2020aArchitectural_MethodsFunction2()" id="Mehta2020aArchitectural_Methods_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Mehta2020aArchitectural_MethodsFunction3()" id="Mehta2020aArchitectural_Methods_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Mehta2020aArchitectural_Methods_more" style="display: none">
    @article{Mehta2020a,<br> archiveprefix = {arXiv},<br> arxivid = {2004.10098},<br> author = {Mehta, Nikhil and Liang, Kevin J and Carin, Lawrence},<br> eprint = {2004.10098},<br> journal = {arXiv},<br> keywords = {[bayes],[cifar],[mnist],[sparsity]},<br> mendeley-tags = {[bayes],[cifar],[mnist],[sparsity]},<br> pages = {1--17},<br> title = {Bayesian Nonparametric Weight Factorization for Continual Learning},<br> url = {http://arxiv.org/abs/2004.10098},<br> year = {2020}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Mehta2020aArchitectural_Methods_more2" style="display: none">
    Naively trained neural networks tend to experience catastrophic forgetting in sequential task settings, where data from previous tasks are unavailable. A number of methods, using various model expansion strategies, have been proposed recently as possible solutions. However, determining how much to expand the model is left to the practitioner, and typically a constant schedule is chosen for simplicity, regardless of how complex the incoming task is. Instead, we propose a principled Bayesian nonparametric approach based on the Indian Buffet Process (IBP) prior, letting the data determine how much to expand the model complexity. We pair this with a factorization of the neural network's weight matrices. Such an approach allows us to scale the number of factors of each weight matrix to the complexity of the task, while the IBP prior imposes weight factor sparsity and encourages factor reuse, promoting positive knowledge transfer between tasks. We demonstrate the effectiveness of our method on a number of continual learning benchmarks and analyze how weight factors are allocated and reused throughout the training.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Mehta2020aArchitectural_Methods_more3" style="display: none">
    N.A.
</span></p>
<script>
    function Mehta2020aArchitectural_MethodsFunction() {
      var moreText = document.getElementById("Mehta2020aArchitectural_Methods_more");
      var moreText2 = document.getElementById("Mehta2020aArchitectural_Methods_more2");
      var moreText3 = document.getElementById("Mehta2020aArchitectural_Methods_more3");
      var btnText = document.getElementById("Mehta2020aArchitectural_Methods_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Mehta2020aArchitectural_MethodsFunction2() {
      var moreText = document.getElementById("Mehta2020aArchitectural_Methods_more2");
      var moreText1 = document.getElementById("Mehta2020aArchitectural_Methods_more");
      var moreText3 = document.getElementById("Mehta2020aArchitectural_Methods_more3");
      var btnText = document.getElementById("Mehta2020aArchitectural_Methods_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Mehta2020aArchitectural_MethodsFunction3() {
      var moreText = document.getElementById("Mehta2020aArchitectural_Methods_more3");
      var moreText1 = document.getElementById("Mehta2020aArchitectural_Methods_more");
      var moreText2 = document.getElementById("Mehta2020aArchitectural_Methods_more2");
      var btnText = document.getElementById("Mehta2020aArchitectural_Methods_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="https://openreview.net/forum?id=Hklso24Kwr">Continual Learning with Adaptive Weights (CLAW)</a> by Tameem Adel, Han Zhao and Richard E Turner. <em>International Conference on Learning Representations</em>, 2020. <span style='background-color:DarkCyan; padding: 2px; border-radius:4px; border: 1px solid black;'>[cifar]</span> <span style='background-color:MediumAquaMarine; padding: 2px; border-radius:4px; border: 1px solid black;'>[mnist]</span> <span style='background-color:#FDD017; padding: 2px; border-radius:4px; border: 1px solid black;'>[omniglot]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="adel2020aArchitectural_MethodsFunction()" id="adel2020aArchitectural_Methods_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="adel2020aArchitectural_MethodsFunction2()" id="adel2020aArchitectural_Methods_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="adel2020aArchitectural_MethodsFunction3()" id="adel2020aArchitectural_Methods_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="adel2020aArchitectural_Methods_more" style="display: none">
    @inproceedings{adel2020a,<br> author = {Adel, Tameem and Zhao, Han and Turner, Richard E},<br> booktitle = {International Conference on Learning Representations},<br> keywords = {[cifar],[mnist],[omniglot]},<br> mendeley-tags = {[cifar],[mnist],[omniglot]},<br> title = {Continual Learning with Adaptive Weights (CLAW)},<br> url = {https://openreview.net/forum?id=Hklso24Kwr},<br> year = {2020}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="adel2020aArchitectural_Methods_more2" style="display: none">
    Approaches to continual learning aim to successfully learn a set of related tasks that arrive in an online manner. Recently, several frameworks have been developed which enable deep learning to be deployed in this learning scenario. A key modelling decision is to what extent the architecture should be shared across tasks. On the one hand, separately modelling each task avoids catastrophic forgetting but it does not support transfer learning and leads to large models. On the other hand, rigidly specifying a shared component and a task-specific part enables task transfer and limits the model size, but it is vulnerable to catastrophic forgetting and restricts the form of task-transfer that can occur. Ideally, the network should adaptively identify which parts of the network to share in a data driven way. Here we introduce such an approach called Continual Learning with Adaptive Weights (CLAW), which is based on probabilistic modelling and variational inference. Experiments show that CLAW achieves state-of-the-art performance on six benchmarks in terms of overall continual learning performance, as measured by classification accuracy, and in terms of addressing catastrophic forgetting.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="adel2020aArchitectural_Methods_more3" style="display: none">
    N.A.
</span></p>
<script>
    function adel2020aArchitectural_MethodsFunction() {
      var moreText = document.getElementById("adel2020aArchitectural_Methods_more");
      var moreText2 = document.getElementById("adel2020aArchitectural_Methods_more2");
      var moreText3 = document.getElementById("adel2020aArchitectural_Methods_more3");
      var btnText = document.getElementById("adel2020aArchitectural_Methods_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function adel2020aArchitectural_MethodsFunction2() {
      var moreText = document.getElementById("adel2020aArchitectural_Methods_more2");
      var moreText1 = document.getElementById("adel2020aArchitectural_Methods_more");
      var moreText3 = document.getElementById("adel2020aArchitectural_Methods_more3");
      var btnText = document.getElementById("adel2020aArchitectural_Methods_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function adel2020aArchitectural_MethodsFunction3() {
      var moreText = document.getElementById("adel2020aArchitectural_Methods_more3");
      var moreText1 = document.getElementById("adel2020aArchitectural_Methods_more");
      var moreText2 = document.getElementById("adel2020aArchitectural_Methods_more2");
      var btnText = document.getElementById("adel2020aArchitectural_Methods_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p>A Progressive Model to Enable Continual Learning for Semantic Slot Filling by Yilin Shen, Xiangyu Zeng and Hongxia Jin. <em>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</em>, 1279–1284, 2019. <span style='background-color:BlueViolet; padding: 2px; border-radius:4px; border: 1px solid black;'>[nlp]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Shen2019aArchitectural_MethodsFunction()" id="Shen2019aArchitectural_Methods_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Shen2019aArchitectural_MethodsFunction2()" id="Shen2019aArchitectural_Methods_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Shen2019aArchitectural_MethodsFunction3()" id="Shen2019aArchitectural_Methods_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Shen2019aArchitectural_Methods_more" style="display: none">
    @inproceedings{Shen2019a,<br> author = {Shen, Yilin and Zeng, Xiangyu and Jin, Hongxia},<br> booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing},<br> file = {::},<br> keywords = {[nlp]},<br> mendeley-tags = {[nlp]},<br> pages = {1279--1284},<br> publisher = {Association for Computational Linguistics},<br> title = {A Progressive Model to Enable Continual Learning for Semantic Slot Filling},<br> year = {2019}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Shen2019aArchitectural_Methods_more2" style="display: none">
    Semantic slot filling is one of the major tasks in spoken language understanding (SLU). After a slot filling model is trained on pre-collected data, it is crucial to continually improve the model after deployment to learn users' new expressions. As the data amount grows, it becomes infeasible to either store such huge data and repeatedly retrain the model on all data or fine tune the model only on new data without forgetting old expressions. In this paper, we introduce a novel progressive slot filling model, ProgModel. ProgModel consists of a novel context gate that transfers previously learned knowledge to a small size expanded component; and meanwhile enables this new component to be fast trained to learn from new data. As such, ProgModel learns the new knowledge by only using new data at each time and meanwhile preserves the previously learned expressions. Our experiments show that ProgModel needs much less training time and smaller model size to outperform various model fine tuning competitors by up to 4.24% and 3.03% on two benchmark datasets.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Shen2019aArchitectural_Methods_more3" style="display: none">
    N.A.
</span></p>
<script>
    function Shen2019aArchitectural_MethodsFunction() {
      var moreText = document.getElementById("Shen2019aArchitectural_Methods_more");
      var moreText2 = document.getElementById("Shen2019aArchitectural_Methods_more2");
      var moreText3 = document.getElementById("Shen2019aArchitectural_Methods_more3");
      var btnText = document.getElementById("Shen2019aArchitectural_Methods_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Shen2019aArchitectural_MethodsFunction2() {
      var moreText = document.getElementById("Shen2019aArchitectural_Methods_more2");
      var moreText1 = document.getElementById("Shen2019aArchitectural_Methods_more");
      var moreText3 = document.getElementById("Shen2019aArchitectural_Methods_more3");
      var btnText = document.getElementById("Shen2019aArchitectural_Methods_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Shen2019aArchitectural_MethodsFunction3() {
      var moreText = document.getElementById("Shen2019aArchitectural_Methods_more3");
      var moreText1 = document.getElementById("Shen2019aArchitectural_Methods_more");
      var moreText2 = document.getElementById("Shen2019aArchitectural_Methods_more2");
      var btnText = document.getElementById("Shen2019aArchitectural_Methods_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p>Continual Unsupervised Representation Learning by Dushyant Rao, Francesco Visin, Andrei A Rusu, Yee Whye Teh, Razvan Pascanu and Raia Hadsell. <em>NeurIPS</em>, 2019. <span style='background-color:MediumAquaMarine; padding: 2px; border-radius:4px; border: 1px solid black;'>[mnist]</span> <span style='background-color:#FDD017; padding: 2px; border-radius:4px; border: 1px solid black;'>[omniglot]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="rao2019aArchitectural_MethodsFunction()" id="rao2019aArchitectural_Methods_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="rao2019aArchitectural_MethodsFunction2()" id="rao2019aArchitectural_Methods_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="rao2019aArchitectural_MethodsFunction3()" id="rao2019aArchitectural_Methods_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="rao2019aArchitectural_Methods_more" style="display: none">
    @inproceedings{rao2019a,<br> author = {Rao, Dushyant and Visin, Francesco and Rusu, Andrei A and Teh, Yee Whye and Pascanu, Razvan and Hadsell, Raia},<br> booktitle = {NeurIPS},<br> file = {::},<br> keywords = {[mnist],[omniglot]},<br> mendeley-tags = {[mnist],[omniglot]},<br> title = {Continual Unsupervised Representation Learning},<br> year = {2019}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="rao2019aArchitectural_Methods_more2" style="display: none">
    Continual learning aims to improve the ability of modern learning systems to deal with non-stationary distributions, typically by attempting to learn a series of tasks sequentially. Prior art in the field has largely considered supervised or reinforcement learning tasks, and often assumes full knowledge of task labels and boundaries. In this work, we propose an approach (CURL) to tackle a more general problem that we will refer to as unsupervised continual learning. The focus is on learning representations without any knowledge about task identity, and we explore scenarios when there are abrupt changes between tasks, smooth transitions from one task to another, or even when the data is shuffled. The proposed approach performs task inference directly within the model, is able to dynamically expand to capture new concepts over its lifetime, and incorporates additional rehearsal-based techniques to deal with catastrophic forgetting. We demonstrate the efficacy of CURL in an unsupervised learning setting with MNIST and Omniglot, where the lack of labels ensures no information is leaked about the task. Further, we demonstrate strong performance compared to prior art in an i.i.d setting, or when adapting the technique to supervised tasks such as incremental class learning.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="rao2019aArchitectural_Methods_more3" style="display: none">
    N.A.
</span></p>
<script>
    function rao2019aArchitectural_MethodsFunction() {
      var moreText = document.getElementById("rao2019aArchitectural_Methods_more");
      var moreText2 = document.getElementById("rao2019aArchitectural_Methods_more2");
      var moreText3 = document.getElementById("rao2019aArchitectural_Methods_more3");
      var btnText = document.getElementById("rao2019aArchitectural_Methods_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function rao2019aArchitectural_MethodsFunction2() {
      var moreText = document.getElementById("rao2019aArchitectural_Methods_more2");
      var moreText1 = document.getElementById("rao2019aArchitectural_Methods_more");
      var moreText3 = document.getElementById("rao2019aArchitectural_Methods_more3");
      var btnText = document.getElementById("rao2019aArchitectural_Methods_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function rao2019aArchitectural_MethodsFunction3() {
      var moreText = document.getElementById("rao2019aArchitectural_Methods_more3");
      var moreText1 = document.getElementById("rao2019aArchitectural_Methods_more");
      var moreText2 = document.getElementById("rao2019aArchitectural_Methods_more2");
      var btnText = document.getElementById("rao2019aArchitectural_Methods_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="http://arxiv.org/abs/1907.10772">Towards AutoML in the presence of Drift: first results</a> by Jorge G. Madrid, Hugo Jair Escalante, Eduardo F. Morales, Wei-Wei Tu, Yang Yu, Lisheng Sun-Hosoya, Isabelle Guyon and Michele Sebag. <em>arXiv</em>, 2019.  <br>
<button style="font-size:75%; line-height:15px" onclick="Madrid2018aArchitectural_MethodsFunction()" id="Madrid2018aArchitectural_Methods_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Madrid2018aArchitectural_MethodsFunction2()" id="Madrid2018aArchitectural_Methods_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Madrid2018aArchitectural_MethodsFunction3()" id="Madrid2018aArchitectural_Methods_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Madrid2018aArchitectural_Methods_more" style="display: none">
    @article{Madrid2018a,<br> archiveprefix = {arXiv},<br> arxivid = {1907.10772},<br> author = {Madrid, Jorge G. and Escalante, Hugo Jair and Morales, Eduardo F. and Tu, Wei-Wei and Yu, Yang and Sun-Hosoya, Lisheng and Guyon, Isabelle and Sebag, Michele},<br> eprint = {1907.10772},<br> file = {::},<br> journal = {arXiv},<br> month = {jul},<br> title = {Towards AutoML in the presence of Drift: first results},<br> url = {http://arxiv.org/abs/1907.10772},<br> year = {2019}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Madrid2018aArchitectural_Methods_more2" style="display: none">
    Research progress in AutoML has lead to state of the art solutions that can cope quite wellwith supervised learning task, e.g., classification with AutoSklearn. However, so far thesesystems do not take into account the changing nature of evolving data over time (i.e., theystill assume i.i.d. data); even when this sort of domains are increasingly available in realapplications (e.g., spam filtering, user preferences, etc.). We describe a first attempt to de-velop an AutoML solution for scenarios in which data distribution changes relatively slowlyover time and in which the problem is approached in a lifelong learning setting. We extendAuto-Sklearn with sound and intuitive mechanisms that allow it to cope with this sort ofproblems. The extended Auto-Sklearn is combined with concept drift detection techniquesthat allow it to automatically determine when the initial models have to be adapted. Wereport experimental results in benchmark data from AutoML competitions that adhere tothis scenario. Results demonstrate the effectiveness of the proposed methodology.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Madrid2018aArchitectural_Methods_more3" style="display: none">
    N.A.
</span></p>
<script>
    function Madrid2018aArchitectural_MethodsFunction() {
      var moreText = document.getElementById("Madrid2018aArchitectural_Methods_more");
      var moreText2 = document.getElementById("Madrid2018aArchitectural_Methods_more2");
      var moreText3 = document.getElementById("Madrid2018aArchitectural_Methods_more3");
      var btnText = document.getElementById("Madrid2018aArchitectural_Methods_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Madrid2018aArchitectural_MethodsFunction2() {
      var moreText = document.getElementById("Madrid2018aArchitectural_Methods_more2");
      var moreText1 = document.getElementById("Madrid2018aArchitectural_Methods_more");
      var moreText3 = document.getElementById("Madrid2018aArchitectural_Methods_more3");
      var btnText = document.getElementById("Madrid2018aArchitectural_Methods_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Madrid2018aArchitectural_MethodsFunction3() {
      var moreText = document.getElementById("Madrid2018aArchitectural_Methods_more3");
      var moreText1 = document.getElementById("Madrid2018aArchitectural_Methods_more");
      var moreText2 = document.getElementById("Madrid2018aArchitectural_Methods_more2");
      var btnText = document.getElementById("Madrid2018aArchitectural_Methods_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="https://arxiv.org/pdf/1904.00310.pdf">Learn to grow: A continual structure learning framework for overcoming catastrophic forgetting</a> by Xilai Li, Yingbo Zhou, Tianfu Wu, Richard Socher and Caiming Xiong. <em>arXiv</em>, 2019. <span style='background-color:DarkCyan; padding: 2px; border-radius:4px; border: 1px solid black;'>[cifar]</span> <span style='background-color:MediumAquaMarine; padding: 2px; border-radius:4px; border: 1px solid black;'>[mnist]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="li2019aArchitectural_MethodsFunction()" id="li2019aArchitectural_Methods_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="li2019aArchitectural_MethodsFunction2()" id="li2019aArchitectural_Methods_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="li2019aArchitectural_MethodsFunction3()" id="li2019aArchitectural_Methods_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="li2019aArchitectural_Methods_more" style="display: none">
    @article{li2019a,<br> archiveprefix = {arXiv},<br> arxivid = {1904.00310},<br> author = {Li, Xilai and Zhou, Yingbo and Wu, Tianfu and Socher, Richard and Xiong, Caiming},<br> eprint = {1904.00310},<br> journal = {arXiv},<br> keywords = {[cifar],[mnist]},<br> mendeley-tags = {[cifar],[mnist]},<br> title = {Learn to grow: A continual structure learning framework for overcoming catastrophic forgetting},<br> url = {https://arxiv.org/pdf/1904.00310.pdf},<br> year = {2019}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="li2019aArchitectural_Methods_more2" style="display: none">
    Addressing catastrophic forgetting is one of the key challenges in continual learning where machine learning systems are trained with sequential or streaming tasks. Despite recent remarkable progress in state-of-the-art deep learning, deep neural networks (DNNs) are still plagued with the catastrophic forgetting problem. This paper presents a conceptually simple yet general and effective framework for handling catastrophic forgetting in continual learning with DNNs. The proposed method consists of two components: a neural structure optimization component and a parameter learning and/or fine-tuning component. By separating the explicit neural structure learning and the parameter estimation, not only is the proposed method capable of evolving neural structures in an intuitively meaningful way, but also shows strong capabilities of alleviating catastrophic forgetting in experiments. Furthermore, the proposed method outperforms all other baselines on the permuted MNIST dataset, the split CIFAR100 dataset and the Visual Domain Decathlon dataset in continual learning setting
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="li2019aArchitectural_Methods_more3" style="display: none">
    N.A.
</span></p>
<script>
    function li2019aArchitectural_MethodsFunction() {
      var moreText = document.getElementById("li2019aArchitectural_Methods_more");
      var moreText2 = document.getElementById("li2019aArchitectural_Methods_more2");
      var moreText3 = document.getElementById("li2019aArchitectural_Methods_more3");
      var btnText = document.getElementById("li2019aArchitectural_Methods_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function li2019aArchitectural_MethodsFunction2() {
      var moreText = document.getElementById("li2019aArchitectural_Methods_more2");
      var moreText1 = document.getElementById("li2019aArchitectural_Methods_more");
      var moreText3 = document.getElementById("li2019aArchitectural_Methods_more3");
      var btnText = document.getElementById("li2019aArchitectural_Methods_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function li2019aArchitectural_MethodsFunction3() {
      var moreText = document.getElementById("li2019aArchitectural_Methods_more3");
      var moreText1 = document.getElementById("li2019aArchitectural_Methods_more");
      var moreText2 = document.getElementById("li2019aArchitectural_Methods_more2");
      var btnText = document.getElementById("li2019aArchitectural_Methods_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="https://openreview.net/forum?id=BkepbpNFwr">Progressive Memory Banks for Incremental Domain Adaptation</a> by Nabiha Asghar, Lili Mou, Kira A Selby, Kevin D Pantasdo, Pascal Poupart and Xin Jiang. <em>International Conference on Learning Representations</em>, 2019. <span style='background-color:BlueViolet; padding: 2px; border-radius:4px; border: 1px solid black;'>[nlp]</span> <span style='background-color:AliceBlue; padding: 2px; border-radius:4px; border: 1px solid black;'>[rnn]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Asghar2019aArchitectural_MethodsFunction()" id="Asghar2019aArchitectural_Methods_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Asghar2019aArchitectural_MethodsFunction2()" id="Asghar2019aArchitectural_Methods_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Asghar2019aArchitectural_MethodsFunction3()" id="Asghar2019aArchitectural_Methods_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Asghar2019aArchitectural_Methods_more" style="display: none">
    @inproceedings{Asghar2019a,<br> annote = {The authors leverage a Recurrent Neural Network with an explicit memory (memory banks) which grows when new computational capabilities are needed. Attention mechanisms are exploited in order to focus on specific component of previous memories.},<br> author = {Asghar, Nabiha and Mou, Lili and Selby, Kira A and Pantasdo, Kevin D and Poupart, Pascal and Jiang, Xin},<br> booktitle = {International Conference on Learning Representations},<br> keywords = {[nlp],[rnn]},<br> mendeley-tags = {[nlp],[rnn]},<br> month = {sep},<br> title = {Progressive Memory Banks for Incremental Domain Adaptation},<br> url = {https://openreview.net/forum?id=BkepbpNFwr},<br> year = {2019}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Asghar2019aArchitectural_Methods_more2" style="display: none">
    This paper addresses the problem of incremental domain adaptation (IDA) in natural language processing (NLP). We assume each domain comes one after another, and that we could only access data in...
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Asghar2019aArchitectural_Methods_more3" style="display: none">
    The authors leverage a Recurrent Neural Network with an explicit memory (memory banks) which grows when new computational capabilities are needed. Attention mechanisms are exploited in order to focus on specific component of previous memories.
</span></p>
<script>
    function Asghar2019aArchitectural_MethodsFunction() {
      var moreText = document.getElementById("Asghar2019aArchitectural_Methods_more");
      var moreText2 = document.getElementById("Asghar2019aArchitectural_Methods_more2");
      var moreText3 = document.getElementById("Asghar2019aArchitectural_Methods_more3");
      var btnText = document.getElementById("Asghar2019aArchitectural_Methods_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Asghar2019aArchitectural_MethodsFunction2() {
      var moreText = document.getElementById("Asghar2019aArchitectural_Methods_more2");
      var moreText1 = document.getElementById("Asghar2019aArchitectural_Methods_more");
      var moreText3 = document.getElementById("Asghar2019aArchitectural_Methods_more3");
      var btnText = document.getElementById("Asghar2019aArchitectural_Methods_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Asghar2019aArchitectural_MethodsFunction3() {
      var moreText = document.getElementById("Asghar2019aArchitectural_Methods_more3");
      var moreText1 = document.getElementById("Asghar2019aArchitectural_Methods_more");
      var moreText2 = document.getElementById("Asghar2019aArchitectural_Methods_more2");
      var btnText = document.getElementById("Asghar2019aArchitectural_Methods_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="http://arxiv.org/abs/1903.04476">Continual Learning via Neural Pruning</a> by Siavash Golkar, Michael Kagan and Kyunghyun Cho. <em>arXiv</em>, 2019. <span style='background-color:DarkCyan; padding: 2px; border-radius:4px; border: 1px solid black;'>[cifar]</span> <span style='background-color:MediumAquaMarine; padding: 2px; border-radius:4px; border: 1px solid black;'>[mnist]</span> <span style='background-color:DarkSalmon; padding: 2px; border-radius:4px; border: 1px solid black;'>[sparsity]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Golkar2019aArchitectural_MethodsFunction()" id="Golkar2019aArchitectural_Methods_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Golkar2019aArchitectural_MethodsFunction2()" id="Golkar2019aArchitectural_Methods_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Golkar2019aArchitectural_MethodsFunction3()" id="Golkar2019aArchitectural_Methods_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Golkar2019aArchitectural_Methods_more" style="display: none">
    @article{Golkar2019a,<br> annote = {Comment: 12 pages, 5 figures, 3 tables<br>arXiv: 1903.04476},<br> author = {Golkar, Siavash and Kagan, Michael and Cho, Kyunghyun},<br> journal = {arXiv},<br> keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computi,Quantitative Biology - Neurons and Cognition,Statistics - Machine Learning,[cifar],[mnist],[sparsity]},<br> mendeley-tags = {[cifar],[mnist],[sparsity]},<br> month = {mar},<br> title = {Continual Learning via Neural Pruning},<br> url = {http://arxiv.org/abs/1903.04476},<br> year = {2019}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Golkar2019aArchitectural_Methods_more2" style="display: none">
    We introduce Continual Learning via Neural Pruning (CLNP), a new method aimed at lifelong learning in fixed capacity models based on neuronal model sparsification. In this method, subsequent tasks are trained using the inactive neurons and filters of the sparsified network and cause zero deterioration to the performance of previous tasks. In order to deal with the possible compromise between model sparsity and performance, we formalize and incorporate the concept of graceful forgetting: the idea that it is preferable to suffer a small amount of forgetting in a controlled manner if it helps regain network capacity and prevents uncontrolled loss of performance during the training of future tasks. CLNP also provides simple continual learning diagnostic tools in terms of the number of free neurons left for the training of future tasks as well as the number of neurons that are being reused. In particular, we see in experiments that CLNP verifies and automatically takes advantage of the fact that the features of earlier layers are more transferable. We show empirically that CLNP leads to significantly improved results over current weight elasticity based methods.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Golkar2019aArchitectural_Methods_more3" style="display: none">
    Comment: 12 pages, 5 figures, 3 tables
            arXiv: 1903.04476
</span></p>
<script>
    function Golkar2019aArchitectural_MethodsFunction() {
      var moreText = document.getElementById("Golkar2019aArchitectural_Methods_more");
      var moreText2 = document.getElementById("Golkar2019aArchitectural_Methods_more2");
      var moreText3 = document.getElementById("Golkar2019aArchitectural_Methods_more3");
      var btnText = document.getElementById("Golkar2019aArchitectural_Methods_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Golkar2019aArchitectural_MethodsFunction2() {
      var moreText = document.getElementById("Golkar2019aArchitectural_Methods_more2");
      var moreText1 = document.getElementById("Golkar2019aArchitectural_Methods_more");
      var moreText3 = document.getElementById("Golkar2019aArchitectural_Methods_more3");
      var btnText = document.getElementById("Golkar2019aArchitectural_Methods_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Golkar2019aArchitectural_MethodsFunction3() {
      var moreText = document.getElementById("Golkar2019aArchitectural_Methods_more3");
      var moreText1 = document.getElementById("Golkar2019aArchitectural_Methods_more");
      var moreText2 = document.getElementById("Golkar2019aArchitectural_Methods_more2");
      var btnText = document.getElementById("Golkar2019aArchitectural_Methods_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p>Lifelong Learning With Dynamically Expandable Networks by Jaehong Yoon, Eunho Yang, Jeongtae Lee and Sung Ju Hwang. <em>ICLR</em>, 11, 2018. <span style='background-color:DarkCyan; padding: 2px; border-radius:4px; border: 1px solid black;'>[cifar]</span> <span style='background-color:MediumAquaMarine; padding: 2px; border-radius:4px; border: 1px solid black;'>[mnist]</span> <span style='background-color:DarkSalmon; padding: 2px; border-radius:4px; border: 1px solid black;'>[sparsity]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Yoon2018Architectural_MethodsFunction()" id="Yoon2018Architectural_Methods_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Yoon2018Architectural_MethodsFunction2()" id="Yoon2018Architectural_Methods_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Yoon2018Architectural_MethodsFunction3()" id="Yoon2018Architectural_Methods_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Yoon2018Architectural_Methods_more" style="display: none">
    @inproceedings{Yoon2018,<br> annote = {The authors propose a method to evaluate the importance of each neuron in the network through the use of sparse connections. The network is then expanded based on the neuron importance for each task.},<br> author = {Yoon, Jaehong and Yang, Eunho and Lee, Jeongtae and Hwang, Sung Ju},<br> booktitle = {ICLR},<br> keywords = {[cifar],[mnist],[sparsity],disadvantages,lifelong learning,modular,progressive},<br> language = {en},<br> mendeley-tags = {[cifar],[mnist],[sparsity]},<br> pages = {11},<br> title = {Lifelong Learning With Dynamically Expandable Networks},<br> year = {2018}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Yoon2018Architectural_Methods_more2" style="display: none">
    We propose a novel deep network architecture for lifelong learning which we refer to as Dynamically Expandable Network (DEN), that can dynamically decide its network capacity as it trains on a sequence of tasks, to learn a compact overlapping knowledge sharing structure among tasks. DEN is efﬁciently trained in an online manner by performing selective retraining, dynamically expands network capacity upon arrival of each task with only the necessary number of units, and effectively prevents semantic drift by splitting/duplicating units and timestamping them. We validate DEN on multiple public datasets under lifelong learning scenarios, on which it not only signiﬁcantly outperforms existing lifelong learning methods for deep networks, but also achieves the same level of performance as the batch counterparts with substantially fewer number of parameters. Further, the obtained network ﬁne-tuned on all tasks obtained siginﬁcantly better performance over the batch models, which shows that it can be used to estimate the optimal network structure even when all tasks are available in the ﬁrst place.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Yoon2018Architectural_Methods_more3" style="display: none">
    The authors propose a method to evaluate the importance of each neuron in the network through the use of sparse connections. The network is then expanded based on the neuron importance for each task.
</span></p>
<script>
    function Yoon2018Architectural_MethodsFunction() {
      var moreText = document.getElementById("Yoon2018Architectural_Methods_more");
      var moreText2 = document.getElementById("Yoon2018Architectural_Methods_more2");
      var moreText3 = document.getElementById("Yoon2018Architectural_Methods_more3");
      var btnText = document.getElementById("Yoon2018Architectural_Methods_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Yoon2018Architectural_MethodsFunction2() {
      var moreText = document.getElementById("Yoon2018Architectural_Methods_more2");
      var moreText1 = document.getElementById("Yoon2018Architectural_Methods_more");
      var moreText3 = document.getElementById("Yoon2018Architectural_Methods_more3");
      var btnText = document.getElementById("Yoon2018Architectural_Methods_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Yoon2018Architectural_MethodsFunction3() {
      var moreText = document.getElementById("Yoon2018Architectural_Methods_more3");
      var moreText1 = document.getElementById("Yoon2018Architectural_Methods_more");
      var moreText2 = document.getElementById("Yoon2018Architectural_Methods_more2");
      var btnText = document.getElementById("Yoon2018Architectural_Methods_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="http://papers.nips.cc/paper/7369-reinforced-continual-learning">Reinforced continual learning</a> by Ju Xu and Zhanxing Zhu. <em>Advances in Neural Information Processing Systems</em>, 899–908, 2018. <span style='background-color:DarkCyan; padding: 2px; border-radius:4px; border: 1px solid black;'>[cifar]</span> <span style='background-color:MediumAquaMarine; padding: 2px; border-radius:4px; border: 1px solid black;'>[mnist]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="xu2018aArchitectural_MethodsFunction()" id="xu2018aArchitectural_Methods_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="xu2018aArchitectural_MethodsFunction2()" id="xu2018aArchitectural_Methods_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="xu2018aArchitectural_MethodsFunction3()" id="xu2018aArchitectural_Methods_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="xu2018aArchitectural_Methods_more" style="display: none">
    @inproceedings{xu2018a,<br> author = {Xu, Ju and Zhu, Zhanxing},<br> booktitle = {Advances in Neural Information Processing Systems},<br> keywords = {[cifar],[mnist]},<br> mendeley-tags = {[cifar],[mnist]},<br> pages = {899--908},<br> title = {Reinforced continual learning},<br> url = {http://papers.nips.cc/paper/7369-reinforced-continual-learning},<br> year = {2018}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="xu2018aArchitectural_Methods_more2" style="display: none">
    Most artificial intelligence models are limited in their ability to solve new tasks faster, without forgetting previously acquired knowledge. The recently emerging paradigm of continual learning aims to solve this issue, in which the model learns various tasks in a sequential fashion. In this work, a novel approach for continual learning is proposed, which searches for the best neural architecture for each coming task via sophisticatedly designed reinforcement learning strategies. We name it as Reinforced Continual Learning. Our method not only has good performance on preventing catastrophic forgetting but also fits new tasks well. The experiments on sequential classification tasks for variants of MNIST and CIFAR-100 datasets demonstrate that the proposed approach outperforms existing continual learning alternatives for deep networks.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="xu2018aArchitectural_Methods_more3" style="display: none">
    N.A.
</span></p>
<script>
    function xu2018aArchitectural_MethodsFunction() {
      var moreText = document.getElementById("xu2018aArchitectural_Methods_more");
      var moreText2 = document.getElementById("xu2018aArchitectural_Methods_more2");
      var moreText3 = document.getElementById("xu2018aArchitectural_Methods_more3");
      var btnText = document.getElementById("xu2018aArchitectural_Methods_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function xu2018aArchitectural_MethodsFunction2() {
      var moreText = document.getElementById("xu2018aArchitectural_Methods_more2");
      var moreText1 = document.getElementById("xu2018aArchitectural_Methods_more");
      var moreText3 = document.getElementById("xu2018aArchitectural_Methods_more3");
      var btnText = document.getElementById("xu2018aArchitectural_Methods_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function xu2018aArchitectural_MethodsFunction3() {
      var moreText = document.getElementById("xu2018aArchitectural_Methods_more3");
      var moreText1 = document.getElementById("xu2018aArchitectural_Methods_more");
      var moreText2 = document.getElementById("xu2018aArchitectural_Methods_more2");
      var btnText = document.getElementById("xu2018aArchitectural_Methods_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="http://openaccess.thecvf.com/content_cvpr_2018/html/Gidaris_Dynamic_Few-Shot_Visual_CVPR_2018_paper.html">Dynamic Few-Shot Visual Learning Without Forgetting</a> by Spyros Gidaris and Nikos Komodakis. <em>Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</em>, 4367–4375, 2018. <span style='background-color:DeepPink; padding: 2px; border-radius:4px; border: 1px solid black;'>[imagenet]</span> <span style='background-color:#E5E4E2; padding: 2px; border-radius:4px; border: 1px solid black;'>[vision]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Gidaris2018aArchitectural_MethodsFunction()" id="Gidaris2018aArchitectural_Methods_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Gidaris2018aArchitectural_MethodsFunction2()" id="Gidaris2018aArchitectural_Methods_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Gidaris2018aArchitectural_MethodsFunction3()" id="Gidaris2018aArchitectural_Methods_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Gidaris2018aArchitectural_Methods_more" style="display: none">
    @inproceedings{Gidaris2018a,<br> archiveprefix = {arXiv},<br> arxivid = {1804.09458},<br> author = {Gidaris, Spyros and Komodakis, Nikos},<br> booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},<br> doi = {10.1109/CVPR.2018.00459},<br> eprint = {1804.09458},<br> isbn = {9781538664209},<br> issn = {10636919},<br> keywords = {[imagenet],[vision]},<br> mendeley-tags = {[imagenet],[vision]},<br> pages = {4367--4375},<br> title = {Dynamic Few-Shot Visual Learning Without Forgetting},<br> url = {http://openaccess.thecvf.com/content_cvpr_2018/html/Gidaris_Dynamic_Few-Shot_Visual_CVPR_2018_paper.html},<br> year = {2018}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Gidaris2018aArchitectural_Methods_more2" style="display: none">
    The human visual system has the remarkably ability to be able to effortlessly learn novel concepts from only a few examples. Mimicking the same behavior on machine learning vision systems is an interesting and very challenging research problem with many practical advantages on real world vision applications. In this context, the goal of our work is to devise a few-shot visual learning system that during test time it will be able to efficiently learn novel categories from only a few training data while at the same time it will not forget the initial categories on which it was trained (here called base categories). To achieve that goal we propose (a) to extend an object recognition system with an attention based few-shot classification weight generator, and (b) to redesign the classifier of a ConvNet model as the cosine similarity function between feature representations and classification weight vectors. The latter, apart from unifying the recognition of both novel and base categories, it also leads to feature representations that generalize better on 'unseen' categories. We extensively evaluate our approach on Mini-ImageNet where we manage to improve the prior state-of-the-art on few-shot recognition (i.e., we achieve 56.20% and 73.00% on the 1-shot and 5-shot settings respectively) while at the same time we do not sacrifice any accuracy on the base categories, which is a characteristic that most prior approaches lack. Finally, we apply our approach on the recently introduced few-shot benchmark of Bharath and Girshick [4] where we also achieve state-of-the-art results.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Gidaris2018aArchitectural_Methods_more3" style="display: none">
    N.A.
</span></p>
<script>
    function Gidaris2018aArchitectural_MethodsFunction() {
      var moreText = document.getElementById("Gidaris2018aArchitectural_Methods_more");
      var moreText2 = document.getElementById("Gidaris2018aArchitectural_Methods_more2");
      var moreText3 = document.getElementById("Gidaris2018aArchitectural_Methods_more3");
      var btnText = document.getElementById("Gidaris2018aArchitectural_Methods_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Gidaris2018aArchitectural_MethodsFunction2() {
      var moreText = document.getElementById("Gidaris2018aArchitectural_Methods_more2");
      var moreText1 = document.getElementById("Gidaris2018aArchitectural_Methods_more");
      var moreText3 = document.getElementById("Gidaris2018aArchitectural_Methods_more3");
      var btnText = document.getElementById("Gidaris2018aArchitectural_Methods_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Gidaris2018aArchitectural_MethodsFunction3() {
      var moreText = document.getElementById("Gidaris2018aArchitectural_Methods_more3");
      var moreText1 = document.getElementById("Gidaris2018aArchitectural_Methods_more");
      var moreText2 = document.getElementById("Gidaris2018aArchitectural_Methods_more2");
      var btnText = document.getElementById("Gidaris2018aArchitectural_Methods_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="https://www.osti.gov/biblio/1424868">Neurogenesis Deep Learning</a> by Timothy John Draelos, Nadine E Miner, Christopher Lamb, Jonathan A Cox, Craig Michael Vineyard, Kristofor David Carlson, William Mark Severa, Conrad D James and James Bradley Aimone. <em>IJCNN</em>, 2017. <span style='background-color:MediumAquaMarine; padding: 2px; border-radius:4px; border: 1px solid black;'>[mnist]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Draelos2017aArchitectural_MethodsFunction()" id="Draelos2017aArchitectural_Methods_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Draelos2017aArchitectural_MethodsFunction2()" id="Draelos2017aArchitectural_Methods_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Draelos2017aArchitectural_MethodsFunction3()" id="Draelos2017aArchitectural_Methods_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Draelos2017aArchitectural_Methods_more" style="display: none">
    @inproceedings{Draelos2017a,<br> annote = {The neurogenesis algorithm selectively expand the original multi-layer autoencoder at the neuron level depending on its reconstruction performance measured at each layer. The model is capable of maintaining plasticity while mitigating forgetting through replay of old samples.},<br> author = {Draelos, Timothy John and Miner, Nadine E and Lamb, Christopher and Cox, Jonathan A and Vineyard, Craig Michael and Carlson, Kristofor David and Severa, William Mark and James, Conrad D and Aimone, James Bradley},<br> booktitle = {IJCNN},<br> keywords = {[mnist],autoencoder,autoencoders,neurogenesis,reconstruction},<br> language = {English},<br> mendeley-tags = {[mnist]},<br> title = {Neurogenesis Deep Learning},<br> url = {https://www.osti.gov/biblio/1424868},<br> year = {2017}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Draelos2017aArchitectural_Methods_more2" style="display: none">
    Neural machine learning methods, such as deep neural networks (DNN), have achieved remarkable success in a number of complex data processing tasks. These methods have arguably had their strongest impact on tasks such as image and audio processing - data processing domains in which humans have long held clear advantages over conventional algorithms. In contrast to biological neural systems, which are capable of learning continuously, deep artificial networks have a limited ability for incorporating new information in an already trained network. As a result, methods for continuous learning are potentially highly impactful in enabling the application of deep networks to dynamic data sets. Here, inspired by the process of adult neurogenesis in the hippocampus, we explore the potential for adding new neurons to deep layers of artificial neural networks in order to facilitate their acquisition of novel information while preserving previously trained data representations. Our results on the MNIST handwritten digit dataset and the NIST SD 19 dataset, which includes lower and upper case letters and digits, demonstrate that neurogenesis is well suited for addressing the stability-plasticity dilemma that has long challenged adaptive machine learning algorithms.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Draelos2017aArchitectural_Methods_more3" style="display: none">
    The neurogenesis algorithm selectively expand the original multi-layer autoencoder at the neuron level depending on its reconstruction performance measured at each layer. The model is capable of maintaining plasticity while mitigating forgetting through replay of old samples.
</span></p>
<script>
    function Draelos2017aArchitectural_MethodsFunction() {
      var moreText = document.getElementById("Draelos2017aArchitectural_Methods_more");
      var moreText2 = document.getElementById("Draelos2017aArchitectural_Methods_more2");
      var moreText3 = document.getElementById("Draelos2017aArchitectural_Methods_more3");
      var btnText = document.getElementById("Draelos2017aArchitectural_Methods_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Draelos2017aArchitectural_MethodsFunction2() {
      var moreText = document.getElementById("Draelos2017aArchitectural_Methods_more2");
      var moreText1 = document.getElementById("Draelos2017aArchitectural_Methods_more");
      var moreText3 = document.getElementById("Draelos2017aArchitectural_Methods_more3");
      var btnText = document.getElementById("Draelos2017aArchitectural_Methods_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Draelos2017aArchitectural_MethodsFunction3() {
      var moreText = document.getElementById("Draelos2017aArchitectural_Methods_more3");
      var moreText1 = document.getElementById("Draelos2017aArchitectural_Methods_more");
      var moreText2 = document.getElementById("Draelos2017aArchitectural_Methods_more2");
      var btnText = document.getElementById("Draelos2017aArchitectural_Methods_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1701.06106">Neurogenesis-inspired dictionary learning: Online model adaption in a changing world</a> by Sahil Garg, Irina Rish, Guillermo Cecchi and Aurelie Lozano. <em>IJCAI International Joint Conference on Artificial Intelligence</em>, 1696–1702, 2017. <span style='background-color:BlueViolet; padding: 2px; border-radius:4px; border: 1px solid black;'>[nlp]</span> <span style='background-color:#E5E4E2; padding: 2px; border-radius:4px; border: 1px solid black;'>[vision]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Garg2017aArchitectural_MethodsFunction()" id="Garg2017aArchitectural_Methods_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Garg2017aArchitectural_MethodsFunction2()" id="Garg2017aArchitectural_Methods_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Garg2017aArchitectural_MethodsFunction3()" id="Garg2017aArchitectural_Methods_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Garg2017aArchitectural_Methods_more" style="display: none">
    @article{Garg2017a,<br> archiveprefix = {arXiv},<br> arxivid = {1701.06106},<br> author = {Garg, Sahil and Rish, Irina and Cecchi, Guillermo and Lozano, Aurelie},<br> doi = {10.24963/ijcai.2017/235},<br> eprint = {1701.06106},<br> isbn = {9780999241103},<br> issn = {10450823},<br> journal = {IJCAI International Joint Conference on Artificial Intelligence},<br> keywords = {[nlp],[vision]},<br> mendeley-tags = {[nlp],[vision]},<br> pages = {1696--1702},<br> title = {Neurogenesis-inspired dictionary learning: Online model adaption in a changing world},<br> url = {https://arxiv.org/abs/1701.06106},<br> year = {2017}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Garg2017aArchitectural_Methods_more2" style="display: none">
    We address the problem of online model adaptation when learning representations from non-stationary data streams. Specifically, we focus here on online dictionary learning (i.e. sparse linear autoencoder), and propose a simple but effective online modelselection approach involving "birth" (addition) and "death" (removal) of hidden units representing dictionary elements, in response to changing inputs; we draw inspiration from the adult neurogenesis phenomenon in the dentate gyrus of the hippocampus, known to be associated with better adaptation to new environments. Empirical evaluation on real-life datasets (images and text), as well as on synthetic data, demonstrates that the proposed approach can considerably outperform the state-of-art non-adaptive online sparse coding of [Mairal et al., 2009] in the presence of non-stationary data. Moreover, we identify certain data- and model properties associated with such improvements.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Garg2017aArchitectural_Methods_more3" style="display: none">
    N.A.
</span></p>
<script>
    function Garg2017aArchitectural_MethodsFunction() {
      var moreText = document.getElementById("Garg2017aArchitectural_Methods_more");
      var moreText2 = document.getElementById("Garg2017aArchitectural_Methods_more2");
      var moreText3 = document.getElementById("Garg2017aArchitectural_Methods_more3");
      var btnText = document.getElementById("Garg2017aArchitectural_Methods_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Garg2017aArchitectural_MethodsFunction2() {
      var moreText = document.getElementById("Garg2017aArchitectural_Methods_more2");
      var moreText1 = document.getElementById("Garg2017aArchitectural_Methods_more");
      var moreText3 = document.getElementById("Garg2017aArchitectural_Methods_more3");
      var btnText = document.getElementById("Garg2017aArchitectural_Methods_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Garg2017aArchitectural_MethodsFunction3() {
      var moreText = document.getElementById("Garg2017aArchitectural_Methods_more3");
      var moreText1 = document.getElementById("Garg2017aArchitectural_Methods_more");
      var moreText2 = document.getElementById("Garg2017aArchitectural_Methods_more2");
      var btnText = document.getElementById("Garg2017aArchitectural_Methods_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="https://core.ac.uk/reader/84859350">Continual learning through evolvable neural turing machines</a> by Benno Luders, Mikkel Schlager and Sebastia Risi. <em>NIPS 2016 Workshop on Continual Learning and Deep Networks</em>, 2016.  <br>
<button style="font-size:75%; line-height:15px" onclick="Luders2016aArchitectural_MethodsFunction()" id="Luders2016aArchitectural_Methods_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Luders2016aArchitectural_MethodsFunction2()" id="Luders2016aArchitectural_Methods_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Luders2016aArchitectural_MethodsFunction3()" id="Luders2016aArchitectural_Methods_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Luders2016aArchitectural_Methods_more" style="display: none">
    @inproceedings{Luders2016a,<br> author = {Luders, Benno and Schlager, Mikkel and Risi, Sebastia},<br> booktitle = {NIPS 2016 Workshop on Continual Learning and Deep Networks},<br> title = {Continual learning through evolvable neural turing machines},<br> url = {https://core.ac.uk/reader/84859350},<br> year = {2016}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Luders2016aArchitectural_Methods_more2" style="display: none">
    Continual learning, i.e. the ability to sequentially learn tasks without catastrophicforgetting of previously learned ones, is an important open challenge in machinelearning. In this paper we take a step in this direction by showing that the recentlyproposedEvolving Neural Turing Machine(ENTM) approach is able to performone-shot learningin a reinforcement learning task without catastrophic forgettingof previously stored associations.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Luders2016aArchitectural_Methods_more3" style="display: none">
    N.A.
</span></p>
<script>
    function Luders2016aArchitectural_MethodsFunction() {
      var moreText = document.getElementById("Luders2016aArchitectural_Methods_more");
      var moreText2 = document.getElementById("Luders2016aArchitectural_Methods_more2");
      var moreText3 = document.getElementById("Luders2016aArchitectural_Methods_more3");
      var btnText = document.getElementById("Luders2016aArchitectural_Methods_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Luders2016aArchitectural_MethodsFunction2() {
      var moreText = document.getElementById("Luders2016aArchitectural_Methods_more2");
      var moreText1 = document.getElementById("Luders2016aArchitectural_Methods_more");
      var moreText3 = document.getElementById("Luders2016aArchitectural_Methods_more3");
      var btnText = document.getElementById("Luders2016aArchitectural_Methods_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Luders2016aArchitectural_MethodsFunction3() {
      var moreText = document.getElementById("Luders2016aArchitectural_Methods_more3");
      var moreText1 = document.getElementById("Luders2016aArchitectural_Methods_more");
      var moreText2 = document.getElementById("Luders2016aArchitectural_Methods_more2");
      var btnText = document.getElementById("Luders2016aArchitectural_Methods_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="http://arxiv.org/abs/1606.04671">Progressive Neural Networks</a> by Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu and Raia Hadsell. <em>arXiv</em>, 2016. <span style='background-color:MediumAquaMarine; padding: 2px; border-radius:4px; border: 1px solid black;'>[mnist]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Rusu2016aArchitectural_MethodsFunction()" id="Rusu2016aArchitectural_Methods_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Rusu2016aArchitectural_MethodsFunction2()" id="Rusu2016aArchitectural_Methods_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Rusu2016aArchitectural_MethodsFunction3()" id="Rusu2016aArchitectural_Methods_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Rusu2016aArchitectural_Methods_more" style="display: none">
    @article{Rusu2016a,<br> annote = {The authors rely on a separate feedforward network (column) for each task the model is trained on. Each column is connected through adaptive connections to all the previous ones. The weights of previous columns are frozen once trained. At inference time, given a known task label, the network choose the appropriate column to produce the output, thus preventing forgetting by design.},<br> author = {Rusu, Andrei A and Rabinowitz, Neil C and Desjardins, Guillaume and Soyer, Hubert and Kirkpatrick, James and Kavukcuoglu, Koray and Pascanu, Razvan and Hadsell, Raia},<br> journal = {arXiv},<br> keywords = {Computer Science - Machine Learning,[mnist],lifelong learning,modular,progressive},<br> language = {en},<br> mendeley-tags = {[mnist]},<br> month = {jun},<br> title = {Progressive Neural Networks},<br> url = {http://arxiv.org/abs/1606.04671},<br> year = {2016}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Rusu2016aArchitectural_Methods_more2" style="display: none">
    Learning to solve complex sequences of tasks—while both leveraging transfer and avoiding catastrophic forgetting—remains a key obstacle to achieving human-level intelligence. The progressive networks approach represents a step forward in this direction: they are immune to forgetting and can leverage prior knowledge via lateral connections to previously learned features. We evaluate this architecture extensively on a wide variety of reinforcement learning tasks (Atari and 3D maze games), and show that it outperforms common baselines based on pretraining and ﬁnetuning. Using a novel sensitivity measure, we demonstrate that transfer occurs at both low-level sensory and high-level control layers of the learned policy.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Rusu2016aArchitectural_Methods_more3" style="display: none">
    The authors rely on a separate feedforward network (column) for each task the model is trained on. Each column is connected through adaptive connections to all the previous ones. The weights of previous columns are frozen once trained. At inference time, given a known task label, the network choose the appropriate column to produce the output, thus preventing forgetting by design.
</span></p>
<script>
    function Rusu2016aArchitectural_MethodsFunction() {
      var moreText = document.getElementById("Rusu2016aArchitectural_Methods_more");
      var moreText2 = document.getElementById("Rusu2016aArchitectural_Methods_more2");
      var moreText3 = document.getElementById("Rusu2016aArchitectural_Methods_more3");
      var btnText = document.getElementById("Rusu2016aArchitectural_Methods_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Rusu2016aArchitectural_MethodsFunction2() {
      var moreText = document.getElementById("Rusu2016aArchitectural_Methods_more2");
      var moreText1 = document.getElementById("Rusu2016aArchitectural_Methods_more");
      var moreText3 = document.getElementById("Rusu2016aArchitectural_Methods_more3");
      var btnText = document.getElementById("Rusu2016aArchitectural_Methods_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Rusu2016aArchitectural_MethodsFunction3() {
      var moreText = document.getElementById("Rusu2016aArchitectural_Methods_more3");
      var moreText1 = document.getElementById("Rusu2016aArchitectural_Methods_more");
      var moreText2 = document.getElementById("Rusu2016aArchitectural_Methods_more2");
      var btnText = document.getElementById("Rusu2016aArchitectural_Methods_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p>A self-organising network that grows when required by Stephen Marsland, Jonathan Shapiro and Ulrich Nehmzow. <em>Neural Networks</em>, 1041–1058, 2002. <span style='background-color:Cornsilk; padding: 2px; border-radius:4px; border: 1px solid black;'>[som]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="marsland2002aArchitectural_MethodsFunction()" id="marsland2002aArchitectural_Methods_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="marsland2002aArchitectural_MethodsFunction2()" id="marsland2002aArchitectural_Methods_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="marsland2002aArchitectural_MethodsFunction3()" id="marsland2002aArchitectural_Methods_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="marsland2002aArchitectural_Methods_more" style="display: none">
    @article{marsland2002a,<br> author = {Marsland, Stephen and Shapiro, Jonathan and Nehmzow, Ulrich},<br> doi = {10.1016/S0893-6080(02)00078-3},<br> issn = {08936080},<br> journal = {Neural Networks},<br> keywords = {Dimensionality reduction,Growing networks,Inspection,Mobile robotics,Novelty detection,Self-organisation,Topology preservation,Unsupervised learning,[som]},<br> mendeley-tags = {[som]},<br> month = {oct},<br> number = {8-9},<br> pages = {1041--1058},<br> publisher = {Pergamon},<br> title = {A self-organising network that grows when required},<br> volume = {15},<br> year = {2002}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="marsland2002aArchitectural_Methods_more2" style="display: none">
    The ability to grow extra nodes is a potentially useful facility for a self-organising neural network. A network that can add nodes into its map space can approximate the input space more accurately, and often more parsimoniously, than a network with predefined structure and size, such as the Self-Organising Map. In addition, a growing network can deal with dynamic input distributions. Most of the growing networks that have been proposed in the literature add new nodes to support the node that has accumulated the highest error during previous iterations or to support topological structures. This usually means that new nodes are added only when the number of iterations is an integer multiple of some pre-defined constant, $łambda$. This paper suggests a way in which the learning algorithm can add nodes whenever the network in its current state does not sufficiently match the input. In this way the network grows very quickly when new data is presented, but stops growing once the network has matched the data. This is particularly important when we consider dynamic data sets, where the distribution of inputs can change to a new regime after some time. We also demonstrate the preservation of neighbourhood relations in the data by the network. The new network is compared to an existing growing network, the Growing Neural Gas (GNG), on a artificial dataset, showing how the network deals with a change in input distribution after some time. Finally, the new network is applied to several novelty detection tasks and is compared with both the GNG and an unsupervised form of the Reduced Coulomb Energy network on a robotic inspection task and with a Support Vector Machine on two benchmark novelty detection tasks. © 2002 Elsevier Science Ltd. All rights reserved.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="marsland2002aArchitectural_Methods_more3" style="display: none">
    N.A.
</span></p>
<script>
    function marsland2002aArchitectural_MethodsFunction() {
      var moreText = document.getElementById("marsland2002aArchitectural_Methods_more");
      var moreText2 = document.getElementById("marsland2002aArchitectural_Methods_more2");
      var moreText3 = document.getElementById("marsland2002aArchitectural_Methods_more3");
      var btnText = document.getElementById("marsland2002aArchitectural_Methods_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function marsland2002aArchitectural_MethodsFunction2() {
      var moreText = document.getElementById("marsland2002aArchitectural_Methods_more2");
      var moreText1 = document.getElementById("marsland2002aArchitectural_Methods_more");
      var moreText3 = document.getElementById("marsland2002aArchitectural_Methods_more3");
      var btnText = document.getElementById("marsland2002aArchitectural_Methods_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function marsland2002aArchitectural_MethodsFunction3() {
      var moreText = document.getElementById("marsland2002aArchitectural_Methods_more3");
      var moreText1 = document.getElementById("marsland2002aArchitectural_Methods_more");
      var moreText2 = document.getElementById("marsland2002aArchitectural_Methods_more2");
      var btnText = document.getElementById("marsland2002aArchitectural_Methods_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
</ul>
</div>
<div class="section" id="regularization-methods">
<h3>Regularization Methods<a class="headerlink" href="#regularization-methods" title="Permalink to this headline">¶</a></h3>
<p>In this section we collect all the papers introducing a continual learning strategy employing some regularization methods.</p>
<ul class="simple">
<li><p><a class="reference external" href="http://www.sciencedirect.com/science/article/pii/S092523122030151X">Efficient continual learning in neural networks with embedding regularization</a> by Jary Pomponi, Simone Scardapane, Vincenzo Lomonaco and Aurelio Uncini. <em>Neurocomputing</em>, 2020. <span style='background-color:DarkCyan; padding: 2px; border-radius:4px; border: 1px solid black;'>[cifar]</span> <span style='background-color:MediumAquaMarine; padding: 2px; border-radius:4px; border: 1px solid black;'>[mnist]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Pomponi2020aRegularization_MethodsFunction()" id="Pomponi2020aRegularization_Methods_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Pomponi2020aRegularization_MethodsFunction2()" id="Pomponi2020aRegularization_Methods_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Pomponi2020aRegularization_MethodsFunction3()" id="Pomponi2020aRegularization_Methods_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Pomponi2020aRegularization_Methods_more" style="display: none">
    @article{Pomponi2020a,<br> author = {Pomponi, Jary and Scardapane, Simone and Lomonaco, Vincenzo and Uncini, Aurelio},<br> doi = {10.1016/j.neucom.2020.01.093},<br> issn = {0925-2312},<br> journal = {Neurocomputing},<br> keywords = {Catastrophic forgetting,Continual learning,Embedding,Regularization,Trainable activation functions,[cifar],[mnist]},<br> language = {en},<br> mendeley-tags = {[cifar],[mnist]},<br> month = {feb},<br> title = {Efficient continual learning in neural networks with embedding regularization},<br> url = {http://www.sciencedirect.com/science/article/pii/S092523122030151X},<br> year = {2020}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Pomponi2020aRegularization_Methods_more2" style="display: none">
    Continual learning of deep neural networks is a key requirement for scaling them up to more complex applicative scenarios and for achieving real lifelong learning of these architectures. Previous approaches to the problem have considered either the progressive increase in the size of the networks, or have tried to regularize the network behavior to equalize it with respect to previously observed tasks. In the latter case, it is essential to understand what type of information best represents this past behavior. Common techniques include regularizing the past outputs, gradients, or individual weights. In this work, we propose a new, relatively simple and efficient method to perform continual learning by regularizing instead the network internal embeddings. To make the approach scalable, we also propose a dynamic sampling strategy to reduce the memory footprint of the required external storage. We show that our method performs favorably with respect to state-of-the-art approaches in the literature, while requiring significantly less space in memory and computational time. In addition, inspired by to recent works, we evaluate the impact of selecting a more flexible model for the activation functions inside the network, evaluating the impact of catastrophic forgetting on the activation functions themselves.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Pomponi2020aRegularization_Methods_more3" style="display: none">
    N.A.
</span></p>
<script>
    function Pomponi2020aRegularization_MethodsFunction() {
      var moreText = document.getElementById("Pomponi2020aRegularization_Methods_more");
      var moreText2 = document.getElementById("Pomponi2020aRegularization_Methods_more2");
      var moreText3 = document.getElementById("Pomponi2020aRegularization_Methods_more3");
      var btnText = document.getElementById("Pomponi2020aRegularization_Methods_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Pomponi2020aRegularization_MethodsFunction2() {
      var moreText = document.getElementById("Pomponi2020aRegularization_Methods_more2");
      var moreText1 = document.getElementById("Pomponi2020aRegularization_Methods_more");
      var moreText3 = document.getElementById("Pomponi2020aRegularization_Methods_more3");
      var btnText = document.getElementById("Pomponi2020aRegularization_Methods_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Pomponi2020aRegularization_MethodsFunction3() {
      var moreText = document.getElementById("Pomponi2020aRegularization_Methods_more3");
      var moreText1 = document.getElementById("Pomponi2020aRegularization_Methods_more");
      var moreText2 = document.getElementById("Pomponi2020aRegularization_Methods_more2");
      var btnText = document.getElementById("Pomponi2020aRegularization_Methods_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="http://arxiv.org/abs/2004.10862">Continual Learning of Object Instances</a> by Kishan Parshotam and Mert Kilickaya. <em>CVPR 2020: Workshop on Continual Learning in Computer Vision</em>, 2020. <span style='background-color:#E5E4E2; padding: 2px; border-radius:4px; border: 1px solid black;'>[vision]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Parshotam2020aRegularization_MethodsFunction()" id="Parshotam2020aRegularization_Methods_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Parshotam2020aRegularization_MethodsFunction2()" id="Parshotam2020aRegularization_Methods_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Parshotam2020aRegularization_MethodsFunction3()" id="Parshotam2020aRegularization_Methods_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Parshotam2020aRegularization_Methods_more" style="display: none">
    @article{Parshotam2020a,<br> archiveprefix = {arXiv},<br> arxivid = {2004.10862},<br> author = {Parshotam, Kishan and Kilickaya, Mert},<br> eprint = {2004.10862},<br> journal = {CVPR 2020: Workshop on Continual Learning in Computer Vision},<br> keywords = {[vision]},<br> mendeley-tags = {[vision]},<br> title = {Continual Learning of Object Instances},<br> url = {http://arxiv.org/abs/2004.10862},<br> year = {2020}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Parshotam2020aRegularization_Methods_more2" style="display: none">
    We propose continual instance learning - a method that applies the concept of continual learning to the task of distinguishing instances of the same object category. We specifically focus on the car object, and incrementally learn to distinguish car instances from each other with metric learning. We begin our paper by evaluating current techniques. Establishing that catastrophic forgetting is evident in existing methods, we then propose two remedies. Firstly, we regularise metric learning via Normalised Cross-Entropy. Secondly, we augment existing models with synthetic data transfer. Our extensive experiments on three large-scale datasets, using two different architectures for five different continual learning methods, reveal that Normalised cross-entropy and synthetic transfer leads to less forgetting in existing techniques.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Parshotam2020aRegularization_Methods_more3" style="display: none">
    N.A.
</span></p>
<script>
    function Parshotam2020aRegularization_MethodsFunction() {
      var moreText = document.getElementById("Parshotam2020aRegularization_Methods_more");
      var moreText2 = document.getElementById("Parshotam2020aRegularization_Methods_more2");
      var moreText3 = document.getElementById("Parshotam2020aRegularization_Methods_more3");
      var btnText = document.getElementById("Parshotam2020aRegularization_Methods_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Parshotam2020aRegularization_MethodsFunction2() {
      var moreText = document.getElementById("Parshotam2020aRegularization_Methods_more2");
      var moreText1 = document.getElementById("Parshotam2020aRegularization_Methods_more");
      var moreText3 = document.getElementById("Parshotam2020aRegularization_Methods_more3");
      var btnText = document.getElementById("Parshotam2020aRegularization_Methods_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Parshotam2020aRegularization_MethodsFunction3() {
      var moreText = document.getElementById("Parshotam2020aRegularization_Methods_more3");
      var moreText1 = document.getElementById("Parshotam2020aRegularization_Methods_more");
      var moreText2 = document.getElementById("Parshotam2020aRegularization_Methods_more2");
      var btnText = document.getElementById("Parshotam2020aRegularization_Methods_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="https://openreview.net/forum?id=SJgwNerKvB">Continual learning with hypernetworks</a> by Johannes von Oswald, Christian Henning, João Sacramento and Benjamin F Grewe. <em>International Conference on Learning Representations</em>, 2020. <span style='background-color:DarkCyan; padding: 2px; border-radius:4px; border: 1px solid black;'>[cifar]</span> <span style='background-color:MediumAquaMarine; padding: 2px; border-radius:4px; border: 1px solid black;'>[mnist]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Oswal2019aRegularization_MethodsFunction()" id="Oswal2019aRegularization_Methods_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Oswal2019aRegularization_MethodsFunction2()" id="Oswal2019aRegularization_Methods_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Oswal2019aRegularization_MethodsFunction3()" id="Oswal2019aRegularization_Methods_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Oswal2019aRegularization_Methods_more" style="display: none">
    @inproceedings{Oswal2019a,<br> author = {von Oswald, Johannes and Henning, Christian and Sacramento, João and Grewe, Benjamin F},<br> booktitle = {International Conference on Learning Representations},<br> keywords = {[cifar],[mnist]},<br> mendeley-tags = {[cifar],[mnist]},<br> month = {sep},<br> title = {Continual learning with hypernetworks},<br> url = {https://openreview.net/forum?id=SJgwNerKvB},<br> year = {2020}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Oswal2019aRegularization_Methods_more2" style="display: none">
    Artificial neural networks suffer from catastrophic forgetting when they are sequentially trained on multiple tasks. To overcome this problem, we present a novel approach based on task-conditioned...
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Oswal2019aRegularization_Methods_more3" style="display: none">
    N.A.
</span></p>
<script>
    function Oswal2019aRegularization_MethodsFunction() {
      var moreText = document.getElementById("Oswal2019aRegularization_Methods_more");
      var moreText2 = document.getElementById("Oswal2019aRegularization_Methods_more2");
      var moreText3 = document.getElementById("Oswal2019aRegularization_Methods_more3");
      var btnText = document.getElementById("Oswal2019aRegularization_Methods_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Oswal2019aRegularization_MethodsFunction2() {
      var moreText = document.getElementById("Oswal2019aRegularization_Methods_more2");
      var moreText1 = document.getElementById("Oswal2019aRegularization_Methods_more");
      var moreText3 = document.getElementById("Oswal2019aRegularization_Methods_more3");
      var btnText = document.getElementById("Oswal2019aRegularization_Methods_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Oswal2019aRegularization_MethodsFunction3() {
      var moreText = document.getElementById("Oswal2019aRegularization_Methods_more3");
      var moreText1 = document.getElementById("Oswal2019aRegularization_Methods_more");
      var moreText2 = document.getElementById("Oswal2019aRegularization_Methods_more2");
      var btnText = document.getElementById("Oswal2019aRegularization_Methods_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Dhar_Learning_Without_Memorizing_CVPR_2019_paper.pdf">Learning without Memorizing</a> by Prithviraj Dhar, Rajat Vikram Singh, Kuan-Chuan Peng, Ziyan Wu and Rama Chellappa. <em>CVPR</em>, 2019. <span style='background-color:DarkCyan; padding: 2px; border-radius:4px; border: 1px solid black;'>[cifar]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="dhar2019aRegularization_MethodsFunction()" id="dhar2019aRegularization_Methods_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="dhar2019aRegularization_MethodsFunction2()" id="dhar2019aRegularization_Methods_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="dhar2019aRegularization_MethodsFunction3()" id="dhar2019aRegularization_Methods_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="dhar2019aRegularization_Methods_more" style="display: none">
    @inproceedings{dhar2019a,<br> author = {Dhar, Prithviraj and Vikram Singh, Rajat and Peng, Kuan-Chuan and Wu, Ziyan and Chellappa, Rama},<br> booktitle = {CVPR},<br> file = {::},<br> keywords = {[cifar]},<br> mendeley-tags = {[cifar]},<br> title = {Learning without Memorizing},<br> url = {https://openaccess.thecvf.com/content_CVPR_2019/papers/Dhar_Learning_Without_Memorizing_CVPR_2019_paper.pdf},<br> year = {2019}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="dhar2019aRegularization_Methods_more2" style="display: none">
    Incremental learning (IL) is an important task aimed at increasing the capability of a trained model, in terms of the number of classes recognizable by the model. The key problem in this task is the requirement of storing data (e.g. images) associated with existing classes, while teaching the classifier to learn new classes. However, this is impractical as it increases the memory requirement at every incre-mental step, which makes it impossible to implement IL algorithms on edge devices with limited memory. Hence, we propose a novel approach, called 'Learning without Memorizing (LwM)', to preserve the information about existing (base) classes, without storing any of their data, while making the classifier progressively learn the new classes. In LwM, we present an information preserving penalty: Attention Distillation Loss (L AD), and demonstrate that penalizing the changes in classifiers' attention maps helps to retain information of the base classes, as new classes are added. We show that adding L AD to the distillation loss which is an existing information preserving loss consistently outperforms the state-of-the-art performance in the iILSVRC-small and iCIFAR-100 datasets in terms of the overall accuracy of base and incrementally learned classes.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="dhar2019aRegularization_Methods_more3" style="display: none">
    N.A.
</span></p>
<script>
    function dhar2019aRegularization_MethodsFunction() {
      var moreText = document.getElementById("dhar2019aRegularization_Methods_more");
      var moreText2 = document.getElementById("dhar2019aRegularization_Methods_more2");
      var moreText3 = document.getElementById("dhar2019aRegularization_Methods_more3");
      var btnText = document.getElementById("dhar2019aRegularization_Methods_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function dhar2019aRegularization_MethodsFunction2() {
      var moreText = document.getElementById("dhar2019aRegularization_Methods_more2");
      var moreText1 = document.getElementById("dhar2019aRegularization_Methods_more");
      var moreText3 = document.getElementById("dhar2019aRegularization_Methods_more3");
      var btnText = document.getElementById("dhar2019aRegularization_Methods_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function dhar2019aRegularization_MethodsFunction3() {
      var moreText = document.getElementById("dhar2019aRegularization_Methods_more3");
      var moreText1 = document.getElementById("dhar2019aRegularization_Methods_more");
      var moreText2 = document.getElementById("dhar2019aRegularization_Methods_more2");
      var btnText = document.getElementById("dhar2019aRegularization_Methods_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="http://arxiv.org/abs/1805.07810">Online Structured Laplace Approximations For Overcoming Catastrophic Forgetting</a> by Hippolyt Ritter, Aleksandar Botev and David Barber. <em>arXiv</em>, 2018. <span style='background-color:Violet; padding: 2px; border-radius:4px; border: 1px solid black;'>[bayes]</span> <span style='background-color:MediumAquaMarine; padding: 2px; border-radius:4px; border: 1px solid black;'>[mnist]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Ritter2018aRegularization_MethodsFunction()" id="Ritter2018aRegularization_Methods_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Ritter2018aRegularization_MethodsFunction2()" id="Ritter2018aRegularization_Methods_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Ritter2018aRegularization_MethodsFunction3()" id="Ritter2018aRegularization_Methods_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Ritter2018aRegularization_Methods_more" style="display: none">
    @article{Ritter2018a,<br> archiveprefix = {arXiv},<br> arxivid = {1805.07810},<br> author = {Ritter, Hippolyt and Botev, Aleksandar and Barber, David},<br> eprint = {1805.07810},<br> journal = {arXiv},<br> keywords = {[bayes],[mnist]},<br> mendeley-tags = {[bayes],[mnist]},<br> title = {Online Structured Laplace Approximations For Overcoming Catastrophic Forgetting},<br> url = {http://arxiv.org/abs/1805.07810},<br> year = {2018}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Ritter2018aRegularization_Methods_more2" style="display: none">
    We introduce the Kronecker factored online Laplace approximation for overcoming catastrophic forgetting in neural networks. The method is grounded in a Bayesian online learning framework, where we recursively approximate the posterior after every task with a Gaussian, leading to a quadratic penalty on changes to the weights. The Laplace approximation requires calculating the Hessian around a mode, which is typically intractable for modern architectures. In order to make our method scalable, we leverage recent block-diagonal Kronecker factored approximations to the curvature. Our algorithm achieves over 90% test accuracy across a sequence of 50 instantiations of the permuted MNIST dataset, substantially outperforming related methods for overcoming catastrophic forgetting.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Ritter2018aRegularization_Methods_more3" style="display: none">
    N.A.
</span></p>
<script>
    function Ritter2018aRegularization_MethodsFunction() {
      var moreText = document.getElementById("Ritter2018aRegularization_Methods_more");
      var moreText2 = document.getElementById("Ritter2018aRegularization_Methods_more2");
      var moreText3 = document.getElementById("Ritter2018aRegularization_Methods_more3");
      var btnText = document.getElementById("Ritter2018aRegularization_Methods_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Ritter2018aRegularization_MethodsFunction2() {
      var moreText = document.getElementById("Ritter2018aRegularization_Methods_more2");
      var moreText1 = document.getElementById("Ritter2018aRegularization_Methods_more");
      var moreText3 = document.getElementById("Ritter2018aRegularization_Methods_more3");
      var btnText = document.getElementById("Ritter2018aRegularization_Methods_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Ritter2018aRegularization_MethodsFunction3() {
      var moreText = document.getElementById("Ritter2018aRegularization_Methods_more3");
      var moreText1 = document.getElementById("Ritter2018aRegularization_Methods_more");
      var moreText2 = document.getElementById("Ritter2018aRegularization_Methods_more2");
      var btnText = document.getElementById("Ritter2018aRegularization_Methods_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p>Rotate your Networks: Better Weight Consolidation and Less Catastrophic Forgetting by Xialei Liu, Marc Masana, Luis Herranz, Joost de Weijer, Antonio M López and Andrew D Bagdanov. <em>2018 24th International Conference on Pattern Recognition (ICPR)</em>, 2262–2268, 2018. <span style='background-color:DarkCyan; padding: 2px; border-radius:4px; border: 1px solid black;'>[cifar]</span> <span style='background-color:MediumAquaMarine; padding: 2px; border-radius:4px; border: 1px solid black;'>[mnist]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Liu2018aRegularization_MethodsFunction()" id="Liu2018aRegularization_Methods_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Liu2018aRegularization_MethodsFunction2()" id="Liu2018aRegularization_Methods_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Liu2018aRegularization_MethodsFunction3()" id="Liu2018aRegularization_Methods_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Liu2018aRegularization_Methods_more" style="display: none">
    @inproceedings{Liu2018a,<br> annote = {ISSN: 1051-4651},<br> author = {Liu, Xialei and Masana, Marc and Herranz, Luis and de Weijer, Joost and López, Antonio M and Bagdanov, Andrew D},<br> booktitle = {2018 24th International Conference on Pattern Recognition (ICPR)},<br> doi = {10.1109/ICPR.2018.8545895},<br> keywords = {Computer vision,Data models,Fisher Information Matrix,Neural networks,Standards,Stanford-40 datasets,Task analysis,Training,Training data,[cifar],[mnist],ewc,fisher,image classification,learning (artificial intelligence),matrix algebra,network parameters,network reparameterization,sequential tasks,standard elastic weight consolidation},<br> mendeley-tags = {[cifar],[mnist]},<br> month = {aug},<br> pages = {2262--2268},<br> shorttitle = {Rotate your Networks},<br> title = {Rotate your Networks: Better Weight Consolidation and Less Catastrophic Forgetting},<br> year = {2018}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Liu2018aRegularization_Methods_more2" style="display: none">
    In this paper we propose an approach to avoiding catastrophic forgetting in sequential task learning scenarios. Our technique is based on a network reparameterization that approximately diagonalizes the Fisher Information Matrix of the network parameters. This reparameterization takes the form of a factorized rotation of parameter space which, when used in conjunction with Elastic Weight Consolidation (which assumes a diagonal Fisher Information Matrix), leads to significantly better performance on lifelong learning of sequential tasks. Experimental results on the MNIST, CIFAR-100, CUB-200 and Stanford-40 datasets demonstrate that we significantly improve the results of standard elastic weight consolidation, and that we obtain competitive results when compared to the state-of-the-art in lifelong learning without forgetting.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Liu2018aRegularization_Methods_more3" style="display: none">
    ISSN: 1051-4651
</span></p>
<script>
    function Liu2018aRegularization_MethodsFunction() {
      var moreText = document.getElementById("Liu2018aRegularization_Methods_more");
      var moreText2 = document.getElementById("Liu2018aRegularization_Methods_more2");
      var moreText3 = document.getElementById("Liu2018aRegularization_Methods_more3");
      var btnText = document.getElementById("Liu2018aRegularization_Methods_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Liu2018aRegularization_MethodsFunction2() {
      var moreText = document.getElementById("Liu2018aRegularization_Methods_more2");
      var moreText1 = document.getElementById("Liu2018aRegularization_Methods_more");
      var moreText3 = document.getElementById("Liu2018aRegularization_Methods_more3");
      var btnText = document.getElementById("Liu2018aRegularization_Methods_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Liu2018aRegularization_MethodsFunction3() {
      var moreText = document.getElementById("Liu2018aRegularization_Methods_more3");
      var moreText1 = document.getElementById("Liu2018aRegularization_Methods_more");
      var moreText2 = document.getElementById("Liu2018aRegularization_Methods_more2");
      var btnText = document.getElementById("Liu2018aRegularization_Methods_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="https://github.com/joansj/hat">Overcoming Catastrophic Forgetting with Hard Attention to the Task</a> by Joan Serrà, Dd́ac Surís, Marius Miron and Alexandros Karatzoglou. <em>ICML</em>, 2018. <span style='background-color:DarkCyan; padding: 2px; border-radius:4px; border: 1px solid black;'>[cifar]</span> <span style='background-color:LightGray; padding: 2px; border-radius:4px; border: 1px solid black;'>[fashion]</span> <span style='background-color:MediumAquaMarine; padding: 2px; border-radius:4px; border: 1px solid black;'>[mnist]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Serra2018aRegularization_MethodsFunction()" id="Serra2018aRegularization_Methods_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Serra2018aRegularization_MethodsFunction2()" id="Serra2018aRegularization_Methods_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Serra2018aRegularization_MethodsFunction3()" id="Serra2018aRegularization_Methods_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Serra2018aRegularization_Methods_more" style="display: none">
    @inproceedings{Serra2018a,<br> author = {Serrà, Joan and Surís, D\d́ac and Miron, Marius and Karatzoglou, Alexandros},<br> booktitle = {ICML},<br> file = {::},<br> keywords = {[cifar],[fashion],[mnist],serra2018a},<br> mendeley-tags = {[cifar],[fashion],[mnist]},<br> title = {Overcoming Catastrophic Forgetting with Hard Attention to the Task},<br> url = {https://github.com/joansj/hat},<br> year = {2018}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Serra2018aRegularization_Methods_more2" style="display: none">
    Catastrophic forgetting occurs when a neural network loses the information learned in a previous task after training on subsequent tasks. This problem remains a hurdle for artificial intelligence systems with sequential learning capabilities. In this paper, we propose a task-based hard attention mechanism that preserves previous tasks' information without affecting the current task's learning. A hard attention mask is learned concurrently to every task, through stochastic gradient descent, and previous masks are exploited to condition such learning. We show that the proposed mechanism is effective for reducing catastrophic forgetting , cutting current rates by 45 to 80%. We also show that it is robust to different hyperparameter choices, and that it offers a number of monitoring capabilities. The approach features the possibility to control both the stability and compactness of the learned knowledge, which we believe makes it also attractive for online learning or network compression applications.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Serra2018aRegularization_Methods_more3" style="display: none">
    N.A.
</span></p>
<script>
    function Serra2018aRegularization_MethodsFunction() {
      var moreText = document.getElementById("Serra2018aRegularization_Methods_more");
      var moreText2 = document.getElementById("Serra2018aRegularization_Methods_more2");
      var moreText3 = document.getElementById("Serra2018aRegularization_Methods_more3");
      var btnText = document.getElementById("Serra2018aRegularization_Methods_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Serra2018aRegularization_MethodsFunction2() {
      var moreText = document.getElementById("Serra2018aRegularization_Methods_more2");
      var moreText1 = document.getElementById("Serra2018aRegularization_Methods_more");
      var moreText3 = document.getElementById("Serra2018aRegularization_Methods_more3");
      var btnText = document.getElementById("Serra2018aRegularization_Methods_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Serra2018aRegularization_MethodsFunction3() {
      var moreText = document.getElementById("Serra2018aRegularization_Methods_more3");
      var moreText1 = document.getElementById("Serra2018aRegularization_Methods_more");
      var moreText2 = document.getElementById("Serra2018aRegularization_Methods_more2");
      var btnText = document.getElementById("Serra2018aRegularization_Methods_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p>Memory Aware Synapses: Learning what (not) to forget by Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach and Tinne Tuytelaars. <em>The European Conference on Computer Vision (ECCV)</em>, 2018. <span style='background-color:#E5E4E2; padding: 2px; border-radius:4px; border: 1px solid black;'>[vision]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Aljundi2018aRegularization_MethodsFunction()" id="Aljundi2018aRegularization_Methods_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Aljundi2018aRegularization_MethodsFunction2()" id="Aljundi2018aRegularization_Methods_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Aljundi2018aRegularization_MethodsFunction3()" id="Aljundi2018aRegularization_Methods_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Aljundi2018aRegularization_Methods_more" style="display: none">
    @inproceedings{Aljundi2018a,<br> author = {Aljundi, Rahaf and Babiloni, Francesca and Elhoseiny, Mohamed and Rohrbach, Marcus and Tuytelaars, Tinne},<br> booktitle = {The European Conference on Computer Vision (ECCV)},<br> keywords = {[vision]},<br> mendeley-tags = {[vision]},<br> month = {sep},<br> title = {Memory Aware Synapses: Learning what (not) to forget},<br> year = {2018}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Aljundi2018aRegularization_Methods_more2" style="display: none">
    N.A.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Aljundi2018aRegularization_Methods_more3" style="display: none">
    N.A.
</span></p>
<script>
    function Aljundi2018aRegularization_MethodsFunction() {
      var moreText = document.getElementById("Aljundi2018aRegularization_Methods_more");
      var moreText2 = document.getElementById("Aljundi2018aRegularization_Methods_more2");
      var moreText3 = document.getElementById("Aljundi2018aRegularization_Methods_more3");
      var btnText = document.getElementById("Aljundi2018aRegularization_Methods_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Aljundi2018aRegularization_MethodsFunction2() {
      var moreText = document.getElementById("Aljundi2018aRegularization_Methods_more2");
      var moreText1 = document.getElementById("Aljundi2018aRegularization_Methods_more");
      var moreText3 = document.getElementById("Aljundi2018aRegularization_Methods_more3");
      var btnText = document.getElementById("Aljundi2018aRegularization_Methods_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Aljundi2018aRegularization_MethodsFunction3() {
      var moreText = document.getElementById("Aljundi2018aRegularization_Methods_more3");
      var moreText1 = document.getElementById("Aljundi2018aRegularization_Methods_more");
      var moreText2 = document.getElementById("Aljundi2018aRegularization_Methods_more2");
      var btnText = document.getElementById("Aljundi2018aRegularization_Methods_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="http://arxiv.org/abs/1612.00796">Overcoming catastrophic forgetting in neural networks</a> by James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran and Raia Hadsell. <em>PNAS</em>, 3521–3526, 2017. <span style='background-color:MediumAquaMarine; padding: 2px; border-radius:4px; border: 1px solid black;'>[mnist]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Kirkpatrick2017aRegularization_MethodsFunction()" id="Kirkpatrick2017aRegularization_Methods_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Kirkpatrick2017aRegularization_MethodsFunction2()" id="Kirkpatrick2017aRegularization_Methods_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Kirkpatrick2017aRegularization_MethodsFunction3()" id="Kirkpatrick2017aRegularization_Methods_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Kirkpatrick2017aRegularization_Methods_more" style="display: none">
    @article{Kirkpatrick2017a,<br> annote = {arXiv: 1612.00796},<br> author = {Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and Hassabis, Demis and Clopath, Claudia and Kumaran, Dharshan and Hadsell, Raia},<br> journal = {PNAS},<br> keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning,[mnist],annotated,ewc},<br> mendeley-tags = {[mnist]},<br> number = {13},<br> pages = {3521--3526},<br> title = {Overcoming catastrophic forgetting in neural networks},<br> url = {http://arxiv.org/abs/1612.00796},<br> volume = {114},<br> year = {2017}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Kirkpatrick2017aRegularization_Methods_more2" style="display: none">
    The ability to learn tasks in a sequential fashion is crucial to the development of artificial intelligence. Neural networks are not, in general, capable of this and it has been widely thought that catastrophic forgetting is an inevitable feature of connectionist models. We show that it is possible to overcome this limitation and train networks that can maintain expertise on tasks which they have not experienced for a long time. Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks. We demonstrate our approach is scalable and effective by solving a set of classification tasks based on the MNIST hand written digit dataset and by learning several Atari 2600 games sequentially.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Kirkpatrick2017aRegularization_Methods_more3" style="display: none">
    arXiv: 1612.00796
</span></p>
<script>
    function Kirkpatrick2017aRegularization_MethodsFunction() {
      var moreText = document.getElementById("Kirkpatrick2017aRegularization_Methods_more");
      var moreText2 = document.getElementById("Kirkpatrick2017aRegularization_Methods_more2");
      var moreText3 = document.getElementById("Kirkpatrick2017aRegularization_Methods_more3");
      var btnText = document.getElementById("Kirkpatrick2017aRegularization_Methods_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Kirkpatrick2017aRegularization_MethodsFunction2() {
      var moreText = document.getElementById("Kirkpatrick2017aRegularization_Methods_more2");
      var moreText1 = document.getElementById("Kirkpatrick2017aRegularization_Methods_more");
      var moreText3 = document.getElementById("Kirkpatrick2017aRegularization_Methods_more3");
      var btnText = document.getElementById("Kirkpatrick2017aRegularization_Methods_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Kirkpatrick2017aRegularization_MethodsFunction3() {
      var moreText = document.getElementById("Kirkpatrick2017aRegularization_Methods_more3");
      var moreText1 = document.getElementById("Kirkpatrick2017aRegularization_Methods_more");
      var moreText2 = document.getElementById("Kirkpatrick2017aRegularization_Methods_more2");
      var btnText = document.getElementById("Kirkpatrick2017aRegularization_Methods_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="http://arxiv.org/abs/1705.09847">Lifelong Generative Modeling</a> by Jason Ramapuram, Magda Gregorova and Alexandros Kalousis. <em>arXiv</em>, 1–14, 2017. <span style='background-color:LightGray; padding: 2px; border-radius:4px; border: 1px solid black;'>[fashion]</span> <span style='background-color:Maroon; padding: 2px; border-radius:4px; border: 1px solid black;'>[generative]</span> <span style='background-color:MediumAquaMarine; padding: 2px; border-radius:4px; border: 1px solid black;'>[mnist]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Ramapuram2017aRegularization_MethodsFunction()" id="Ramapuram2017aRegularization_Methods_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Ramapuram2017aRegularization_MethodsFunction2()" id="Ramapuram2017aRegularization_Methods_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Ramapuram2017aRegularization_MethodsFunction3()" id="Ramapuram2017aRegularization_Methods_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Ramapuram2017aRegularization_Methods_more" style="display: none">
    @article{Ramapuram2017a,<br> archiveprefix = {arXiv},<br> arxivid = {1705.09847},<br> author = {Ramapuram, Jason and Gregorova, Magda and Kalousis, Alexandros},<br> eprint = {1705.09847},<br> journal = {arXiv},<br> keywords = {[fashion],[generative],[mnist]},<br> mendeley-tags = {[fashion],[generative],[mnist]},<br> number = {2010},<br> pages = {1--14},<br> title = {Lifelong Generative Modeling},<br> url = {http://arxiv.org/abs/1705.09847},<br> year = {2017}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Ramapuram2017aRegularization_Methods_more2" style="display: none">
    Lifelong learning is the problem of learning multiple consecutive tasks in an online manner and is essential towards the development of intelligent machines that can adapt to their surroundings. In this work we focus on learning a lifelong approach to generative modeling whereby we continuously incorporate newly observed distributions into our model representation. We utilize two models, aptly named the student and the teacher, in order to aggregate information about all past distributions without the preservation of any of the past data or previous models. The teacher is utilized as a form of compressed memory in order to allow for the student model to learn over the past as well as present data. We demonstrate why a naive approach to lifelong generative modeling fails and introduce a regularizer with which we demonstrate learning across a long range of distributions.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Ramapuram2017aRegularization_Methods_more3" style="display: none">
    N.A.
</span></p>
<script>
    function Ramapuram2017aRegularization_MethodsFunction() {
      var moreText = document.getElementById("Ramapuram2017aRegularization_Methods_more");
      var moreText2 = document.getElementById("Ramapuram2017aRegularization_Methods_more2");
      var moreText3 = document.getElementById("Ramapuram2017aRegularization_Methods_more3");
      var btnText = document.getElementById("Ramapuram2017aRegularization_Methods_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Ramapuram2017aRegularization_MethodsFunction2() {
      var moreText = document.getElementById("Ramapuram2017aRegularization_Methods_more2");
      var moreText1 = document.getElementById("Ramapuram2017aRegularization_Methods_more");
      var moreText3 = document.getElementById("Ramapuram2017aRegularization_Methods_more3");
      var btnText = document.getElementById("Ramapuram2017aRegularization_Methods_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Ramapuram2017aRegularization_MethodsFunction3() {
      var moreText = document.getElementById("Ramapuram2017aRegularization_Methods_more3");
      var moreText1 = document.getElementById("Ramapuram2017aRegularization_Methods_more");
      var moreText2 = document.getElementById("Ramapuram2017aRegularization_Methods_more2");
      var btnText = document.getElementById("Ramapuram2017aRegularization_Methods_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="http://proceedings.mlr.press/v70/zenke17a.html">Continual Learning Through Synaptic Intelligence</a> by Friedemann Zenke, Ben Poole and Surya Ganguli. <em>International Conference on Machine Learning</em>, 3987–3995, 2017. <span style='background-color:DarkCyan; padding: 2px; border-radius:4px; border: 1px solid black;'>[cifar]</span> <span style='background-color:MediumAquaMarine; padding: 2px; border-radius:4px; border: 1px solid black;'>[mnist]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Zenke2017aRegularization_MethodsFunction()" id="Zenke2017aRegularization_Methods_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Zenke2017aRegularization_MethodsFunction2()" id="Zenke2017aRegularization_Methods_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Zenke2017aRegularization_MethodsFunction3()" id="Zenke2017aRegularization_Methods_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Zenke2017aRegularization_Methods_more" style="display: none">
    @inproceedings{Zenke2017a,<br> author = {Zenke, Friedemann and Poole, Ben and Ganguli, Surya},<br> booktitle = {International Conference on Machine Learning},<br> keywords = {[cifar],[mnist],mnist},<br> language = {en},<br> mendeley-tags = {[cifar],[mnist]},<br> month = {jul},<br> pages = {3987--3995},<br> title = {Continual Learning Through Synaptic Intelligence},<br> url = {http://proceedings.mlr.press/v70/zenke17a.html},<br> year = {2017}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Zenke2017aRegularization_Methods_more2" style="display: none">
    While deep learning has led to remarkable advances across diverse applications, it struggles in domains where the data distribution changes over the course of learning. In stark contrast, biologica...
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Zenke2017aRegularization_Methods_more3" style="display: none">
    N.A.
</span></p>
<script>
    function Zenke2017aRegularization_MethodsFunction() {
      var moreText = document.getElementById("Zenke2017aRegularization_Methods_more");
      var moreText2 = document.getElementById("Zenke2017aRegularization_Methods_more2");
      var moreText3 = document.getElementById("Zenke2017aRegularization_Methods_more3");
      var btnText = document.getElementById("Zenke2017aRegularization_Methods_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Zenke2017aRegularization_MethodsFunction2() {
      var moreText = document.getElementById("Zenke2017aRegularization_Methods_more2");
      var moreText1 = document.getElementById("Zenke2017aRegularization_Methods_more");
      var moreText3 = document.getElementById("Zenke2017aRegularization_Methods_more3");
      var btnText = document.getElementById("Zenke2017aRegularization_Methods_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Zenke2017aRegularization_MethodsFunction3() {
      var moreText = document.getElementById("Zenke2017aRegularization_Methods_more3");
      var moreText1 = document.getElementById("Zenke2017aRegularization_Methods_more");
      var moreText2 = document.getElementById("Zenke2017aRegularization_Methods_more2");
      var btnText = document.getElementById("Zenke2017aRegularization_Methods_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="http://arxiv.org/abs/1703.08475">Overcoming Catastrophic Forgetting by Incremental Moment Matching</a> by Sang-Woo Lee, Jin-Hwa Kim, Jaehyun Jun, Jung-Woo Ha and Byoung-Tak Zhang. <em>Advances in Neural Information Processing Systems</em>, 4653–4663, 2017. <span style='background-color:DarkCyan; padding: 2px; border-radius:4px; border: 1px solid black;'>[cifar]</span> <span style='background-color:MediumAquaMarine; padding: 2px; border-radius:4px; border: 1px solid black;'>[mnist]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Lee2017aRegularization_MethodsFunction()" id="Lee2017aRegularization_Methods_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Lee2017aRegularization_MethodsFunction2()" id="Lee2017aRegularization_Methods_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Lee2017aRegularization_MethodsFunction3()" id="Lee2017aRegularization_Methods_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Lee2017aRegularization_Methods_more" style="display: none">
    @inproceedings{Lee2017a,<br> archiveprefix = {arXiv},<br> arxivid = {1703.08475},<br> author = {Lee, Sang-Woo and Kim, Jin-Hwa and Jun, Jaehyun and Ha, Jung-Woo and Zhang, Byoung-Tak},<br> booktitle = {Advances in Neural Information Processing Systems},<br> eprint = {1703.08475},<br> file = {::},<br> keywords = {[cifar],[mnist]},<br> mendeley-tags = {[cifar],[mnist]},<br> month = {mar},<br> pages = {4653--4663},<br> publisher = {Neural information processing systems foundation},<br> title = {Overcoming Catastrophic Forgetting by Incremental Moment Matching},<br> url = {http://arxiv.org/abs/1703.08475},<br> volume = {2017-Decem},<br> year = {2017}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Lee2017aRegularization_Methods_more2" style="display: none">
    Catastrophic forgetting is a problem of neural networks that loses the information of the first task after training the second task. Here, we propose a method, i.e. incremental moment matching (IMM), to resolve this problem. IMM incrementally matches the moment of the posterior distribution of the neural network which is trained on the first and the second task, respectively. To make the search space of posterior parameter smooth, the IMM procedure is complemented by various transfer learning techniques including weight transfer, L2-norm of the old and the new parameter, and a variant of dropout with the old parameter. We analyze our approach on a variety of datasets including the MNIST, CIFAR-10, Caltech-UCSD-Birds, and Lifelog datasets. The experimental results show that IMM achieves state-of-the-art performance by balancing the information between an old and a new network.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Lee2017aRegularization_Methods_more3" style="display: none">
    N.A.
</span></p>
<script>
    function Lee2017aRegularization_MethodsFunction() {
      var moreText = document.getElementById("Lee2017aRegularization_Methods_more");
      var moreText2 = document.getElementById("Lee2017aRegularization_Methods_more2");
      var moreText3 = document.getElementById("Lee2017aRegularization_Methods_more3");
      var btnText = document.getElementById("Lee2017aRegularization_Methods_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Lee2017aRegularization_MethodsFunction2() {
      var moreText = document.getElementById("Lee2017aRegularization_Methods_more2");
      var moreText1 = document.getElementById("Lee2017aRegularization_Methods_more");
      var moreText3 = document.getElementById("Lee2017aRegularization_Methods_more3");
      var btnText = document.getElementById("Lee2017aRegularization_Methods_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Lee2017aRegularization_MethodsFunction3() {
      var moreText = document.getElementById("Lee2017aRegularization_Methods_more3");
      var moreText1 = document.getElementById("Lee2017aRegularization_Methods_more");
      var moreText2 = document.getElementById("Lee2017aRegularization_Methods_more2");
      var btnText = document.getElementById("Lee2017aRegularization_Methods_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="http://arxiv.org/abs/1606.09282">Learning without Forgetting</a> by Zhizhong Li and Derek Hoiem. <em>European Conference on Computer Vision</em>, 614–629, 2016. <span style='background-color:DeepPink; padding: 2px; border-radius:4px; border: 1px solid black;'>[imagenet]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Li2016aRegularization_MethodsFunction()" id="Li2016aRegularization_Methods_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Li2016aRegularization_MethodsFunction2()" id="Li2016aRegularization_Methods_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Li2016aRegularization_MethodsFunction3()" id="Li2016aRegularization_Methods_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Li2016aRegularization_Methods_more" style="display: none">
    @inproceedings{Li2016a,<br> annote = {Comment: Conference version appears in ECCV 2016; updated with journal version<br>arXiv: 1606.09282},<br> author = {Li, Zhizhong and Hoiem, Derek},<br> booktitle = {European Conference on Computer Vision},<br> keywords = {Computer Science - Computer Vision and Pattern Rec,Computer Science - Machine Learning,Statistics - Machine Learning,[imagenet]},<br> language = {en},<br> mendeley-tags = {[imagenet]},<br> pages = {614--629},<br> series = {Springer},<br> title = {Learning without Forgetting},<br> url = {http://arxiv.org/abs/1606.09282},<br> year = {2016}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Li2016aRegularization_Methods_more2" style="display: none">
    When building a uniﬁed vision system or gradually adding new capabilities to a system, the usual assumption is that training data for all tasks is always available. However, as the number of tasks grows, storing and retraining on such data becomes infeasible. A new problem arises where we add new capabilities to a Convolutional Neural Network (CNN), but the training data for its existing capabilities are unavailable. We propose our Learning without Forgetting method, which uses only new task data to train the network while preserving the original capabilities. Our method performs favorably compared to commonly used feature extraction and ﬁne-tuning adaption techniques and performs similarly to multitask learning that uses original task data we assume unavailable. A more surprising observation is that Learning without Forgetting may be able to replace ﬁne-tuning with similar old and new task datasets for improved new task performance.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Li2016aRegularization_Methods_more3" style="display: none">
    Comment: Conference version appears in ECCV 2016; updated with journal version
            arXiv: 1606.09282
</span></p>
<script>
    function Li2016aRegularization_MethodsFunction() {
      var moreText = document.getElementById("Li2016aRegularization_Methods_more");
      var moreText2 = document.getElementById("Li2016aRegularization_Methods_more2");
      var moreText3 = document.getElementById("Li2016aRegularization_Methods_more3");
      var btnText = document.getElementById("Li2016aRegularization_Methods_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Li2016aRegularization_MethodsFunction2() {
      var moreText = document.getElementById("Li2016aRegularization_Methods_more2");
      var moreText1 = document.getElementById("Li2016aRegularization_Methods_more");
      var moreText3 = document.getElementById("Li2016aRegularization_Methods_more3");
      var btnText = document.getElementById("Li2016aRegularization_Methods_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Li2016aRegularization_MethodsFunction3() {
      var moreText = document.getElementById("Li2016aRegularization_Methods_more3");
      var moreText1 = document.getElementById("Li2016aRegularization_Methods_more");
      var moreText2 = document.getElementById("Li2016aRegularization_Methods_more2");
      var btnText = document.getElementById("Li2016aRegularization_Methods_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
</ul>
</div>
<div class="section" id="rehearsal-methods">
<h3>Rehearsal Methods<a class="headerlink" href="#rehearsal-methods" title="Permalink to this headline">¶</a></h3>
<p>In this section we collect all the papers introducing a continual learning strategy employing some rehearsal methods.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://openreview.net/forum?id=SJgwNerKvB">Continual learning with hypernetworks</a> by Johannes von Oswald, Christian Henning, João Sacramento and Benjamin F Grewe. <em>International Conference on Learning Representations</em>, 2020. <span style='background-color:DarkCyan; padding: 2px; border-radius:4px; border: 1px solid black;'>[cifar]</span> <span style='background-color:MediumAquaMarine; padding: 2px; border-radius:4px; border: 1px solid black;'>[mnist]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Oswal2019aRehearsal_MethodsFunction()" id="Oswal2019aRehearsal_Methods_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Oswal2019aRehearsal_MethodsFunction2()" id="Oswal2019aRehearsal_Methods_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Oswal2019aRehearsal_MethodsFunction3()" id="Oswal2019aRehearsal_Methods_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Oswal2019aRehearsal_Methods_more" style="display: none">
    @inproceedings{Oswal2019a,<br> author = {von Oswald, Johannes and Henning, Christian and Sacramento, João and Grewe, Benjamin F},<br> booktitle = {International Conference on Learning Representations},<br> keywords = {[cifar],[mnist]},<br> mendeley-tags = {[cifar],[mnist]},<br> month = {sep},<br> title = {Continual learning with hypernetworks},<br> url = {https://openreview.net/forum?id=SJgwNerKvB},<br> year = {2020}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Oswal2019aRehearsal_Methods_more2" style="display: none">
    Artificial neural networks suffer from catastrophic forgetting when they are sequentially trained on multiple tasks. To overcome this problem, we present a novel approach based on task-conditioned...
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Oswal2019aRehearsal_Methods_more3" style="display: none">
    N.A.
</span></p>
<script>
    function Oswal2019aRehearsal_MethodsFunction() {
      var moreText = document.getElementById("Oswal2019aRehearsal_Methods_more");
      var moreText2 = document.getElementById("Oswal2019aRehearsal_Methods_more2");
      var moreText3 = document.getElementById("Oswal2019aRehearsal_Methods_more3");
      var btnText = document.getElementById("Oswal2019aRehearsal_Methods_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Oswal2019aRehearsal_MethodsFunction2() {
      var moreText = document.getElementById("Oswal2019aRehearsal_Methods_more2");
      var moreText1 = document.getElementById("Oswal2019aRehearsal_Methods_more");
      var moreText3 = document.getElementById("Oswal2019aRehearsal_Methods_more3");
      var btnText = document.getElementById("Oswal2019aRehearsal_Methods_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Oswal2019aRehearsal_MethodsFunction3() {
      var moreText = document.getElementById("Oswal2019aRehearsal_Methods_more3");
      var moreText1 = document.getElementById("Oswal2019aRehearsal_Methods_more");
      var moreText2 = document.getElementById("Oswal2019aRehearsal_Methods_more2");
      var btnText = document.getElementById("Oswal2019aRehearsal_Methods_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="http://arxiv.org/abs/2004.03794">CALM: Continuous Adaptive Learning for Language Modeling</a> by Kristjan Arumae and Parminder Bhatia. <em>arXiv</em>, 2020. <span style='background-color:BlueViolet; padding: 2px; border-radius:4px; border: 1px solid black;'>[nlp]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Arumae2020aRehearsal_MethodsFunction()" id="Arumae2020aRehearsal_Methods_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Arumae2020aRehearsal_MethodsFunction2()" id="Arumae2020aRehearsal_Methods_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Arumae2020aRehearsal_MethodsFunction3()" id="Arumae2020aRehearsal_Methods_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Arumae2020aRehearsal_Methods_more" style="display: none">
    @article{Arumae2020a,<br> archiveprefix = {arXiv},<br> arxivid = {2004.03794},<br> author = {Arumae, Kristjan and Bhatia, Parminder},<br> eprint = {2004.03794},<br> journal = {arXiv},<br> keywords = {[nlp]},<br> mendeley-tags = {[nlp]},<br> title = {CALM: Continuous Adaptive Learning for Language Modeling},<br> url = {http://arxiv.org/abs/2004.03794},<br> year = {2020}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Arumae2020aRehearsal_Methods_more2" style="display: none">
    Training large language representation models has become a standard in the natural language processing community. This allows for fine tuning on any number of specific tasks, however, these large high capacity models can continue to train on domain specific unlabeled data to make initialization even more robust for supervised tasks. We demonstrate that in practice these pre-trained models present performance deterioration in the form of catastrophic forgetting when evaluated on tasks from a general domain such as GLUE. In this work we propose CALM, Continuous Adaptive Learning for Language Modeling: techniques to render models which retain knowledge across multiple domains. With these methods, we are able to reduce the performance gap across supervised tasks introduced by task specific models which we demonstrate using a continual learning setting in biomedical and clinical domains.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Arumae2020aRehearsal_Methods_more3" style="display: none">
    N.A.
</span></p>
<script>
    function Arumae2020aRehearsal_MethodsFunction() {
      var moreText = document.getElementById("Arumae2020aRehearsal_Methods_more");
      var moreText2 = document.getElementById("Arumae2020aRehearsal_Methods_more2");
      var moreText3 = document.getElementById("Arumae2020aRehearsal_Methods_more3");
      var btnText = document.getElementById("Arumae2020aRehearsal_Methods_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Arumae2020aRehearsal_MethodsFunction2() {
      var moreText = document.getElementById("Arumae2020aRehearsal_Methods_more2");
      var moreText1 = document.getElementById("Arumae2020aRehearsal_Methods_more");
      var moreText3 = document.getElementById("Arumae2020aRehearsal_Methods_more3");
      var btnText = document.getElementById("Arumae2020aRehearsal_Methods_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Arumae2020aRehearsal_MethodsFunction3() {
      var moreText = document.getElementById("Arumae2020aRehearsal_Methods_more3");
      var moreText1 = document.getElementById("Arumae2020aRehearsal_Methods_more");
      var moreText2 = document.getElementById("Arumae2020aRehearsal_Methods_more2");
      var btnText = document.getElementById("Arumae2020aRehearsal_Methods_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="http://arxiv.org/abs/2004.09578">CLOPS: Continual Learning of Physiological Signals</a> by Dani Kiyasseh, Tingting Zhu and David A Clifton. <em>arXiv</em>, 2020.  <br>
<button style="font-size:75%; line-height:15px" onclick="Kiyasseh2020Rehearsal_MethodsFunction()" id="Kiyasseh2020Rehearsal_Methods_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Kiyasseh2020Rehearsal_MethodsFunction2()" id="Kiyasseh2020Rehearsal_Methods_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Kiyasseh2020Rehearsal_MethodsFunction3()" id="Kiyasseh2020Rehearsal_Methods_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Kiyasseh2020Rehearsal_Methods_more" style="display: none">
    @article{Kiyasseh2020,<br> archiveprefix = {arXiv},<br> arxivid = {2004.09578},<br> author = {Kiyasseh, Dani and Zhu, Tingting and Clifton, David A},<br> eprint = {2004.09578},<br> journal = {arXiv},<br> title = {CLOPS: Continual Learning of Physiological Signals},<br> url = {http://arxiv.org/abs/2004.09578},<br> year = {2020}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Kiyasseh2020Rehearsal_Methods_more2" style="display: none">
    Deep learning algorithms are known to experience destructive interference when instances violate the assumption of being independent and identically distributed (i.i.d). This violation, however, is ubiquitous in clinical settings where data are streamed temporally and from a multitude of physiological sensors. To overcome this obstacle, we propose CLOPS, a healthcare-specific replay-based continual learning strategy. In three continual learning scenarios based on three publically-available datasets, we show that CLOPS can outperform its multi-task learning counterpart. Moreover, we propose end-to-end trainable parameters, which we term task-instance parameters, that can be used to quantify task difficulty and similarity. This quantification yields insights into both network interpretability and clinical applications, where task difficulty is poorly quantified.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Kiyasseh2020Rehearsal_Methods_more3" style="display: none">
    N.A.
</span></p>
<script>
    function Kiyasseh2020Rehearsal_MethodsFunction() {
      var moreText = document.getElementById("Kiyasseh2020Rehearsal_Methods_more");
      var moreText2 = document.getElementById("Kiyasseh2020Rehearsal_Methods_more2");
      var moreText3 = document.getElementById("Kiyasseh2020Rehearsal_Methods_more3");
      var btnText = document.getElementById("Kiyasseh2020Rehearsal_Methods_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Kiyasseh2020Rehearsal_MethodsFunction2() {
      var moreText = document.getElementById("Kiyasseh2020Rehearsal_Methods_more2");
      var moreText1 = document.getElementById("Kiyasseh2020Rehearsal_Methods_more");
      var moreText3 = document.getElementById("Kiyasseh2020Rehearsal_Methods_more3");
      var btnText = document.getElementById("Kiyasseh2020Rehearsal_Methods_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Kiyasseh2020Rehearsal_MethodsFunction3() {
      var moreText = document.getElementById("Kiyasseh2020Rehearsal_Methods_more3");
      var moreText1 = document.getElementById("Kiyasseh2020Rehearsal_Methods_more");
      var moreText2 = document.getElementById("Kiyasseh2020Rehearsal_Methods_more2");
      var btnText = document.getElementById("Kiyasseh2020Rehearsal_Methods_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="http://papers.nips.cc/paper/9357-online-continual-learning-with-maximal-interfered-retrieval.pdf">Online Continual Learning with Maximal Interfered Retrieval</a> by Rahaf Aljundi, Eugene Belilovsky, Tinne Tuytelaars, Laurent Charlin, Massimo Caccia, Min Lin and Lucas Page-Caccia. <em>Advances in Neural Information Processing Systems 32</em>, 11849–11860, 2019. <span style='background-color:DarkCyan; padding: 2px; border-radius:4px; border: 1px solid black;'>[cifar]</span> <span style='background-color:MediumAquaMarine; padding: 2px; border-radius:4px; border: 1px solid black;'>[mnist]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Aljundi2019aRehearsal_MethodsFunction()" id="Aljundi2019aRehearsal_Methods_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Aljundi2019aRehearsal_MethodsFunction2()" id="Aljundi2019aRehearsal_Methods_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Aljundi2019aRehearsal_MethodsFunction3()" id="Aljundi2019aRehearsal_Methods_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Aljundi2019aRehearsal_Methods_more" style="display: none">
    @inproceedings{Aljundi2019a,<br> author = {Aljundi, Rahaf and Belilovsky, Eugene and Tuytelaars, Tinne and Charlin, Laurent and Caccia, Massimo and Lin, Min and Page-Caccia, Lucas},<br> booktitle = {Advances in Neural Information Processing Systems 32},<br> editor = {Wallach, H and Larochelle, H and Beygelzimer, A and d' Alché-Buc, F and Fox, E and Garnett, R},<br> keywords = {[cifar],[mnist]},<br> mendeley-tags = {[cifar],[mnist]},<br> pages = {11849--11860},<br> publisher = {Curran Associates, Inc.},<br> title = {Online Continual Learning with Maximal Interfered Retrieval},<br> url = {http://papers.nips.cc/paper/9357-online-continual-learning-with-maximal-interfered-retrieval.pdf},<br> year = {2019}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Aljundi2019aRehearsal_Methods_more2" style="display: none">
    N.A.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Aljundi2019aRehearsal_Methods_more3" style="display: none">
    N.A.
</span></p>
<script>
    function Aljundi2019aRehearsal_MethodsFunction() {
      var moreText = document.getElementById("Aljundi2019aRehearsal_Methods_more");
      var moreText2 = document.getElementById("Aljundi2019aRehearsal_Methods_more2");
      var moreText3 = document.getElementById("Aljundi2019aRehearsal_Methods_more3");
      var btnText = document.getElementById("Aljundi2019aRehearsal_Methods_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Aljundi2019aRehearsal_MethodsFunction2() {
      var moreText = document.getElementById("Aljundi2019aRehearsal_Methods_more2");
      var moreText1 = document.getElementById("Aljundi2019aRehearsal_Methods_more");
      var moreText3 = document.getElementById("Aljundi2019aRehearsal_Methods_more3");
      var btnText = document.getElementById("Aljundi2019aRehearsal_Methods_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Aljundi2019aRehearsal_MethodsFunction3() {
      var moreText = document.getElementById("Aljundi2019aRehearsal_Methods_more3");
      var moreText1 = document.getElementById("Aljundi2019aRehearsal_Methods_more");
      var moreText2 = document.getElementById("Aljundi2019aRehearsal_Methods_more2");
      var btnText = document.getElementById("Aljundi2019aRehearsal_Methods_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
</ul>
</div>
<div class="section" id="generative-replay-methods">
<h3>Generative Replay Methods<a class="headerlink" href="#generative-replay-methods" title="Permalink to this headline">¶</a></h3>
<p>In this section we collect all the papers introducing a continual learning strategy employing some generative replay methods.</p>
<ul class="simple">
<li><p><a class="reference external" href="http://arxiv.org/abs/1906.00654">Continual Learning of New Sound Classes using Generative Replay</a> by Zhepei Wang, Cem Subakan, Efthymios Tzinis, Paris Smaragdis and Laurent Charlin. <em>arXiv</em>, 2019. <span style='background-color:#50EBEC; padding: 2px; border-radius:4px; border: 1px solid black;'>[audio]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Wang2019aGenerative_Replay_MethodsFunction()" id="Wang2019aGenerative_Replay_Methods_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Wang2019aGenerative_Replay_MethodsFunction2()" id="Wang2019aGenerative_Replay_Methods_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Wang2019aGenerative_Replay_MethodsFunction3()" id="Wang2019aGenerative_Replay_Methods_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Wang2019aGenerative_Replay_Methods_more" style="display: none">
    @article{Wang2019a,<br> annote = {arXiv: 1906.00654},<br> author = {Wang, Zhepei and Subakan, Cem and Tzinis, Efthymios and Smaragdis, Paris and Charlin, Laurent},<br> journal = {arXiv},<br> keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio,Statistics - Machine Learning,[audio],audio,sequence,sequences,time series},<br> mendeley-tags = {[audio]},<br> month = {jun},<br> title = {Continual Learning of New Sound Classes using Generative Replay},<br> url = {http://arxiv.org/abs/1906.00654},<br> year = {2019}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Wang2019aGenerative_Replay_Methods_more2" style="display: none">
    Continual learning consists in incrementally training a model on a sequence of datasets and testing on the union of all datasets. In this paper, we examine continual learning for the problem of sound classification, in which we wish to refine already trained models to learn new sound classes. In practice one does not want to maintain all past training data and retrain from scratch, but naively updating a model with new data(sets) results in a degradation of already learned tasks, which is referred to as "catastrophic forgetting." We develop a generative replay procedure for generating training audio spectrogram data, in place of keeping older training datasets. We show that by incrementally refining a classifier with generative replay a generator that is 4% of the size of all previous training data matches the performance of refining the classifier keeping 20% of all previous training data. We thus conclude that we can extend a trained sound classifier to learn new classes without having to keep previously used datasets.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Wang2019aGenerative_Replay_Methods_more3" style="display: none">
    arXiv: 1906.00654
</span></p>
<script>
    function Wang2019aGenerative_Replay_MethodsFunction() {
      var moreText = document.getElementById("Wang2019aGenerative_Replay_Methods_more");
      var moreText2 = document.getElementById("Wang2019aGenerative_Replay_Methods_more2");
      var moreText3 = document.getElementById("Wang2019aGenerative_Replay_Methods_more3");
      var btnText = document.getElementById("Wang2019aGenerative_Replay_Methods_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Wang2019aGenerative_Replay_MethodsFunction2() {
      var moreText = document.getElementById("Wang2019aGenerative_Replay_Methods_more2");
      var moreText1 = document.getElementById("Wang2019aGenerative_Replay_Methods_more");
      var moreText3 = document.getElementById("Wang2019aGenerative_Replay_Methods_more3");
      var btnText = document.getElementById("Wang2019aGenerative_Replay_Methods_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Wang2019aGenerative_Replay_MethodsFunction3() {
      var moreText = document.getElementById("Wang2019aGenerative_Replay_Methods_more3");
      var moreText1 = document.getElementById("Wang2019aGenerative_Replay_Methods_more");
      var moreText2 = document.getElementById("Wang2019aGenerative_Replay_Methods_more2");
      var btnText = document.getElementById("Wang2019aGenerative_Replay_Methods_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="https://www.frontiersin.org/articles/10.3389/fnbot.2018.00078/full">Lifelong Learning of Spatiotemporal Representations With Dual-Memory Recurrent Self-Organization</a> by German I Parisi, Jun Tani, Cornelius Weber and Stefan Wermter. <em>Frontiers in Neurorobotics</em>, 2018. <span style='background-color:Chartreuse; padding: 2px; border-radius:4px; border: 1px solid black;'>[core50]</span> <span style='background-color:green; padding: 2px; border-radius:4px; border: 1px solid black;'>[dual]</span> <span style='background-color:Cornsilk; padding: 2px; border-radius:4px; border: 1px solid black;'>[som]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Parisi2018aGenerative_Replay_MethodsFunction()" id="Parisi2018aGenerative_Replay_Methods_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Parisi2018aGenerative_Replay_MethodsFunction2()" id="Parisi2018aGenerative_Replay_Methods_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Parisi2018aGenerative_Replay_MethodsFunction3()" id="Parisi2018aGenerative_Replay_Methods_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Parisi2018aGenerative_Replay_Methods_more" style="display: none">
    @article{Parisi2018a,<br> author = {Parisi, German I and Tani, Jun and Weber, Cornelius and Wermter, Stefan},<br> doi = {10.3389/fnbot.2018.00078},<br> issn = {1662-5218},<br> journal = {Frontiers in Neurorobotics},<br> keywords = {CLS,Incremental Learning,Lifelong learning,Memory,Self-organizing Network,[core50],[dual],[som],object recognition systems},<br> language = {English},<br> mendeley-tags = {[core50],[dual],[som]},<br> title = {Lifelong Learning of Spatiotemporal Representations With Dual-Memory Recurrent Self-Organization},<br> url = {https://www.frontiersin.org/articles/10.3389/fnbot.2018.00078/full},<br> volume = {12},<br> year = {2018}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Parisi2018aGenerative_Replay_Methods_more2" style="display: none">
    Artificial autonomous agents and robots interacting in complex environments are required to continually acquire and fine-tune knowledge over sustained periods of time. The ability to learn from continuous streams of information is referred to as lifelong learning and represents a long-standing challenge for neural network models due to catastrophic forgetting in which novel sensory experience interferes with existing representations and leads to abrupt decreases in the performance on previously acquired knowledge. Computational models of lifelong learning typically alleviate catastrophic forgetting in experimental scenarios with given datasets of static images and limited complexity, thereby differing significantly from the conditions artificial agents are exposed to. In more natural settings, sequential information may become progressively available over time and access to previous experience may be restricted. Therefore, specialized neural network mechanisms are required that adapt to novel sequential experience while preventing disruptive interference with existing representations. In this paper, we propose a dual-memory self-organizing architecture for lifelong learning scenarios. The architecture comprises two growing recurrent networks with the complementary tasks of learning object instances (episodic memory) and categories (semantic memory). Both growing networks can expand in response to novel sensory experience: the episodic memory learns fine-grained spatiotemporal representations of object instances in an unsupervised fashion while the semantic memory uses task-relevant signals to regulate structural plasticity levels and develop more compact representations from episodic experience. For the consolidation of knowledge in the absence of external sensory input, the episodic memory periodically replays trajectories of neural reactivations. We evaluate the proposed model on the CORe50 benchmark dataset for continuous object recognition, showing that we significantly outperform current methods of lifelong learning in three different incremental learning scenarios.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Parisi2018aGenerative_Replay_Methods_more3" style="display: none">
    N.A.
</span></p>
<script>
    function Parisi2018aGenerative_Replay_MethodsFunction() {
      var moreText = document.getElementById("Parisi2018aGenerative_Replay_Methods_more");
      var moreText2 = document.getElementById("Parisi2018aGenerative_Replay_Methods_more2");
      var moreText3 = document.getElementById("Parisi2018aGenerative_Replay_Methods_more3");
      var btnText = document.getElementById("Parisi2018aGenerative_Replay_Methods_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Parisi2018aGenerative_Replay_MethodsFunction2() {
      var moreText = document.getElementById("Parisi2018aGenerative_Replay_Methods_more2");
      var moreText1 = document.getElementById("Parisi2018aGenerative_Replay_Methods_more");
      var moreText3 = document.getElementById("Parisi2018aGenerative_Replay_Methods_more3");
      var btnText = document.getElementById("Parisi2018aGenerative_Replay_Methods_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Parisi2018aGenerative_Replay_MethodsFunction3() {
      var moreText = document.getElementById("Parisi2018aGenerative_Replay_Methods_more3");
      var moreText1 = document.getElementById("Parisi2018aGenerative_Replay_Methods_more");
      var moreText2 = document.getElementById("Parisi2018aGenerative_Replay_Methods_more2");
      var btnText = document.getElementById("Parisi2018aGenerative_Replay_Methods_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="http://papers.nips.cc/paper/6892-continual-learning-with-deep-generative-replay.pdf">Continual Learning with Deep Generative Replay</a> by Hanul Shin, Jung Kwon Lee, Jaehong Kim and Jiwon Kim. <em>Advances in Neural Information Processing Systems 30</em>, 2990–2999, 2017. <span style='background-color:MediumAquaMarine; padding: 2px; border-radius:4px; border: 1px solid black;'>[mnist]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Shin2017aGenerative_Replay_MethodsFunction()" id="Shin2017aGenerative_Replay_Methods_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Shin2017aGenerative_Replay_MethodsFunction2()" id="Shin2017aGenerative_Replay_Methods_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Shin2017aGenerative_Replay_MethodsFunction3()" id="Shin2017aGenerative_Replay_Methods_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Shin2017aGenerative_Replay_Methods_more" style="display: none">
    @inproceedings{Shin2017a,<br> author = {Shin, Hanul and Lee, Jung Kwon and Kim, Jaehong and Kim, Jiwon},<br> booktitle = {Advances in Neural Information Processing Systems 30},<br> editor = {Guyon, I and Luxburg, U V and Bengio, S and Wallach, H and Fergus, R and Vishwanathan, S and Garnett, R},<br> keywords = {[mnist]},<br> mendeley-tags = {[mnist]},<br> pages = {2990--2999},<br> publisher = {Curran Associates, Inc.},<br> title = {Continual Learning with Deep Generative Replay},<br> url = {http://papers.nips.cc/paper/6892-continual-learning-with-deep-generative-replay.pdf},<br> year = {2017}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Shin2017aGenerative_Replay_Methods_more2" style="display: none">
    N.A.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Shin2017aGenerative_Replay_Methods_more3" style="display: none">
    N.A.
</span></p>
<script>
    function Shin2017aGenerative_Replay_MethodsFunction() {
      var moreText = document.getElementById("Shin2017aGenerative_Replay_Methods_more");
      var moreText2 = document.getElementById("Shin2017aGenerative_Replay_Methods_more2");
      var moreText3 = document.getElementById("Shin2017aGenerative_Replay_Methods_more3");
      var btnText = document.getElementById("Shin2017aGenerative_Replay_Methods_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Shin2017aGenerative_Replay_MethodsFunction2() {
      var moreText = document.getElementById("Shin2017aGenerative_Replay_Methods_more2");
      var moreText1 = document.getElementById("Shin2017aGenerative_Replay_Methods_more");
      var moreText3 = document.getElementById("Shin2017aGenerative_Replay_Methods_more3");
      var btnText = document.getElementById("Shin2017aGenerative_Replay_Methods_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Shin2017aGenerative_Replay_MethodsFunction3() {
      var moreText = document.getElementById("Shin2017aGenerative_Replay_Methods_more3");
      var moreText1 = document.getElementById("Shin2017aGenerative_Replay_Methods_more");
      var moreText2 = document.getElementById("Shin2017aGenerative_Replay_Methods_more2");
      var btnText = document.getElementById("Shin2017aGenerative_Replay_Methods_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
</ul>
</div>
<div class="section" id="hybrid-methods">
<h3>Hybrid Methods<a class="headerlink" href="#hybrid-methods" title="Permalink to this headline">¶</a></h3>
<p>In this section we collect all the papers introducing a continual learning strategy employing some hybrid methods.</p>
<ul class="simple">
<li><p><a class="reference external" href="http://arxiv.org/abs/1906.00654">Continual Learning of New Sound Classes using Generative Replay</a> by Zhepei Wang, Cem Subakan, Efthymios Tzinis, Paris Smaragdis and Laurent Charlin. <em>arXiv</em>, 2019. <span style='background-color:#50EBEC; padding: 2px; border-radius:4px; border: 1px solid black;'>[audio]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Wang2019aHybrid_MethodsFunction()" id="Wang2019aHybrid_Methods_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Wang2019aHybrid_MethodsFunction2()" id="Wang2019aHybrid_Methods_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Wang2019aHybrid_MethodsFunction3()" id="Wang2019aHybrid_Methods_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Wang2019aHybrid_Methods_more" style="display: none">
    @article{Wang2019a,<br> annote = {arXiv: 1906.00654},<br> author = {Wang, Zhepei and Subakan, Cem and Tzinis, Efthymios and Smaragdis, Paris and Charlin, Laurent},<br> journal = {arXiv},<br> keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio,Statistics - Machine Learning,[audio],audio,sequence,sequences,time series},<br> mendeley-tags = {[audio]},<br> month = {jun},<br> title = {Continual Learning of New Sound Classes using Generative Replay},<br> url = {http://arxiv.org/abs/1906.00654},<br> year = {2019}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Wang2019aHybrid_Methods_more2" style="display: none">
    Continual learning consists in incrementally training a model on a sequence of datasets and testing on the union of all datasets. In this paper, we examine continual learning for the problem of sound classification, in which we wish to refine already trained models to learn new sound classes. In practice one does not want to maintain all past training data and retrain from scratch, but naively updating a model with new data(sets) results in a degradation of already learned tasks, which is referred to as "catastrophic forgetting." We develop a generative replay procedure for generating training audio spectrogram data, in place of keeping older training datasets. We show that by incrementally refining a classifier with generative replay a generator that is 4% of the size of all previous training data matches the performance of refining the classifier keeping 20% of all previous training data. We thus conclude that we can extend a trained sound classifier to learn new classes without having to keep previously used datasets.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Wang2019aHybrid_Methods_more3" style="display: none">
    arXiv: 1906.00654
</span></p>
<script>
    function Wang2019aHybrid_MethodsFunction() {
      var moreText = document.getElementById("Wang2019aHybrid_Methods_more");
      var moreText2 = document.getElementById("Wang2019aHybrid_Methods_more2");
      var moreText3 = document.getElementById("Wang2019aHybrid_Methods_more3");
      var btnText = document.getElementById("Wang2019aHybrid_Methods_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Wang2019aHybrid_MethodsFunction2() {
      var moreText = document.getElementById("Wang2019aHybrid_Methods_more2");
      var moreText1 = document.getElementById("Wang2019aHybrid_Methods_more");
      var moreText3 = document.getElementById("Wang2019aHybrid_Methods_more3");
      var btnText = document.getElementById("Wang2019aHybrid_Methods_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Wang2019aHybrid_MethodsFunction3() {
      var moreText = document.getElementById("Wang2019aHybrid_Methods_more3");
      var moreText1 = document.getElementById("Wang2019aHybrid_Methods_more");
      var moreText2 = document.getElementById("Wang2019aHybrid_Methods_more2");
      var btnText = document.getElementById("Wang2019aHybrid_Methods_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="http://proceedings.mlr.press/v80/schwarz18a.html">Progress &amp; Compress: A scalable framework for continual learning</a> by Jonathan Schwarz, Wojciech Czarnecki, Jelena Luketina, Agnieszka Grabska-Barwinska, Yee Whye Teh, Razvan Pascanu and Raia Hadsell. <em>International Conference on Machine Learning</em>, 4528–4537, 2018. <span style='background-color:#E5E4E2; padding: 2px; border-radius:4px; border: 1px solid black;'>[vision]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Schwarz2018aHybrid_MethodsFunction()" id="Schwarz2018aHybrid_Methods_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Schwarz2018aHybrid_MethodsFunction2()" id="Schwarz2018aHybrid_Methods_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Schwarz2018aHybrid_MethodsFunction3()" id="Schwarz2018aHybrid_Methods_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Schwarz2018aHybrid_Methods_more" style="display: none">
    @inproceedings{Schwarz2018a,<br> author = {Schwarz, Jonathan and Czarnecki, Wojciech and Luketina, Jelena and Grabska-Barwinska, Agnieszka and Teh, Yee Whye and Pascanu, Razvan and Hadsell, Raia},<br> booktitle = {International Conference on Machine Learning},<br> keywords = {[vision],ewc,normalized ewc,online ewc},<br> language = {en},<br> mendeley-tags = {[vision]},<br> month = {jul},<br> pages = {4528--4537},<br> shorttitle = {Progress & Compress},<br> title = {Progress & Compress: A scalable framework for continual learning},<br> url = {http://proceedings.mlr.press/v80/schwarz18a.html},<br> year = {2018}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Schwarz2018aHybrid_Methods_more2" style="display: none">
    We introduce a conceptually simple and scalable framework for continual learning domains where tasks are learned sequentially. Our method is constant in the number of parameters and is designed to ...
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Schwarz2018aHybrid_Methods_more3" style="display: none">
    N.A.
</span></p>
<script>
    function Schwarz2018aHybrid_MethodsFunction() {
      var moreText = document.getElementById("Schwarz2018aHybrid_Methods_more");
      var moreText2 = document.getElementById("Schwarz2018aHybrid_Methods_more2");
      var moreText3 = document.getElementById("Schwarz2018aHybrid_Methods_more3");
      var btnText = document.getElementById("Schwarz2018aHybrid_Methods_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Schwarz2018aHybrid_MethodsFunction2() {
      var moreText = document.getElementById("Schwarz2018aHybrid_Methods_more2");
      var moreText1 = document.getElementById("Schwarz2018aHybrid_Methods_more");
      var moreText3 = document.getElementById("Schwarz2018aHybrid_Methods_more3");
      var btnText = document.getElementById("Schwarz2018aHybrid_Methods_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Schwarz2018aHybrid_MethodsFunction3() {
      var moreText = document.getElementById("Schwarz2018aHybrid_Methods_more3");
      var moreText1 = document.getElementById("Schwarz2018aHybrid_Methods_more");
      var moreText2 = document.getElementById("Schwarz2018aHybrid_Methods_more2");
      var btnText = document.getElementById("Schwarz2018aHybrid_Methods_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="http://arxiv.org/abs/1806.08568">Continuous Learning in Single-Incremental-Task Scenarios</a> by Davide Maltoni and Vincenzo Lomonaco. <em>arXiv</em>, 2018. <span style='background-color:Chartreuse; padding: 2px; border-radius:4px; border: 1px solid black;'>[core50]</span> <span style='background-color:Tomato; padding: 2px; border-radius:4px; border: 1px solid black;'>[framework]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Maltoni2018aHybrid_MethodsFunction()" id="Maltoni2018aHybrid_Methods_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Maltoni2018aHybrid_MethodsFunction2()" id="Maltoni2018aHybrid_Methods_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Maltoni2018aHybrid_MethodsFunction3()" id="Maltoni2018aHybrid_Methods_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Maltoni2018aHybrid_Methods_more" style="display: none">
    @article{Maltoni2018a,<br> annote = {Comment: 26 pages, 13 figures; v3: major revision (e.g. added Sec. 4.4), several typos and minor mistakes corrected<br>arXiv: 1806.08568},<br> author = {Maltoni, Davide and Lomonaco, Vincenzo},<br> journal = {arXiv},<br> keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Rec,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computi,Continuous learning,Deep learning,Incremental class learning,Lifelong learning,Object recognition,Single-incremental-task,Statistics - Machine Learning,[core50],[framework],ewc,incremental task,review},<br> language = {en},<br> mendeley-tags = {[core50],[framework]},<br> month = {jun},<br> title = {Continuous Learning in Single-Incremental-Task Scenarios},<br> url = {http://arxiv.org/abs/1806.08568},<br> year = {2018}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Maltoni2018aHybrid_Methods_more2" style="display: none">
    It was recently shown that architectural, regularization and rehearsal strategies can be used to train deep models sequentially on a number of disjoint tasks without forgetting previously acquired knowledge. However, these strategies are still unsatisfactory if the tasks are not disjoint but constitute a single incremental task (e.g., class-incremental learning). In this paper we point out the differences between multi-task and single-incremental-task scenarios and show that well-known approaches such as LWF, EWC and SI are not ideal for incremental task scenarios. A new approach, denoted as AR1, combining architectural and regularization strategies is then speciﬁcally proposed. AR1 overhead (in terms of memory and computation) is very small thus making it suitable for online learning. When tested on CORe50 and iCIFAR-100, AR1 outperformed existing regularization strategies by a good margin.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Maltoni2018aHybrid_Methods_more3" style="display: none">
    Comment: 26 pages, 13 figures; v3: major revision (e.g. added Sec. 4.4), several typos and minor mistakes corrected
            arXiv: 1806.08568
</span></p>
<script>
    function Maltoni2018aHybrid_MethodsFunction() {
      var moreText = document.getElementById("Maltoni2018aHybrid_Methods_more");
      var moreText2 = document.getElementById("Maltoni2018aHybrid_Methods_more2");
      var moreText3 = document.getElementById("Maltoni2018aHybrid_Methods_more3");
      var btnText = document.getElementById("Maltoni2018aHybrid_Methods_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Maltoni2018aHybrid_MethodsFunction2() {
      var moreText = document.getElementById("Maltoni2018aHybrid_Methods_more2");
      var moreText1 = document.getElementById("Maltoni2018aHybrid_Methods_more");
      var moreText3 = document.getElementById("Maltoni2018aHybrid_Methods_more3");
      var btnText = document.getElementById("Maltoni2018aHybrid_Methods_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Maltoni2018aHybrid_MethodsFunction3() {
      var moreText = document.getElementById("Maltoni2018aHybrid_Methods_more3");
      var moreText1 = document.getElementById("Maltoni2018aHybrid_Methods_more");
      var moreText2 = document.getElementById("Maltoni2018aHybrid_Methods_more2");
      var btnText = document.getElementById("Maltoni2018aHybrid_Methods_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
</ul>
</div>
<div class="section" id="metrics-and-evaluations">
<h3>Metrics and Evaluations<a class="headerlink" href="#metrics-and-evaluations" title="Permalink to this headline">¶</a></h3>
<p>In this section we list all the papers related to the continual learning evalution protocols and metrics.</p>
<ul class="simple">
<li><p><a class="reference external" href="http://arxiv.org/abs/1912.03049">Regularization Shortcomings for Continual Learning</a> by Timothée Lesort, Andrei Stoian and David Filliat. <em>arXiv</em>, 2020. <span style='background-color:LightGray; padding: 2px; border-radius:4px; border: 1px solid black;'>[fashion]</span> <span style='background-color:MediumAquaMarine; padding: 2px; border-radius:4px; border: 1px solid black;'>[mnist]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Lesort2020bMetrics_and_EvaluationsFunction()" id="Lesort2020bMetrics_and_Evaluations_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Lesort2020bMetrics_and_EvaluationsFunction2()" id="Lesort2020bMetrics_and_Evaluations_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Lesort2020bMetrics_and_EvaluationsFunction3()" id="Lesort2020bMetrics_and_Evaluations_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Lesort2020bMetrics_and_Evaluations_more" style="display: none">
    @article{Lesort2020b,<br> annote = {arXiv: 1912.03049},<br> author = {Lesort, Timothée and Stoian, Andrei and Filliat, David},<br> journal = {arXiv},<br> keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,[fashion],[mnist],class incremental,regularization},<br> mendeley-tags = {[fashion],[mnist]},<br> month = {feb},<br> title = {Regularization Shortcomings for Continual Learning},<br> url = {http://arxiv.org/abs/1912.03049},<br> year = {2020}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Lesort2020bMetrics_and_Evaluations_more2" style="display: none">
    In most machine learning algorithms, training data are assumed independent and identically distributed (iid). Otherwise, the algorithms' performances are challenged. A famous phenomenon with non-iid data distribution is known as \saycatastrophic forgetting. Algorithms dealing with it are gathered in the \textitContinual Learning research field. In this article, we study the \textitregularization based approaches to continual learning. We show that those approaches can not learn to discriminate classes from different tasks in an elemental continual benchmark: class-incremental setting. We make theoretical reasoning to prove this shortcoming and illustrate it with examples and experiments.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Lesort2020bMetrics_and_Evaluations_more3" style="display: none">
    arXiv: 1912.03049
</span></p>
<script>
    function Lesort2020bMetrics_and_EvaluationsFunction() {
      var moreText = document.getElementById("Lesort2020bMetrics_and_Evaluations_more");
      var moreText2 = document.getElementById("Lesort2020bMetrics_and_Evaluations_more2");
      var moreText3 = document.getElementById("Lesort2020bMetrics_and_Evaluations_more3");
      var btnText = document.getElementById("Lesort2020bMetrics_and_Evaluations_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Lesort2020bMetrics_and_EvaluationsFunction2() {
      var moreText = document.getElementById("Lesort2020bMetrics_and_Evaluations_more2");
      var moreText1 = document.getElementById("Lesort2020bMetrics_and_Evaluations_more");
      var moreText3 = document.getElementById("Lesort2020bMetrics_and_Evaluations_more3");
      var btnText = document.getElementById("Lesort2020bMetrics_and_Evaluations_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Lesort2020bMetrics_and_EvaluationsFunction3() {
      var moreText = document.getElementById("Lesort2020bMetrics_and_Evaluations_more3");
      var moreText1 = document.getElementById("Lesort2020bMetrics_and_Evaluations_more");
      var moreText2 = document.getElementById("Lesort2020bMetrics_and_Evaluations_more2");
      var btnText = document.getElementById("Lesort2020bMetrics_and_Evaluations_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="http://www.sciencedirect.com/science/article/pii/S1566253519307377">Continual learning for robotics: Definition, framework, learning strategies, opportunities and challenges</a> by Timothée Lesort, Vincenzo Lomonaco, Andrei Stoian, Davide Maltoni, David Filliat and Natalia Díaz-Rodrǵuez. <em>Information Fusion</em>, 52–68, 2020. <span style='background-color:Tomato; padding: 2px; border-radius:4px; border: 1px solid black;'>[framework]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Lesort2020aMetrics_and_EvaluationsFunction()" id="Lesort2020aMetrics_and_Evaluations_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Lesort2020aMetrics_and_EvaluationsFunction2()" id="Lesort2020aMetrics_and_Evaluations_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Lesort2020aMetrics_and_EvaluationsFunction3()" id="Lesort2020aMetrics_and_Evaluations_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Lesort2020aMetrics_and_Evaluations_more" style="display: none">
    @article{Lesort2020a,<br> annote = {Overview of a CL framework for applications in robotic, together with discussion of existing CL strategies and techniques.},<br> author = {Lesort, Timothée and Lomonaco, Vincenzo and Stoian, Andrei and Maltoni, Davide and Filliat, David and Díaz-Rodr\ǵuez, Natalia},<br> doi = {10.1016/j.inffus.2019.12.004},<br> issn = {1566-2535},<br> journal = {Information Fusion},<br> keywords = {Catastrophic Forgetting,Continual Learning,Deep Learning,Lifelong Learning,Reinforcement Learning,Robotics,[framework]},<br> language = {en},<br> mendeley-tags = {[framework]},<br> month = {jun},<br> pages = {52--68},<br> shorttitle = {Continual learning for robotics},<br> title = {Continual learning for robotics: Definition, framework, learning strategies, opportunities and challenges},<br> url = {http://www.sciencedirect.com/science/article/pii/S1566253519307377},<br> volume = {58},<br> year = {2020}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Lesort2020aMetrics_and_Evaluations_more2" style="display: none">
    Continual learning (CL) is a particular machine learning paradigm where the data distribution and learning objective change through time, or where all the training data and objective criteria are never available at once. The evolution of the learning process is modeled by a sequence of learning experiences where the goal is to be able to learn new skills all along the sequence without forgetting what has been previously learned. CL can be seen as an online learning where knowledge fusion needs to take place in order to learn from streams of data presented sequentially in time. Continual learning also aims at the same time at optimizing the memory, the computation power and the speed during the learning process. An important challenge for machine learning is not necessarily finding solutions that work in the real world but rather finding stable algorithms that can learn in real world. Hence, the ideal approach would be tackling the real world in a embodied platform: an autonomous agent. Continual learning would then be effective in an autonomous agent or robot, which would learn autonomously through time about the external world, and incrementally develop a set of complex skills and knowledge.Robotic agents have to learn to adapt and interact with their environment using a continuous stream of observations. Some recent approaches aim at tackling continual learning for robotics, but most recent papers on continual learning only experiment approaches in simulation or with static datasets. Unfortunately, the evaluation of those algorithms does not provide insights on whether their solutions may help continual learning in the context of robotics. This paper aims at reviewing the existing state of the art of continual learning, summarizing existing benchmarks and metrics, and proposing a framework for presenting and evaluating both robotics and non robotics approaches in a way that makes transfer between both fields easier. We put light on continual learning in the context of robotics to create connections between fields and normalize approaches.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Lesort2020aMetrics_and_Evaluations_more3" style="display: none">
    Overview of a CL framework for applications in robotic, together with discussion of existing CL strategies and techniques.
</span></p>
<script>
    function Lesort2020aMetrics_and_EvaluationsFunction() {
      var moreText = document.getElementById("Lesort2020aMetrics_and_Evaluations_more");
      var moreText2 = document.getElementById("Lesort2020aMetrics_and_Evaluations_more2");
      var moreText3 = document.getElementById("Lesort2020aMetrics_and_Evaluations_more3");
      var btnText = document.getElementById("Lesort2020aMetrics_and_Evaluations_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Lesort2020aMetrics_and_EvaluationsFunction2() {
      var moreText = document.getElementById("Lesort2020aMetrics_and_Evaluations_more2");
      var moreText1 = document.getElementById("Lesort2020aMetrics_and_Evaluations_more");
      var moreText3 = document.getElementById("Lesort2020aMetrics_and_Evaluations_more3");
      var btnText = document.getElementById("Lesort2020aMetrics_and_Evaluations_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Lesort2020aMetrics_and_EvaluationsFunction3() {
      var moreText = document.getElementById("Lesort2020aMetrics_and_Evaluations_more3");
      var moreText1 = document.getElementById("Lesort2020aMetrics_and_Evaluations_more");
      var moreText2 = document.getElementById("Lesort2020aMetrics_and_Evaluations_more2");
      var btnText = document.getElementById("Lesort2020aMetrics_and_Evaluations_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="http://arxiv.org/abs/2003.05856">Online Fast Adaptation and Knowledge Accumulation: a New Approach to Continual Learning</a> by Massimo Caccia, Pau Rodriguez, Oleksiy Ostapenko, Fabrice Normandin, Min Lin, Lucas Caccia, Issam Laradji, Irina Rish, Alexande Lacoste, David Vazquez and Laurent Charlin. <em>arXiv</em>, 2020. <span style='background-color:LightGray; padding: 2px; border-radius:4px; border: 1px solid black;'>[fashion]</span> <span style='background-color:Tomato; padding: 2px; border-radius:4px; border: 1px solid black;'>[framework]</span> <span style='background-color:MediumAquaMarine; padding: 2px; border-radius:4px; border: 1px solid black;'>[mnist]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Caccia2020aMetrics_and_EvaluationsFunction()" id="Caccia2020aMetrics_and_Evaluations_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Caccia2020aMetrics_and_EvaluationsFunction2()" id="Caccia2020aMetrics_and_Evaluations_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Caccia2020aMetrics_and_EvaluationsFunction3()" id="Caccia2020aMetrics_and_Evaluations_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Caccia2020aMetrics_and_Evaluations_more" style="display: none">
    @article{Caccia2020a,<br> annote = {arXiv: 2003.05856},<br> author = {Caccia, Massimo and Rodriguez, Pau and Ostapenko, Oleksiy and Normandin, Fabrice and Lin, Min and Caccia, Lucas and Laradji, Issam and Rish, Irina and Lacoste, Alexande and Vazquez, David and Charlin, Laurent},<br> journal = {arXiv},<br> keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,MAML,OSAKA,[fashion],[framework],[mnist],continual meta learning,framework,meta continual learning},<br> mendeley-tags = {[fashion],[framework],[mnist]},<br> month = {mar},<br> shorttitle = {Online Fast Adaptation and Knowledge Accumulation},<br> title = {Online Fast Adaptation and Knowledge Accumulation: a New Approach to Continual Learning},<br> url = {http://arxiv.org/abs/2003.05856},<br> year = {2020}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Caccia2020aMetrics_and_Evaluations_more2" style="display: none">
    Learning from non-stationary data remains a great challenge for machine learning. Continual learning addresses this problem in scenarios where the learning agent faces a stream of changing tasks. In these scenarios, the agent is expected to retain its highest performance on previous tasks without revisiting them while adapting well to the new tasks. Two new recent continual-learning scenarios have been proposed. In meta-continual learning, the model is pre-trained to minimize catastrophic forgetting when trained on a sequence of tasks. In continual-meta learning, the goal is faster remembering, i.e., focusing on how quickly the agent recovers performance rather than measuring the agent's performance without any adaptation. Both scenarios have the potential to propel the field forward. Yet in their original formulations, they each have limitations. As a remedy, we propose a more general scenario where an agent must quickly solve (new) out-of-distribution tasks, while also requiring fast remembering. We show that current continual learning, meta learning, meta-continual learning, and continual-meta learning techniques fail in this new scenario. Accordingly, we propose a strong baseline: Continual-MAML, an online extension of the popular MAML algorithm. In our empirical experiments, we show that our method is better suited to the new scenario than the methodologies mentioned above, as well as standard continual learning and meta learning approaches.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Caccia2020aMetrics_and_Evaluations_more3" style="display: none">
    arXiv: 2003.05856
</span></p>
<script>
    function Caccia2020aMetrics_and_EvaluationsFunction() {
      var moreText = document.getElementById("Caccia2020aMetrics_and_Evaluations_more");
      var moreText2 = document.getElementById("Caccia2020aMetrics_and_Evaluations_more2");
      var moreText3 = document.getElementById("Caccia2020aMetrics_and_Evaluations_more3");
      var btnText = document.getElementById("Caccia2020aMetrics_and_Evaluations_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Caccia2020aMetrics_and_EvaluationsFunction2() {
      var moreText = document.getElementById("Caccia2020aMetrics_and_Evaluations_more2");
      var moreText1 = document.getElementById("Caccia2020aMetrics_and_Evaluations_more");
      var moreText3 = document.getElementById("Caccia2020aMetrics_and_Evaluations_more3");
      var btnText = document.getElementById("Caccia2020aMetrics_and_Evaluations_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Caccia2020aMetrics_and_EvaluationsFunction3() {
      var moreText = document.getElementById("Caccia2020aMetrics_and_Evaluations_more3");
      var moreText1 = document.getElementById("Caccia2020aMetrics_and_Evaluations_more");
      var moreText2 = document.getElementById("Caccia2020aMetrics_and_Evaluations_more2");
      var btnText = document.getElementById("Caccia2020aMetrics_and_Evaluations_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="http://arxiv.org/abs/1805.09733">Towards Robust Evaluations of Continual Learning</a> by Sebastian Farquhar and Yarin Gal. <em>Privacy in Machine Learning and Artificial Intelligence workshop, ICML</em>, 2019. <span style='background-color:LightGray; padding: 2px; border-radius:4px; border: 1px solid black;'>[fashion]</span> <span style='background-color:Tomato; padding: 2px; border-radius:4px; border: 1px solid black;'>[framework]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Farquhar2019aMetrics_and_EvaluationsFunction()" id="Farquhar2019aMetrics_and_Evaluations_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Farquhar2019aMetrics_and_EvaluationsFunction2()" id="Farquhar2019aMetrics_and_Evaluations_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Farquhar2019aMetrics_and_EvaluationsFunction3()" id="Farquhar2019aMetrics_and_Evaluations_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Farquhar2019aMetrics_and_Evaluations_more" style="display: none">
    @inproceedings{Farquhar2019a,<br> annote = {arXiv: 1805.09733},<br> author = {Farquhar, Sebastian and Gal, Yarin},<br> booktitle = {Privacy in Machine Learning and Artificial Intelligence workshop, ICML},<br> keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,[fashion],[framework],critique,evaluation,metrics},<br> mendeley-tags = {[fashion],[framework]},<br> month = {jun},<br> title = {Towards Robust Evaluations of Continual Learning},<br> url = {http://arxiv.org/abs/1805.09733},<br> year = {2019}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Farquhar2019aMetrics_and_Evaluations_more2" style="display: none">
    Experiments used in current continual learning research do not faithfully assess fundamental challenges of learning continually. Instead of assessing performance on challenging and representative experiment designs, recent research has focused on increased dataset difficulty, while still using flawed experiment set-ups. We examine standard evaluations and show why these evaluations make some continual learning approaches look better than they are. We introduce desiderata for continual learning evaluations and explain why their absence creates misleading comparisons. Based on our desiderata we then propose new experiment designs which we demonstrate with various continual learning approaches and datasets. Our analysis calls for a reprioritization of research effort by the community.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Farquhar2019aMetrics_and_Evaluations_more3" style="display: none">
    arXiv: 1805.09733
</span></p>
<script>
    function Farquhar2019aMetrics_and_EvaluationsFunction() {
      var moreText = document.getElementById("Farquhar2019aMetrics_and_Evaluations_more");
      var moreText2 = document.getElementById("Farquhar2019aMetrics_and_Evaluations_more2");
      var moreText3 = document.getElementById("Farquhar2019aMetrics_and_Evaluations_more3");
      var btnText = document.getElementById("Farquhar2019aMetrics_and_Evaluations_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Farquhar2019aMetrics_and_EvaluationsFunction2() {
      var moreText = document.getElementById("Farquhar2019aMetrics_and_Evaluations_more2");
      var moreText1 = document.getElementById("Farquhar2019aMetrics_and_Evaluations_more");
      var moreText3 = document.getElementById("Farquhar2019aMetrics_and_Evaluations_more3");
      var btnText = document.getElementById("Farquhar2019aMetrics_and_Evaluations_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Farquhar2019aMetrics_and_EvaluationsFunction3() {
      var moreText = document.getElementById("Farquhar2019aMetrics_and_Evaluations_more3");
      var moreText1 = document.getElementById("Farquhar2019aMetrics_and_Evaluations_more");
      var moreText2 = document.getElementById("Farquhar2019aMetrics_and_Evaluations_more2");
      var btnText = document.getElementById("Farquhar2019aMetrics_and_Evaluations_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="http://www.sciencedirect.com/science/article/pii/S0893608019300231">Continual lifelong learning with neural networks: A review</a> by German I Parisi, Ronald Kemker, Jose L Part, Christopher Kanan and Stefan Wermter. <em>Neural Networks</em>, 54–71, 2019. <span style='background-color:Tomato; padding: 2px; border-radius:4px; border: 1px solid black;'>[framework]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Parisi2019aMetrics_and_EvaluationsFunction()" id="Parisi2019aMetrics_and_Evaluations_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Parisi2019aMetrics_and_EvaluationsFunction2()" id="Parisi2019aMetrics_and_Evaluations_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Parisi2019aMetrics_and_EvaluationsFunction3()" id="Parisi2019aMetrics_and_Evaluations_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Parisi2019aMetrics_and_Evaluations_more" style="display: none">
    @article{Parisi2019a,<br> annote = {A general survey of the most important and used CL techniques, with a characterisation of the catastrophic forgetting phenomenon.},<br> author = {Parisi, German I and Kemker, Ronald and Part, Jose L and Kanan, Christopher and Wermter, Stefan},<br> doi = {10.1016/j.neunet.2019.01.012},<br> issn = {0893-6080},<br> journal = {Neural Networks},<br> keywords = {Catastrophic forgetting,Continual learning,Developmental systems,Lifelong learning,Memory consolidation,[framework]},<br> language = {en},<br> mendeley-tags = {[framework]},<br> month = {may},<br> pages = {54--71},<br> shorttitle = {Continual lifelong learning with neural networks},<br> title = {Continual lifelong learning with neural networks: A review},<br> url = {http://www.sciencedirect.com/science/article/pii/S0893608019300231},<br> volume = {113},<br> year = {2019}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Parisi2019aMetrics_and_Evaluations_more2" style="display: none">
    Humans and animals have the ability to continually acquire, fine-tune, and transfer knowledge and skills throughout their lifespan. This ability, referred to as lifelong learning, is mediated by a rich set of neurocognitive mechanisms that together contribute to the development and specialization of our sensorimotor skills as well as to long-term memory consolidation and retrieval. Consequently, lifelong learning capabilities are crucial for computational learning systems and autonomous agents interacting in the real world and processing continuous streams of information. However, lifelong learning remains a long-standing challenge for machine learning and neural network models since the continual acquisition of incrementally available information from non-stationary data distributions generally leads to catastrophic forgetting or interference. This limitation represents a major drawback for state-of-the-art deep neural network models that typically learn representations from stationary batches of training data, thus without accounting for situations in which information becomes incrementally available over time. In this review, we critically summarize the main challenges linked to lifelong learning for artificial learning systems and compare existing neural network approaches that alleviate, to different extents, catastrophic forgetting. Although significant advances have been made in domain-specific learning with neural networks, extensive research efforts are required for the development of robust lifelong learning on autonomous agents and robots. We discuss well-established and emerging research motivated by lifelong learning factors in biological systems such as structural plasticity, memory replay, curriculum and transfer learning, intrinsic motivation, and multisensory integration.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Parisi2019aMetrics_and_Evaluations_more3" style="display: none">
    A general survey of the most important and used CL techniques, with a characterisation of the catastrophic forgetting phenomenon.
</span></p>
<script>
    function Parisi2019aMetrics_and_EvaluationsFunction() {
      var moreText = document.getElementById("Parisi2019aMetrics_and_Evaluations_more");
      var moreText2 = document.getElementById("Parisi2019aMetrics_and_Evaluations_more2");
      var moreText3 = document.getElementById("Parisi2019aMetrics_and_Evaluations_more3");
      var btnText = document.getElementById("Parisi2019aMetrics_and_Evaluations_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Parisi2019aMetrics_and_EvaluationsFunction2() {
      var moreText = document.getElementById("Parisi2019aMetrics_and_Evaluations_more2");
      var moreText1 = document.getElementById("Parisi2019aMetrics_and_Evaluations_more");
      var moreText3 = document.getElementById("Parisi2019aMetrics_and_Evaluations_more3");
      var btnText = document.getElementById("Parisi2019aMetrics_and_Evaluations_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Parisi2019aMetrics_and_EvaluationsFunction3() {
      var moreText = document.getElementById("Parisi2019aMetrics_and_Evaluations_more3");
      var moreText1 = document.getElementById("Parisi2019aMetrics_and_Evaluations_more");
      var moreText2 = document.getElementById("Parisi2019aMetrics_and_Evaluations_more2");
      var btnText = document.getElementById("Parisi2019aMetrics_and_Evaluations_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p>Strategies for improving single-head continual learning performance by Alaa El Khatib and Fakhri Karray. <em>Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</em>, 452–460, 2019. <span style='background-color:DarkCyan; padding: 2px; border-radius:4px; border: 1px solid black;'>[cifar]</span> <span style='background-color:MediumAquaMarine; padding: 2px; border-radius:4px; border: 1px solid black;'>[mnist]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="khatib2019aMetrics_and_EvaluationsFunction()" id="khatib2019aMetrics_and_Evaluations_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="khatib2019aMetrics_and_EvaluationsFunction2()" id="khatib2019aMetrics_and_Evaluations_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="khatib2019aMetrics_and_EvaluationsFunction3()" id="khatib2019aMetrics_and_Evaluations_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="khatib2019aMetrics_and_Evaluations_more" style="display: none">
    @inproceedings{khatib2019a,<br> author = {El Khatib, Alaa and Karray, Fakhri},<br> booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},<br> doi = {10.1007/978-3-030-27202-9_41},<br> file = {::},<br> isbn = {9783030272012},<br> issn = {16113349},<br> keywords = {Catastrophic forgetting,Continual learning,[cifar],[mnist]},<br> mendeley-tags = {[cifar],[mnist]},<br> month = {aug},<br> pages = {452--460},<br> publisher = {Springer Verlag},<br> title = {Strategies for improving single-head continual learning performance},<br> volume = {11662 LNCS},<br> year = {2019}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="khatib2019aMetrics_and_Evaluations_more2" style="display: none">
    Catastrophic forgetting has long been seen as the main obstacle to building continual learning models. We argue in this paper that an equally challenging characteristic of the continual learning framework is that data are never completely available at the same time, making it difficult to learn joint conditional distributions over them. This is most evident in the usually large gap between single-head and multi-head performance of continual learning models. We propose in this paper two strategies to improve performance of continual learning models, particularly in the single-head framework and for image classification tasks. First, we argue that learning multiple binary classifiers, rather than a single multi-class classifier, for each presentation of data is more consistent with the single-head framework. Moreover, we argue that auxiliary, unlabelled data can be used in tandem with this approach to slow the decay in performance of these binary classifiers over time.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="khatib2019aMetrics_and_Evaluations_more3" style="display: none">
    N.A.
</span></p>
<script>
    function khatib2019aMetrics_and_EvaluationsFunction() {
      var moreText = document.getElementById("khatib2019aMetrics_and_Evaluations_more");
      var moreText2 = document.getElementById("khatib2019aMetrics_and_Evaluations_more2");
      var moreText3 = document.getElementById("khatib2019aMetrics_and_Evaluations_more3");
      var btnText = document.getElementById("khatib2019aMetrics_and_Evaluations_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function khatib2019aMetrics_and_EvaluationsFunction2() {
      var moreText = document.getElementById("khatib2019aMetrics_and_Evaluations_more2");
      var moreText1 = document.getElementById("khatib2019aMetrics_and_Evaluations_more");
      var moreText3 = document.getElementById("khatib2019aMetrics_and_Evaluations_more3");
      var btnText = document.getElementById("khatib2019aMetrics_and_Evaluations_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function khatib2019aMetrics_and_EvaluationsFunction3() {
      var moreText = document.getElementById("khatib2019aMetrics_and_Evaluations_more3");
      var moreText1 = document.getElementById("khatib2019aMetrics_and_Evaluations_more");
      var moreText2 = document.getElementById("khatib2019aMetrics_and_Evaluations_more2");
      var btnText = document.getElementById("khatib2019aMetrics_and_Evaluations_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="http://arxiv.org/abs/1904.07734">Three scenarios for continual learning</a> by Gido M van de Ven and Andreas S Tolias. <em>Continual Learning Workshop NeurIPS</em>, 2018. <span style='background-color:Tomato; padding: 2px; border-radius:4px; border: 1px solid black;'>[framework]</span> <span style='background-color:MediumAquaMarine; padding: 2px; border-radius:4px; border: 1px solid black;'>[mnist]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="VanDeVen2018aMetrics_and_EvaluationsFunction()" id="VanDeVen2018aMetrics_and_Evaluations_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="VanDeVen2018aMetrics_and_EvaluationsFunction2()" id="VanDeVen2018aMetrics_and_Evaluations_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="VanDeVen2018aMetrics_and_EvaluationsFunction3()" id="VanDeVen2018aMetrics_and_Evaluations_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="VanDeVen2018aMetrics_and_Evaluations_more" style="display: none">
    @inproceedings{VanDeVen2018a,<br> annote = {Comment: Extended version of work presented at the NeurIPS Continual Learning workshop (2018); 18 pages, 5 figures, 6 tables. Related to arXiv:1809.10635<br>arXiv: 1904.07734},<br> author = {van de Ven, Gido M and Tolias, Andreas S},<br> booktitle = {Continual Learning Workshop NeurIPS},<br> keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Rec,Computer Science - Machine Learning,Statistics - Machine Learning,[framework],[mnist]},<br> mendeley-tags = {[framework],[mnist]},<br> title = {Three scenarios for continual learning},<br> url = {http://arxiv.org/abs/1904.07734},<br> year = {2018}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="VanDeVen2018aMetrics_and_Evaluations_more2" style="display: none">
    Standard artificial neural networks suffer from the well-known issue of catastrophic forgetting, making continual or lifelong learning difficult for machine learning. In recent years, numerous methods have been proposed for continual learning, but due to differences in evaluation protocols it is difficult to directly compare their performance. To enable more structured comparisons, we describe three continual learning scenarios based on whether at test time task identity is provided and–in case it is not–whether it must be inferred. Any sequence of well-defined tasks can be performed according to each scenario. Using the split and permuted MNIST task protocols, for each scenario we carry out an extensive comparison of recently proposed continual learning methods. We demonstrate substantial differences between the three scenarios in terms of difficulty and in terms of how efficient different methods are. In particular, when task identity must be inferred (i.e., class incremental learning), we find that regularization-based approaches (e.g., elastic weight consolidation) fail and that replaying representations of previous experiences seems required for solving this scenario.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="VanDeVen2018aMetrics_and_Evaluations_more3" style="display: none">
    Comment: Extended version of work presented at the NeurIPS Continual Learning workshop (2018); 18 pages, 5 figures, 6 tables. Related to arXiv:1809.10635
            arXiv: 1904.07734
</span></p>
<script>
    function VanDeVen2018aMetrics_and_EvaluationsFunction() {
      var moreText = document.getElementById("VanDeVen2018aMetrics_and_Evaluations_more");
      var moreText2 = document.getElementById("VanDeVen2018aMetrics_and_Evaluations_more2");
      var moreText3 = document.getElementById("VanDeVen2018aMetrics_and_Evaluations_more3");
      var btnText = document.getElementById("VanDeVen2018aMetrics_and_Evaluations_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function VanDeVen2018aMetrics_and_EvaluationsFunction2() {
      var moreText = document.getElementById("VanDeVen2018aMetrics_and_Evaluations_more2");
      var moreText1 = document.getElementById("VanDeVen2018aMetrics_and_Evaluations_more");
      var moreText3 = document.getElementById("VanDeVen2018aMetrics_and_Evaluations_more3");
      var btnText = document.getElementById("VanDeVen2018aMetrics_and_Evaluations_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function VanDeVen2018aMetrics_and_EvaluationsFunction3() {
      var moreText = document.getElementById("VanDeVen2018aMetrics_and_Evaluations_more3");
      var moreText1 = document.getElementById("VanDeVen2018aMetrics_and_Evaluations_more");
      var moreText2 = document.getElementById("VanDeVen2018aMetrics_and_Evaluations_more2");
      var btnText = document.getElementById("VanDeVen2018aMetrics_and_Evaluations_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
</ul>
</div>
<div class="section" id="bioinspired-methods">
<h3>Bioinspired Methods<a class="headerlink" href="#bioinspired-methods" title="Permalink to this headline">¶</a></h3>
<p>In this section we list all the papers related to bio-inspired continual learning approaches.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://baicsworkshop.github.io/pdf/BAICS_8.pdf">Brain-like Replay for Continual Learning with Artificial Neural Networks</a> by Gido M. van de Ven, Hava T. Siegelmann and Andreas S. Tolias. <em>International Conference on Learning Representations</em>, 2020. <span style='background-color:DarkCyan; padding: 2px; border-radius:4px; border: 1px solid black;'>[cifar]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="ven2020aBioinspired_MethodsFunction()" id="ven2020aBioinspired_Methods_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="ven2020aBioinspired_MethodsFunction2()" id="ven2020aBioinspired_Methods_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="ven2020aBioinspired_MethodsFunction3()" id="ven2020aBioinspired_Methods_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="ven2020aBioinspired_Methods_more" style="display: none">
    @inproceedings{ven2020a,<br> author = {van de Ven, Gido M. and Siegelmann, Hava T. and Tolias, Andreas S.},<br> booktitle = {International Conference on Learning Representations},<br> keywords = {[cifar]},<br> mendeley-tags = {[cifar]},<br> title = {Brain-like Replay for Continual Learning with Artificial Neural Networks},<br> url = {https://baicsworkshop.github.io/pdf/BAICS_8.pdf},<br> year = {2020}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="ven2020aBioinspired_Methods_more2" style="display: none">
    Artificial neural networks suffer from catastrophic forgetting. Unlike humans, when these networks are trained on something new, they rapidly forget what was learned before. In the brain, a mechanism thought to be important for protecting memories is the replay of neuronal activity patterns representing those memories. In artificial neural networks, such memory replay has been implemented in the form of ‘generative replay', which can successfully prevent catastrophic forgetting in a range of toy examples. Scaling up generative replay to problems with more complex inputs, however, turns out to be challenging. We propose a new, more brain-like variant of replay in which internal or hidden representations are replayed that are generated by the network's own, context-modulated feedback connections. In contrast to established continual learning methods, our method achieves acceptable performance on the challenging problem of class-incremental learning on natural images without relying on stored data.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="ven2020aBioinspired_Methods_more3" style="display: none">
    N.A.
</span></p>
<script>
    function ven2020aBioinspired_MethodsFunction() {
      var moreText = document.getElementById("ven2020aBioinspired_Methods_more");
      var moreText2 = document.getElementById("ven2020aBioinspired_Methods_more2");
      var moreText3 = document.getElementById("ven2020aBioinspired_Methods_more3");
      var btnText = document.getElementById("ven2020aBioinspired_Methods_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function ven2020aBioinspired_MethodsFunction2() {
      var moreText = document.getElementById("ven2020aBioinspired_Methods_more2");
      var moreText1 = document.getElementById("ven2020aBioinspired_Methods_more");
      var moreText3 = document.getElementById("ven2020aBioinspired_Methods_more3");
      var btnText = document.getElementById("ven2020aBioinspired_Methods_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function ven2020aBioinspired_MethodsFunction3() {
      var moreText = document.getElementById("ven2020aBioinspired_Methods_more3");
      var moreText1 = document.getElementById("ven2020aBioinspired_Methods_more");
      var moreText2 = document.getElementById("ven2020aBioinspired_Methods_more2");
      var btnText = document.getElementById("ven2020aBioinspired_Methods_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="https://www.frontiersin.org/article/10.3389/fnins.2020.00007/full">Controlled Forgetting: Targeted Stimulation and Dopaminergic Plasticity Modulation for Unsupervised Lifelong Learning in Spiking Neural Networks</a> by Jason M. Allred and Kaushik Roy. <em>Frontiers in Neuroscience</em>, 7, 2020. <span style='background-color:yellow; padding: 2px; border-radius:4px; border: 1px solid black;'>[spiking]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Allred2020aBioinspired_MethodsFunction()" id="Allred2020aBioinspired_Methods_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Allred2020aBioinspired_MethodsFunction2()" id="Allred2020aBioinspired_Methods_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Allred2020aBioinspired_MethodsFunction3()" id="Allred2020aBioinspired_Methods_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Allred2020aBioinspired_Methods_more" style="display: none">
    @article{Allred2020a,<br> author = {Allred, Jason M. and Roy, Kaushik},<br> doi = {10.3389/fnins.2020.00007},<br> file = {::},<br> issn = {1662-453X},<br> journal = {Frontiers in Neuroscience},<br> keywords = {Spike Timing Dependent Plasticity,Spiking Neural Networks,[spiking],catastrophic forgetting,continual learning,controlled forgetting,dopaminergic learning,lifelong learning,stability-plasticity dilemma},<br> mendeley-tags = {[spiking]},<br> month = {jan},<br> pages = {7},<br> publisher = {Frontiers Media S.A.},<br> title = {Controlled Forgetting: Targeted Stimulation and Dopaminergic Plasticity Modulation for Unsupervised Lifelong Learning in Spiking Neural Networks},<br> url = {https://www.frontiersin.org/article/10.3389/fnins.2020.00007/full},<br> volume = {14},<br> year = {2020}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Allred2020aBioinspired_Methods_more2" style="display: none">
    Stochastic gradient descent requires that training samples be drawn from a uniformly random distribution of the data. For a deployed system that must learn online from an uncontrolled and unknown environment, the ordering of input samples often fails to meet this criterion, making lifelong learning a difficult challenge. We exploit the locality of the unsupervised Spike Timing Dependent Plasticity (STDP) learning rule to target local representations in a Spiking Neural Network (SNN) to adapt to novel information while protecting essential information in the remainder of the SNN from catastrophic forgetting. In our Controlled Forgetting Networks (CFNs), novel information triggers stimulated firing and heterogeneously modulated plasticity, inspired by biological dopamine signals, to cause rapid and isolated adaptation in the synapses of neurons associated with outlier information. This targeting controls the forgetting process in a way that reduces the degradation of accuracy for older tasks while learning new tasks. Our experimental results on the MNIST dataset validate the capability of CFNs to learn successfully over time from an unknown, changing environment, achieving 95.24% accuracy, which we believe is the best unsupervised accuracy ever achieved by a fixed-size, single-layer SNN on a completely disjoint MNIST dataset.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Allred2020aBioinspired_Methods_more3" style="display: none">
    N.A.
</span></p>
<script>
    function Allred2020aBioinspired_MethodsFunction() {
      var moreText = document.getElementById("Allred2020aBioinspired_Methods_more");
      var moreText2 = document.getElementById("Allred2020aBioinspired_Methods_more2");
      var moreText3 = document.getElementById("Allred2020aBioinspired_Methods_more3");
      var btnText = document.getElementById("Allred2020aBioinspired_Methods_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Allred2020aBioinspired_MethodsFunction2() {
      var moreText = document.getElementById("Allred2020aBioinspired_Methods_more2");
      var moreText1 = document.getElementById("Allred2020aBioinspired_Methods_more");
      var moreText3 = document.getElementById("Allred2020aBioinspired_Methods_more3");
      var btnText = document.getElementById("Allred2020aBioinspired_Methods_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Allred2020aBioinspired_MethodsFunction3() {
      var moreText = document.getElementById("Allred2020aBioinspired_Methods_more3");
      var moreText1 = document.getElementById("Allred2020aBioinspired_Methods_more");
      var moreText2 = document.getElementById("Allred2020aBioinspired_Methods_more2");
      var btnText = document.getElementById("Allred2020aBioinspired_Methods_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="http://arxiv.org/abs/1908.08655">Spiking Neural Predictive Coding for Continual Learning from Data Streams</a> by  and Alexander Ororbia. <em>arXiv</em>, 2020. <span style='background-color:yellow; padding: 2px; border-radius:4px; border: 1px solid black;'>[spiking]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Ororbia2020aBioinspired_MethodsFunction()" id="Ororbia2020aBioinspired_Methods_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Ororbia2020aBioinspired_MethodsFunction2()" id="Ororbia2020aBioinspired_Methods_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Ororbia2020aBioinspired_MethodsFunction3()" id="Ororbia2020aBioinspired_Methods_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Ororbia2020aBioinspired_Methods_more" style="display: none">
    @article{Ororbia2020a,<br> annote = {Comment: Revised version of manuscript – includes updated experimental results<br>arXiv: 1908.08655},<br> author = {Ororbia, Alexander},<br> journal = {arXiv},<br> keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computi,Quantitative Biology - Neurons and Cognition,[spiking]},<br> mendeley-tags = {[spiking]},<br> month = {jan},<br> title = {Spiking Neural Predictive Coding for Continual Learning from Data Streams},<br> url = {http://arxiv.org/abs/1908.08655},<br> year = {2020}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Ororbia2020aBioinspired_Methods_more2" style="display: none">
    For energy-efficient computation in specialized neuromorphic hardware, we present the Spiking Neural Coding Network, an instantiation of a family of artificial neural models strongly motivated by the theory of predictive coding. The model, in essence, works by operating in a never-ending process of "guess-and-check", where neurons predict the activity values of one another and then immediately adjust their own activities to make better future predictions. The interactive, iterative nature of our neural system fits well into the continuous time formulation of data sensory stream prediction and, as we show, the model's structure yields a simple, local synaptic update rule, which could be used to complement or replace online spike-timing dependent plasticity. In this article, we experiment with an instantiation of our model that consists of leaky integrate-and-fire units. However, the general framework within which our model is situated can naturally incorporate more complex, formal neurons such as the Hodgkin-Huxley model. Our experimental results in pattern recognition demonstrate the potential of the proposed model when binary spike trains are the primary paradigm for inter-neuron communication. Notably, our model is competitive in terms of classification performance, can conduct online semi-supervised learning, naturally experiences less forgetting when learning from a sequence of tasks, and is more computationally economical and biologically-plausible than popular artificial neural networks.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Ororbia2020aBioinspired_Methods_more3" style="display: none">
    Comment: Revised version of manuscript – includes updated experimental results
            arXiv: 1908.08655
</span></p>
<script>
    function Ororbia2020aBioinspired_MethodsFunction() {
      var moreText = document.getElementById("Ororbia2020aBioinspired_Methods_more");
      var moreText2 = document.getElementById("Ororbia2020aBioinspired_Methods_more2");
      var moreText3 = document.getElementById("Ororbia2020aBioinspired_Methods_more3");
      var btnText = document.getElementById("Ororbia2020aBioinspired_Methods_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Ororbia2020aBioinspired_MethodsFunction2() {
      var moreText = document.getElementById("Ororbia2020aBioinspired_Methods_more2");
      var moreText1 = document.getElementById("Ororbia2020aBioinspired_Methods_more");
      var moreText3 = document.getElementById("Ororbia2020aBioinspired_Methods_more3");
      var btnText = document.getElementById("Ororbia2020aBioinspired_Methods_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Ororbia2020aBioinspired_MethodsFunction3() {
      var moreText = document.getElementById("Ororbia2020aBioinspired_Methods_more3");
      var moreText1 = document.getElementById("Ororbia2020aBioinspired_Methods_more");
      var moreText2 = document.getElementById("Ororbia2020aBioinspired_Methods_more2");
      var btnText = document.getElementById("Ororbia2020aBioinspired_Methods_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="https://openreview.net/forum?id=Bkxbrn0cYX">Selfless Sequential Learning</a> by Rahaf Aljundi, Marcus Rohrbach and Tinne Tuytelaars. <em>ICLR</em>, 2019. <span style='background-color:DarkCyan; padding: 2px; border-radius:4px; border: 1px solid black;'>[cifar]</span> <span style='background-color:MediumAquaMarine; padding: 2px; border-radius:4px; border: 1px solid black;'>[mnist]</span> <span style='background-color:DarkSalmon; padding: 2px; border-radius:4px; border: 1px solid black;'>[sparsity]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Aljundi2019cBioinspired_MethodsFunction()" id="Aljundi2019cBioinspired_Methods_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Aljundi2019cBioinspired_MethodsFunction2()" id="Aljundi2019cBioinspired_Methods_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Aljundi2019cBioinspired_MethodsFunction3()" id="Aljundi2019cBioinspired_Methods_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Aljundi2019cBioinspired_Methods_more" style="display: none">
    @inproceedings{Aljundi2019c,<br> annote = {The authors combine multiple penalizations to (1) induce sparse activations through lateral inhibitions between neurons and to (2) penalize changes in most important weights in order to prevent forgetting.},<br> author = {Aljundi, Rahaf and Rohrbach, Marcus and Tuytelaars, Tinne},<br> booktitle = {ICLR},<br> keywords = {[cifar],[mnist],[sparsity]},<br> mendeley-tags = {[cifar],[mnist],[sparsity]},<br> title = {Selfless Sequential Learning},<br> url = {https://openreview.net/forum?id=Bkxbrn0cYX},<br> year = {2019}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Aljundi2019cBioinspired_Methods_more2" style="display: none">
    Sequential learning, also called lifelong learning, studies the problem of learning tasks in a sequence with access restricted to only the data of the current task. In this paper we look at a...
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Aljundi2019cBioinspired_Methods_more3" style="display: none">
    The authors combine multiple penalizations to (1) induce sparse activations through lateral inhibitions between neurons and to (2) penalize changes in most important weights in order to prevent forgetting.
</span></p>
<script>
    function Aljundi2019cBioinspired_MethodsFunction() {
      var moreText = document.getElementById("Aljundi2019cBioinspired_Methods_more");
      var moreText2 = document.getElementById("Aljundi2019cBioinspired_Methods_more2");
      var moreText3 = document.getElementById("Aljundi2019cBioinspired_Methods_more3");
      var btnText = document.getElementById("Aljundi2019cBioinspired_Methods_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Aljundi2019cBioinspired_MethodsFunction2() {
      var moreText = document.getElementById("Aljundi2019cBioinspired_Methods_more2");
      var moreText1 = document.getElementById("Aljundi2019cBioinspired_Methods_more");
      var moreText3 = document.getElementById("Aljundi2019cBioinspired_Methods_more3");
      var btnText = document.getElementById("Aljundi2019cBioinspired_Methods_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Aljundi2019cBioinspired_MethodsFunction3() {
      var moreText = document.getElementById("Aljundi2019cBioinspired_Methods_more3");
      var moreText1 = document.getElementById("Aljundi2019cBioinspired_Methods_more");
      var moreText2 = document.getElementById("Aljundi2019cBioinspired_Methods_more2");
      var btnText = document.getElementById("Aljundi2019cBioinspired_Methods_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="http://arxiv.org/abs/1810.07411">Continual Learning of Recurrent Neural Networks by Locally Aligning Distributed Representations</a> by Alexander Ororbia, Ankur Mali, C Lee Giles and Daniel Kifer. <em>arXiv</em>, 2019. <span style='background-color:MediumAquaMarine; padding: 2px; border-radius:4px; border: 1px solid black;'>[mnist]</span> <span style='background-color:yellow; padding: 2px; border-radius:4px; border: 1px solid black;'>[spiking]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Ororbia2019aBioinspired_MethodsFunction()" id="Ororbia2019aBioinspired_Methods_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Ororbia2019aBioinspired_MethodsFunction2()" id="Ororbia2019aBioinspired_Methods_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Ororbia2019aBioinspired_MethodsFunction3()" id="Ororbia2019aBioinspired_Methods_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Ororbia2019aBioinspired_Methods_more" style="display: none">
    @article{Ororbia2019a,<br> annote = {Comment: Important revisions made throughout (additional items/results added, including a complexity analysis)<br>arXiv: 1810.07411},<br> author = {Ororbia, Alexander and Mali, Ankur and Giles, C Lee and Kifer, Daniel},<br> journal = {arXiv},<br> keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computi,[mnist],[spiking],credi assignment},<br> mendeley-tags = {[mnist],[spiking]},<br> month = {aug},<br> title = {Continual Learning of Recurrent Neural Networks by Locally Aligning Distributed Representations},<br> url = {http://arxiv.org/abs/1810.07411},<br> year = {2019}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Ororbia2019aBioinspired_Methods_more2" style="display: none">
    Temporal models based on recurrent neural networks have proven to be quite powerful in a wide variety of applications. However, training these models often relies on back-propagation through time, which entails unfolding the network over many time steps, making the process of conducting credit assignment considerably more challenging. Furthermore, the nature of back-propagation itself does not permit the use of non-differentiable activation functions and is inherently sequential, making parallelization of the underlying training process difficult. Here, we propose the Parallel Temporal Neural Coding Network (P-TNCN), a biologically inspired model trained by the learning algorithm we call Local Representation Alignment. It aims to resolve the difficulties and problems that plague recurrent networks trained by back-propagation through time. The architecture requires neither unrolling in time nor the derivatives of its internal activation functions. We compare our model and learning procedure to other back-propagation through time alternatives (which also tend to be computationally expensive), including real-time recurrent learning, echo state networks, and unbiased online recurrent optimization. We show that it outperforms these on sequence modeling benchmarks such as Bouncing MNIST, a new benchmark we denote as Bouncing NotMNIST, and Penn Treebank. Notably, our approach can in some instances outperform full back-propagation through time as well as variants such as sparse attentive back-tracking. Significantly, the hidden unit correction phase of P-TNCN allows it to adapt to new datasets even if its synaptic weights are held fixed (zero-shot adaptation) and facilitates retention of prior generative knowledge when faced with a task sequence. We present results that show the P-TNCN's ability to conduct zero-shot adaptation and online continual sequence modeling.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Ororbia2019aBioinspired_Methods_more3" style="display: none">
    Comment: Important revisions made throughout (additional items/results added, including a complexity analysis)
            arXiv: 1810.07411
</span></p>
<script>
    function Ororbia2019aBioinspired_MethodsFunction() {
      var moreText = document.getElementById("Ororbia2019aBioinspired_Methods_more");
      var moreText2 = document.getElementById("Ororbia2019aBioinspired_Methods_more2");
      var moreText3 = document.getElementById("Ororbia2019aBioinspired_Methods_more3");
      var btnText = document.getElementById("Ororbia2019aBioinspired_Methods_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Ororbia2019aBioinspired_MethodsFunction2() {
      var moreText = document.getElementById("Ororbia2019aBioinspired_Methods_more2");
      var moreText1 = document.getElementById("Ororbia2019aBioinspired_Methods_more");
      var moreText3 = document.getElementById("Ororbia2019aBioinspired_Methods_more3");
      var btnText = document.getElementById("Ororbia2019aBioinspired_Methods_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Ororbia2019aBioinspired_MethodsFunction3() {
      var moreText = document.getElementById("Ororbia2019aBioinspired_Methods_more3");
      var moreText1 = document.getElementById("Ororbia2019aBioinspired_Methods_more");
      var moreText2 = document.getElementById("Ororbia2019aBioinspired_Methods_more2");
      var btnText = document.getElementById("Ororbia2019aBioinspired_Methods_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="http://arxiv.org/abs/1905.10696">Lifelong Neural Predictive Coding: Sparsity Yields Less Forgetting when Learning Cumulatively</a> by Alexander Ororbia, Ankur Mali, Daniel Kifer and C Lee Giles. <em>arXiv</em>, 1–11, 2019. <span style='background-color:LightGray; padding: 2px; border-radius:4px; border: 1px solid black;'>[fashion]</span> <span style='background-color:MediumAquaMarine; padding: 2px; border-radius:4px; border: 1px solid black;'>[mnist]</span> <span style='background-color:DarkSalmon; padding: 2px; border-radius:4px; border: 1px solid black;'>[sparsity]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Ororbia2019bBioinspired_MethodsFunction()" id="Ororbia2019bBioinspired_Methods_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Ororbia2019bBioinspired_MethodsFunction2()" id="Ororbia2019bBioinspired_Methods_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Ororbia2019bBioinspired_MethodsFunction3()" id="Ororbia2019bBioinspired_Methods_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Ororbia2019bBioinspired_Methods_more" style="display: none">
    @article{Ororbia2019b,<br> archiveprefix = {arXiv},<br> arxivid = {1905.10696},<br> author = {Ororbia, Alexander and Mali, Ankur and Kifer, Daniel and Giles, C Lee},<br> eprint = {1905.10696},<br> journal = {arXiv},<br> keywords = {[fashion],[mnist],[sparsity]},<br> mendeley-tags = {[fashion],[mnist],[sparsity]},<br> pages = {1--11},<br> title = {Lifelong Neural Predictive Coding: Sparsity Yields Less Forgetting when Learning Cumulatively},<br> url = {http://arxiv.org/abs/1905.10696},<br> year = {2019}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Ororbia2019bBioinspired_Methods_more2" style="display: none">
    In lifelong learning systems, especially those based on artificial neural networks, one of the biggest obstacles is the severe inability to retain old knowledge as new information is encountered. This phenomenon is known as catastrophic forgetting. In this paper, we present a new connectionist model, the Sequential Neural Coding Network, and its learning procedure, grounded in the neurocognitive theory of predictive coding. The architecture experiences significantly less forgetting as compared to standard neural models and outperforms a variety of previously proposed remedies and methods when trained across multiple task datasets in a stream-like fashion. The promising performance demonstrated in our experiments offers motivation that directly incorporating mechanisms prominent in real neuronal systems, such as competition, sparse activation patterns, and iterative input processing, can create viable pathways for tackling the challenge of lifelong machine learning.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Ororbia2019bBioinspired_Methods_more3" style="display: none">
    N.A.
</span></p>
<script>
    function Ororbia2019bBioinspired_MethodsFunction() {
      var moreText = document.getElementById("Ororbia2019bBioinspired_Methods_more");
      var moreText2 = document.getElementById("Ororbia2019bBioinspired_Methods_more2");
      var moreText3 = document.getElementById("Ororbia2019bBioinspired_Methods_more3");
      var btnText = document.getElementById("Ororbia2019bBioinspired_Methods_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Ororbia2019bBioinspired_MethodsFunction2() {
      var moreText = document.getElementById("Ororbia2019bBioinspired_Methods_more2");
      var moreText1 = document.getElementById("Ororbia2019bBioinspired_Methods_more");
      var moreText3 = document.getElementById("Ororbia2019bBioinspired_Methods_more3");
      var btnText = document.getElementById("Ororbia2019bBioinspired_Methods_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Ororbia2019bBioinspired_MethodsFunction3() {
      var moreText = document.getElementById("Ororbia2019bBioinspired_Methods_more3");
      var moreText1 = document.getElementById("Ororbia2019bBioinspired_Methods_more");
      var moreText2 = document.getElementById("Ororbia2019bBioinspired_Methods_more2");
      var btnText = document.getElementById("Ororbia2019bBioinspired_Methods_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="https://www.frontiersin.org/articles/10.3389/fnbot.2018.00078/full">Lifelong Learning of Spatiotemporal Representations With Dual-Memory Recurrent Self-Organization</a> by German I Parisi, Jun Tani, Cornelius Weber and Stefan Wermter. <em>Frontiers in Neurorobotics</em>, 2018. <span style='background-color:Chartreuse; padding: 2px; border-radius:4px; border: 1px solid black;'>[core50]</span> <span style='background-color:green; padding: 2px; border-radius:4px; border: 1px solid black;'>[dual]</span> <span style='background-color:Cornsilk; padding: 2px; border-radius:4px; border: 1px solid black;'>[som]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Parisi2018aBioinspired_MethodsFunction()" id="Parisi2018aBioinspired_Methods_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Parisi2018aBioinspired_MethodsFunction2()" id="Parisi2018aBioinspired_Methods_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Parisi2018aBioinspired_MethodsFunction3()" id="Parisi2018aBioinspired_Methods_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Parisi2018aBioinspired_Methods_more" style="display: none">
    @article{Parisi2018a,<br> author = {Parisi, German I and Tani, Jun and Weber, Cornelius and Wermter, Stefan},<br> doi = {10.3389/fnbot.2018.00078},<br> issn = {1662-5218},<br> journal = {Frontiers in Neurorobotics},<br> keywords = {CLS,Incremental Learning,Lifelong learning,Memory,Self-organizing Network,[core50],[dual],[som],object recognition systems},<br> language = {English},<br> mendeley-tags = {[core50],[dual],[som]},<br> title = {Lifelong Learning of Spatiotemporal Representations With Dual-Memory Recurrent Self-Organization},<br> url = {https://www.frontiersin.org/articles/10.3389/fnbot.2018.00078/full},<br> volume = {12},<br> year = {2018}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Parisi2018aBioinspired_Methods_more2" style="display: none">
    Artificial autonomous agents and robots interacting in complex environments are required to continually acquire and fine-tune knowledge over sustained periods of time. The ability to learn from continuous streams of information is referred to as lifelong learning and represents a long-standing challenge for neural network models due to catastrophic forgetting in which novel sensory experience interferes with existing representations and leads to abrupt decreases in the performance on previously acquired knowledge. Computational models of lifelong learning typically alleviate catastrophic forgetting in experimental scenarios with given datasets of static images and limited complexity, thereby differing significantly from the conditions artificial agents are exposed to. In more natural settings, sequential information may become progressively available over time and access to previous experience may be restricted. Therefore, specialized neural network mechanisms are required that adapt to novel sequential experience while preventing disruptive interference with existing representations. In this paper, we propose a dual-memory self-organizing architecture for lifelong learning scenarios. The architecture comprises two growing recurrent networks with the complementary tasks of learning object instances (episodic memory) and categories (semantic memory). Both growing networks can expand in response to novel sensory experience: the episodic memory learns fine-grained spatiotemporal representations of object instances in an unsupervised fashion while the semantic memory uses task-relevant signals to regulate structural plasticity levels and develop more compact representations from episodic experience. For the consolidation of knowledge in the absence of external sensory input, the episodic memory periodically replays trajectories of neural reactivations. We evaluate the proposed model on the CORe50 benchmark dataset for continuous object recognition, showing that we significantly outperform current methods of lifelong learning in three different incremental learning scenarios.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Parisi2018aBioinspired_Methods_more3" style="display: none">
    N.A.
</span></p>
<script>
    function Parisi2018aBioinspired_MethodsFunction() {
      var moreText = document.getElementById("Parisi2018aBioinspired_Methods_more");
      var moreText2 = document.getElementById("Parisi2018aBioinspired_Methods_more2");
      var moreText3 = document.getElementById("Parisi2018aBioinspired_Methods_more3");
      var btnText = document.getElementById("Parisi2018aBioinspired_Methods_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Parisi2018aBioinspired_MethodsFunction2() {
      var moreText = document.getElementById("Parisi2018aBioinspired_Methods_more2");
      var moreText1 = document.getElementById("Parisi2018aBioinspired_Methods_more");
      var moreText3 = document.getElementById("Parisi2018aBioinspired_Methods_more3");
      var btnText = document.getElementById("Parisi2018aBioinspired_Methods_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Parisi2018aBioinspired_MethodsFunction3() {
      var moreText = document.getElementById("Parisi2018aBioinspired_Methods_more3");
      var moreText1 = document.getElementById("Parisi2018aBioinspired_Methods_more");
      var moreText2 = document.getElementById("Parisi2018aBioinspired_Methods_more2");
      var btnText = document.getElementById("Parisi2018aBioinspired_Methods_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="https://openreview.net/pdf?id=SJ1Xmf-Rb">FearNet: Brain-Inspired Model for Incremental Learning</a> by Ronald Kemker and Christopher Kanan. <em>ICLR</em>, 2018. <span style='background-color:#50EBEC; padding: 2px; border-radius:4px; border: 1px solid black;'>[audio]</span> <span style='background-color:DarkCyan; padding: 2px; border-radius:4px; border: 1px solid black;'>[cifar]</span> <span style='background-color:Maroon; padding: 2px; border-radius:4px; border: 1px solid black;'>[generative]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="kemker2018bBioinspired_MethodsFunction()" id="kemker2018bBioinspired_Methods_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="kemker2018bBioinspired_MethodsFunction2()" id="kemker2018bBioinspired_Methods_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="kemker2018bBioinspired_MethodsFunction3()" id="kemker2018bBioinspired_Methods_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="kemker2018bBioinspired_Methods_more" style="display: none">
    @inproceedings{kemker2018b,<br> author = {Kemker, Ronald and Kanan, Christopher},<br> booktitle = {ICLR},<br> file = {::},<br> keywords = {[audio],[cifar],[generative]},<br> mendeley-tags = {[audio],[cifar],[generative]},<br> month = {feb},<br> title = {FearNet: Brain-Inspired Model for Incremental Learning},<br> url = {https://openreview.net/pdf?id=SJ1Xmf-Rb},<br> year = {2018}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="kemker2018bBioinspired_Methods_more2" style="display: none">
    Incremental class learning involves sequentially learning classes in bursts of examples from the same class. This violates the assumptions that underlie methods for training standard deep neural networks, and will cause them to suffer from catastrophic forgetting. Arguably, the best method for incremental class learning is iCaRL, but it requires storing training examples for each class, making it challenging to scale. Here, we propose FearNet for incremental class learning. FearNet is a generative model that does not store previous examples, making it memory efficient. FearNet uses a brain-inspired dual-memory system in which new memories are consolidated from a network for recent memories inspired by the mammalian hippocampal complex to a network for long-term storage inspired by medial prefrontal cortex. Memory consolidation is inspired by mechanisms that occur during sleep. FearNet also uses a module inspired by the basolateral amygdala for determining which memory system to use for recall. FearNet achieves state-of-the-art performance at incremental class learning on image (CIFAR-100, CUB-200) and audio classification (AudioSet) benchmarks.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="kemker2018bBioinspired_Methods_more3" style="display: none">
    N.A.
</span></p>
<script>
    function kemker2018bBioinspired_MethodsFunction() {
      var moreText = document.getElementById("kemker2018bBioinspired_Methods_more");
      var moreText2 = document.getElementById("kemker2018bBioinspired_Methods_more2");
      var moreText3 = document.getElementById("kemker2018bBioinspired_Methods_more3");
      var btnText = document.getElementById("kemker2018bBioinspired_Methods_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function kemker2018bBioinspired_MethodsFunction2() {
      var moreText = document.getElementById("kemker2018bBioinspired_Methods_more2");
      var moreText1 = document.getElementById("kemker2018bBioinspired_Methods_more");
      var moreText3 = document.getElementById("kemker2018bBioinspired_Methods_more3");
      var btnText = document.getElementById("kemker2018bBioinspired_Methods_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function kemker2018bBioinspired_MethodsFunction3() {
      var moreText = document.getElementById("kemker2018bBioinspired_Methods_more3");
      var moreText1 = document.getElementById("kemker2018bBioinspired_Methods_more");
      var moreText2 = document.getElementById("kemker2018bBioinspired_Methods_more2");
      var btnText = document.getElementById("kemker2018bBioinspired_Methods_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1701.06106">Neurogenesis-inspired dictionary learning: Online model adaption in a changing world</a> by Sahil Garg, Irina Rish, Guillermo Cecchi and Aurelie Lozano. <em>IJCAI International Joint Conference on Artificial Intelligence</em>, 1696–1702, 2017. <span style='background-color:BlueViolet; padding: 2px; border-radius:4px; border: 1px solid black;'>[nlp]</span> <span style='background-color:#E5E4E2; padding: 2px; border-radius:4px; border: 1px solid black;'>[vision]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Garg2017aBioinspired_MethodsFunction()" id="Garg2017aBioinspired_Methods_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Garg2017aBioinspired_MethodsFunction2()" id="Garg2017aBioinspired_Methods_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Garg2017aBioinspired_MethodsFunction3()" id="Garg2017aBioinspired_Methods_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Garg2017aBioinspired_Methods_more" style="display: none">
    @article{Garg2017a,<br> archiveprefix = {arXiv},<br> arxivid = {1701.06106},<br> author = {Garg, Sahil and Rish, Irina and Cecchi, Guillermo and Lozano, Aurelie},<br> doi = {10.24963/ijcai.2017/235},<br> eprint = {1701.06106},<br> isbn = {9780999241103},<br> issn = {10450823},<br> journal = {IJCAI International Joint Conference on Artificial Intelligence},<br> keywords = {[nlp],[vision]},<br> mendeley-tags = {[nlp],[vision]},<br> pages = {1696--1702},<br> title = {Neurogenesis-inspired dictionary learning: Online model adaption in a changing world},<br> url = {https://arxiv.org/abs/1701.06106},<br> year = {2017}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Garg2017aBioinspired_Methods_more2" style="display: none">
    We address the problem of online model adaptation when learning representations from non-stationary data streams. Specifically, we focus here on online dictionary learning (i.e. sparse linear autoencoder), and propose a simple but effective online modelselection approach involving "birth" (addition) and "death" (removal) of hidden units representing dictionary elements, in response to changing inputs; we draw inspiration from the adult neurogenesis phenomenon in the dentate gyrus of the hippocampus, known to be associated with better adaptation to new environments. Empirical evaluation on real-life datasets (images and text), as well as on synthetic data, demonstrates that the proposed approach can considerably outperform the state-of-art non-adaptive online sparse coding of [Mairal et al., 2009] in the presence of non-stationary data. Moreover, we identify certain data- and model properties associated with such improvements.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Garg2017aBioinspired_Methods_more3" style="display: none">
    N.A.
</span></p>
<script>
    function Garg2017aBioinspired_MethodsFunction() {
      var moreText = document.getElementById("Garg2017aBioinspired_Methods_more");
      var moreText2 = document.getElementById("Garg2017aBioinspired_Methods_more2");
      var moreText3 = document.getElementById("Garg2017aBioinspired_Methods_more3");
      var btnText = document.getElementById("Garg2017aBioinspired_Methods_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Garg2017aBioinspired_MethodsFunction2() {
      var moreText = document.getElementById("Garg2017aBioinspired_Methods_more2");
      var moreText1 = document.getElementById("Garg2017aBioinspired_Methods_more");
      var moreText3 = document.getElementById("Garg2017aBioinspired_Methods_more3");
      var btnText = document.getElementById("Garg2017aBioinspired_Methods_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Garg2017aBioinspired_MethodsFunction3() {
      var moreText = document.getElementById("Garg2017aBioinspired_Methods_more3");
      var moreText1 = document.getElementById("Garg2017aBioinspired_Methods_more");
      var moreText2 = document.getElementById("Garg2017aBioinspired_Methods_more2");
      var btnText = document.getElementById("Garg2017aBioinspired_Methods_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="https://doi.org/10.1162/NECO_a_00893">Continuous Online Sequence Learning with an Unsupervised Neural Network Model</a> by Yuwei Cui, Subutai Ahmad and Jeff Hawkins. <em>Neural Computation</em>, 2474–2504, 2016. <span style='background-color:yellow; padding: 2px; border-radius:4px; border: 1px solid black;'>[spiking]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Cui2016aBioinspired_MethodsFunction()" id="Cui2016aBioinspired_Methods_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Cui2016aBioinspired_MethodsFunction2()" id="Cui2016aBioinspired_Methods_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Cui2016aBioinspired_MethodsFunction3()" id="Cui2016aBioinspired_Methods_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Cui2016aBioinspired_Methods_more" style="display: none">
    @article{Cui2016a,<br> annote = {Publisher: MIT Press},<br> author = {Cui, Yuwei and Ahmad, Subutai and Hawkins, Jeff},<br> doi = {10.1162/NECO_a_00893},<br> issn = {0899-7667},<br> journal = {Neural Computation},<br> keywords = {[spiking],htm},<br> mendeley-tags = {[spiking]},<br> month = {sep},<br> number = {11},<br> pages = {2474--2504},<br> title = {Continuous Online Sequence Learning with an Unsupervised Neural Network Model},<br> url = {https://doi.org/10.1162/NECO_a_00893},<br> volume = {28},<br> year = {2016}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Cui2016aBioinspired_Methods_more2" style="display: none">
    The ability to recognize and predict temporal sequences of sensory inputs is vital for survival in natural environments. Based on many known properties of cortical neurons, hierarchical temporal memory (HTM) sequence memory recently has been proposed as a theoretical framework for sequence learning in the cortex. In this letter, we analyze properties of HTM sequence memory and apply it to sequence learning and prediction problems with streaming data. We show the model is able to continuously learn a large number of variable order temporal sequences using an unsupervised Hebbian-like learning rule. The sparse temporal codes formed by the model can robustly handle branching temporal sequences by maintaining multiple predictions until there is sufficient disambiguating evidence. We compare the HTM sequence memory with other sequence learning algorithms, including statistical methods—autoregressive integrated moving average; feedforward neural networks—time delay neural network and online sequential extreme learning machine; and recurrent neural networks—long short-term memory and echo-state networks on sequence prediction problems with both artificial and real-world data. The HTM model achieves comparable accuracy to other state-of-the-art algorithms. The model also exhibits properties that are critical for sequence learning, including continuous online learning, the ability to handle multiple predictions and branching sequences with high-order statistics, robustness to sensor noise and fault tolerance, and good performance without task-specific hyperparameter tuning. Therefore, the HTM sequence memory not only advances our understanding of how the brain may solve the sequence learning problem but is also applicable to real-world sequence learning problems from continuous data streams.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Cui2016aBioinspired_Methods_more3" style="display: none">
    Publisher: MIT Press
</span></p>
<script>
    function Cui2016aBioinspired_MethodsFunction() {
      var moreText = document.getElementById("Cui2016aBioinspired_Methods_more");
      var moreText2 = document.getElementById("Cui2016aBioinspired_Methods_more2");
      var moreText3 = document.getElementById("Cui2016aBioinspired_Methods_more3");
      var btnText = document.getElementById("Cui2016aBioinspired_Methods_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Cui2016aBioinspired_MethodsFunction2() {
      var moreText = document.getElementById("Cui2016aBioinspired_Methods_more2");
      var moreText1 = document.getElementById("Cui2016aBioinspired_Methods_more");
      var moreText3 = document.getElementById("Cui2016aBioinspired_Methods_more3");
      var btnText = document.getElementById("Cui2016aBioinspired_Methods_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Cui2016aBioinspired_MethodsFunction3() {
      var moreText = document.getElementById("Cui2016aBioinspired_Methods_more3");
      var moreText1 = document.getElementById("Cui2016aBioinspired_Methods_more");
      var moreText2 = document.getElementById("Cui2016aBioinspired_Methods_more2");
      var btnText = document.getElementById("Cui2016aBioinspired_Methods_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p>Compete to Compute by Rupesh Kumar Srivastava, Jonathan Masci, Sohrob Kazerounian, Faustino Gomez and Jürgen Schmidhuber. <em>Advances in Neural Information Processing Systems 26</em>, 2013. <span style='background-color:MediumAquaMarine; padding: 2px; border-radius:4px; border: 1px solid black;'>[mnist]</span> <span style='background-color:DarkSalmon; padding: 2px; border-radius:4px; border: 1px solid black;'>[sparsity]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="srivastava2013aBioinspired_MethodsFunction()" id="srivastava2013aBioinspired_Methods_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="srivastava2013aBioinspired_MethodsFunction2()" id="srivastava2013aBioinspired_Methods_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="srivastava2013aBioinspired_MethodsFunction3()" id="srivastava2013aBioinspired_Methods_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="srivastava2013aBioinspired_Methods_more" style="display: none">
    @inproceedings{srivastava2013a,<br> author = {Srivastava, Rupesh Kumar and Masci, Jonathan and Kazerounian, Sohrob and Gomez, Faustino and Schmidhuber, Jürgen},<br> booktitle = {Advances in Neural Information Processing Systems 26},<br> file = {::},<br> keywords = {[mnist],[sparsity]},<br> mendeley-tags = {[mnist],[sparsity]},<br> title = {Compete to Compute},<br> year = {2013}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="srivastava2013aBioinspired_Methods_more2" style="display: none">
    Local competition among neighboring neurons is common in biological neu-ral networks (NNs). In this paper, we apply the concept to gradient-based, backprop-trained artificial multilayer NNs. NNs with competing linear units tend to outperform those with non-competing nonlinear units, and avoid catastrophic forgetting when training sets change over time.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="srivastava2013aBioinspired_Methods_more3" style="display: none">
    N.A.
</span></p>
<script>
    function srivastava2013aBioinspired_MethodsFunction() {
      var moreText = document.getElementById("srivastava2013aBioinspired_Methods_more");
      var moreText2 = document.getElementById("srivastava2013aBioinspired_Methods_more2");
      var moreText3 = document.getElementById("srivastava2013aBioinspired_Methods_more3");
      var btnText = document.getElementById("srivastava2013aBioinspired_Methods_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function srivastava2013aBioinspired_MethodsFunction2() {
      var moreText = document.getElementById("srivastava2013aBioinspired_Methods_more2");
      var moreText1 = document.getElementById("srivastava2013aBioinspired_Methods_more");
      var moreText3 = document.getElementById("srivastava2013aBioinspired_Methods_more3");
      var btnText = document.getElementById("srivastava2013aBioinspired_Methods_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function srivastava2013aBioinspired_MethodsFunction3() {
      var moreText = document.getElementById("srivastava2013aBioinspired_Methods_more3");
      var moreText1 = document.getElementById("srivastava2013aBioinspired_Methods_more");
      var moreText2 = document.getElementById("srivastava2013aBioinspired_Methods_more2");
      var btnText = document.getElementById("srivastava2013aBioinspired_Methods_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="http://ieeexplore.ieee.org/document/6707047/">Mitigation of catastrophic forgetting in recurrent neural networks using a Fixed Expansion Layer</a> by Robert Coop and Itamar Arel. <em>The 2013 International Joint Conference on Neural Networks (IJCNN)</em>, 1–7, 2013. <span style='background-color:MediumAquaMarine; padding: 2px; border-radius:4px; border: 1px solid black;'>[mnist]</span> <span style='background-color:AliceBlue; padding: 2px; border-radius:4px; border: 1px solid black;'>[rnn]</span> <span style='background-color:DarkSalmon; padding: 2px; border-radius:4px; border: 1px solid black;'>[sparsity]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Coop2013aBioinspired_MethodsFunction()" id="Coop2013aBioinspired_Methods_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Coop2013aBioinspired_MethodsFunction2()" id="Coop2013aBioinspired_Methods_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Coop2013aBioinspired_MethodsFunction3()" id="Coop2013aBioinspired_Methods_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Coop2013aBioinspired_Methods_more" style="display: none">
    @inproceedings{Coop2013a,<br> address = {Dallas, TX, USA},<br> author = {Coop, Robert and Arel, Itamar},<br> booktitle = {The 2013 International Joint Conference on Neural Networks (IJCNN)},<br> doi = {10.1109/IJCNN.2013.6707047},<br> isbn = {978-1-4673-6129-3 978-1-4673-6128-6},<br> keywords = {[mnist],[rnn],[sparsity],fel,recurrent fel},<br> language = {en},<br> mendeley-tags = {[mnist],[rnn],[sparsity]},<br> month = {aug},<br> pages = {1--7},<br> publisher = {IEEE},<br> title = {Mitigation of catastrophic forgetting in recurrent neural networks using a Fixed Expansion Layer},<br> url = {http://ieeexplore.ieee.org/document/6707047/},<br> year = {2013}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Coop2013aBioinspired_Methods_more2" style="display: none">
    Catastrophic forgetting (or catastrophic interference) in supervised learning systems is the drastic loss of previously stored information caused by the learning of new information. While substantial work has been published on addressing catastrophic forgetting in memoryless supervised learning systems (e.g. feedforward neural networks), the problem has received limited attention in the context of dynamic systems, particularly recurrent neural networks. In this paper, we introduce a solution for mitigating catastrophic forgetting in RNNs based on enhancing the Fixed Expansion Layer (FEL) neural network which exploits sparse coding of hidden neuron activations. Simulation results on several non-stationary data sets clearly demonstrate the effectiveness of the proposed architecture.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Coop2013aBioinspired_Methods_more3" style="display: none">
    N.A.
</span></p>
<script>
    function Coop2013aBioinspired_MethodsFunction() {
      var moreText = document.getElementById("Coop2013aBioinspired_Methods_more");
      var moreText2 = document.getElementById("Coop2013aBioinspired_Methods_more2");
      var moreText3 = document.getElementById("Coop2013aBioinspired_Methods_more3");
      var btnText = document.getElementById("Coop2013aBioinspired_Methods_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Coop2013aBioinspired_MethodsFunction2() {
      var moreText = document.getElementById("Coop2013aBioinspired_Methods_more2");
      var moreText1 = document.getElementById("Coop2013aBioinspired_Methods_more");
      var moreText3 = document.getElementById("Coop2013aBioinspired_Methods_more3");
      var btnText = document.getElementById("Coop2013aBioinspired_Methods_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Coop2013aBioinspired_MethodsFunction3() {
      var moreText = document.getElementById("Coop2013aBioinspired_Methods_more3");
      var moreText1 = document.getElementById("Coop2013aBioinspired_Methods_more");
      var moreText2 = document.getElementById("Coop2013aBioinspired_Methods_more2");
      var btnText = document.getElementById("Coop2013aBioinspired_Methods_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p>Mitigation of catastrophic interference in neural networks using a fixed expansion layer by Robert Coop and Itamar Arel. <em>2012 IEEE 55th International Midwest Symposium on Circuits and Systems (MWSCAS)</em>, 726–729, 2012. <span style='background-color:DarkSalmon; padding: 2px; border-radius:4px; border: 1px solid black;'>[sparsity]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Coop2012aBioinspired_MethodsFunction()" id="Coop2012aBioinspired_Methods_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Coop2012aBioinspired_MethodsFunction2()" id="Coop2012aBioinspired_Methods_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Coop2012aBioinspired_MethodsFunction3()" id="Coop2012aBioinspired_Methods_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Coop2012aBioinspired_Methods_more" style="display: none">
    @inproceedings{Coop2012a,<br> annote = {ISSN: 1548-3746},<br> author = {Coop, Robert and Arel, Itamar},<br> booktitle = {2012 IEEE 55th International Midwest Symposium on Circuits and Systems (MWSCAS)},<br> doi = {10.1109/MWSCAS.2012.6292123},<br> keywords = {Accuracy,Biological neural networks,Feedforward neural networks,Interference,Neurons,Training,[sparsity],binary activations,catastrophic forgetting,catastrophic interference,fixed expansion layer feedforward neural network,multilayer perceptron,multilayer perceptrons,non-stationary inputs,sparse neurons},<br> mendeley-tags = {[sparsity]},<br> month = {aug},<br> pages = {726--729},<br> title = {Mitigation of catastrophic interference in neural networks using a fixed expansion layer},<br> year = {2012}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Coop2012aBioinspired_Methods_more2" style="display: none">
    In this paper we present the fixed expansion layer (FEL) feedforward neural network designed for balancing plasticity and stability in the presence of non-stationary inputs. Catastrophic interference (or catastrophic forgetting) refers to the drastic loss of previously learned information when a neural network is trained on new or different information. The goal of the FEL network is to reduce the effect of catastrophic interference by augmenting a multilayer perceptron with a layer of sparse neurons with binary activations. We compare the FEL network's performance to that of other algorithms designed to combat the effects of catastrophic interference and demonstrate that the FEL network is able to retain information for significantly longer periods of time with substantially lower computational requirements.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Coop2012aBioinspired_Methods_more3" style="display: none">
    ISSN: 1548-3746
</span></p>
<script>
    function Coop2012aBioinspired_MethodsFunction() {
      var moreText = document.getElementById("Coop2012aBioinspired_Methods_more");
      var moreText2 = document.getElementById("Coop2012aBioinspired_Methods_more2");
      var moreText3 = document.getElementById("Coop2012aBioinspired_Methods_more3");
      var btnText = document.getElementById("Coop2012aBioinspired_Methods_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Coop2012aBioinspired_MethodsFunction2() {
      var moreText = document.getElementById("Coop2012aBioinspired_Methods_more2");
      var moreText1 = document.getElementById("Coop2012aBioinspired_Methods_more");
      var moreText3 = document.getElementById("Coop2012aBioinspired_Methods_more3");
      var btnText = document.getElementById("Coop2012aBioinspired_Methods_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Coop2012aBioinspired_MethodsFunction3() {
      var moreText = document.getElementById("Coop2012aBioinspired_Methods_more3");
      var moreText1 = document.getElementById("Coop2012aBioinspired_Methods_more");
      var moreText2 = document.getElementById("Coop2012aBioinspired_Methods_more2");
      var btnText = document.getElementById("Coop2012aBioinspired_Methods_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="https://www.nature.com/articles/nn1100_1178">Synaptic plasticity: taming the beast</a> by L F Abbott and Sacha B Nelson. <em>Nature Neuroscience</em>, 1178–1183, 2000. <span style='background-color:#99C68E; padding: 2px; border-radius:4px; border: 1px solid black;'>[hebbian]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Abbott2000aBioinspired_MethodsFunction()" id="Abbott2000aBioinspired_Methods_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Abbott2000aBioinspired_MethodsFunction2()" id="Abbott2000aBioinspired_Methods_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Abbott2000aBioinspired_MethodsFunction3()" id="Abbott2000aBioinspired_Methods_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Abbott2000aBioinspired_Methods_more" style="display: none">
    @article{Abbott2000a,<br> author = {Abbott, L F and Nelson, Sacha B},<br> doi = {10.1038/81453},<br> issn = {1546-1726},<br> journal = {Nature Neuroscience},<br> keywords = {[hebbian]},<br> language = {en},<br> mendeley-tags = {[hebbian]},<br> month = {nov},<br> number = {11},<br> pages = {1178--1183},<br> shorttitle = {Synaptic plasticity},<br> title = {Synaptic plasticity: taming the beast},<br> url = {https://www.nature.com/articles/nn1100_1178},<br> volume = {3},<br> year = {2000}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Abbott2000aBioinspired_Methods_more2" style="display: none">
    Synaptic plasticity provides the basis for most models of learning, memory and development in neural circuits. To generate realistic results, synapse-specific Hebbian forms of plasticity, such as long-term potentiation and depression, must be augmented by global processes that regulate overall levels of neuronal and network activity. Regulatory processes are often as important as the more intensively studied Hebbian processes in determining the consequences of synaptic plasticity for network function. Recent experimental results suggest several novel mechanisms for regulating levels of activity in conjunction with Hebbian synaptic modification. We review three of them—synaptic scaling, spike-timing dependent plasticity and synaptic redistribution—and discuss their functional implications.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Abbott2000aBioinspired_Methods_more3" style="display: none">
    N.A.
</span></p>
<script>
    function Abbott2000aBioinspired_MethodsFunction() {
      var moreText = document.getElementById("Abbott2000aBioinspired_Methods_more");
      var moreText2 = document.getElementById("Abbott2000aBioinspired_Methods_more2");
      var moreText3 = document.getElementById("Abbott2000aBioinspired_Methods_more3");
      var btnText = document.getElementById("Abbott2000aBioinspired_Methods_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Abbott2000aBioinspired_MethodsFunction2() {
      var moreText = document.getElementById("Abbott2000aBioinspired_Methods_more2");
      var moreText1 = document.getElementById("Abbott2000aBioinspired_Methods_more");
      var moreText3 = document.getElementById("Abbott2000aBioinspired_Methods_more3");
      var btnText = document.getElementById("Abbott2000aBioinspired_Methods_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Abbott2000aBioinspired_MethodsFunction3() {
      var moreText = document.getElementById("Abbott2000aBioinspired_Methods_more3");
      var moreText1 = document.getElementById("Abbott2000aBioinspired_Methods_more");
      var moreText2 = document.getElementById("Abbott2000aBioinspired_Methods_more2");
      var btnText = document.getElementById("Abbott2000aBioinspired_Methods_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
</ul>
</div>
<div class="section" id="meta-continual-learning">
<h3>Meta Continual Learning<a class="headerlink" href="#meta-continual-learning" title="Permalink to this headline">¶</a></h3>
<p>In this section we list all the papers related to the meta-continual learning.</p>
<ul class="simple">
<li><p><a class="reference external" href="http://arxiv.org/abs/2002.09571">Learning to Continually Learn</a> by Shawn Beaulieu, Lapo Frati, Thomas Miconi, Joel Lehman, Kenneth O. Stanley, Jeff Clune and Nick Cheney. <em>ECAI</em>, 2020. <span style='background-color:#E5E4E2; padding: 2px; border-radius:4px; border: 1px solid black;'>[vision]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="beaulieu2020aMeta_Continual_LearningFunction()" id="beaulieu2020aMeta_Continual_Learning_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="beaulieu2020aMeta_Continual_LearningFunction2()" id="beaulieu2020aMeta_Continual_Learning_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="beaulieu2020aMeta_Continual_LearningFunction3()" id="beaulieu2020aMeta_Continual_Learning_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="beaulieu2020aMeta_Continual_Learning_more" style="display: none">
    @inproceedings{beaulieu2020a,<br> archiveprefix = {arXiv},<br> arxivid = {2002.09571},<br> author = {Beaulieu, Shawn and Frati, Lapo and Miconi, Thomas and Lehman, Joel and Stanley, Kenneth O. and Clune, Jeff and Cheney, Nick},<br> booktitle = {ECAI},<br> eprint = {2002.09571},<br> file = {:home/andrea/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Beaulieu et al. - 2020 - Learning to Continually Learn(3).pdf:pdf},<br> month = {feb},<br> title = {Learning to Continually Learn},<br> url = {http://arxiv.org/abs/2002.09571},<br> year = {2020}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="beaulieu2020aMeta_Continual_Learning_more2" style="display: none">
    Continual lifelong learning requires an agent or model to learn many sequentially ordered tasks, building on previous knowledge without catastrophically forgetting it. Much work has gone towards preventing the default tendency of machine learning models to catastrophically forget, yet virtually all such work involves manually-designed solutions to the problem. We instead advocate meta-learning a solution to catastrophic forgetting, allowing AI to learn to continually learn. Inspired by neuromodulatory processes in the brain, we propose A Neuromodulated Meta-Learning Algorithm (ANML). It differentiates through a sequential learning process to meta-learn an activation-gating function that enables context-dependent selective activation within a deep neural network. Specifically, a neuromodulatory (NM) neural network gates the forward pass of another (otherwise normal) neural network called the prediction learning network (PLN). The NM network also thus indirectly controls selective plasticity (i.e. the backward pass of) the PLN. ANML enables continual learning without catastrophic forgetting at scale: it produces state-of-the-art continual learning performance, sequentially learning as many as 600 classes (over 9,000 SGD updates).
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="beaulieu2020aMeta_Continual_Learning_more3" style="display: none">
    N.A.
</span></p>
<script>
    function beaulieu2020aMeta_Continual_LearningFunction() {
      var moreText = document.getElementById("beaulieu2020aMeta_Continual_Learning_more");
      var moreText2 = document.getElementById("beaulieu2020aMeta_Continual_Learning_more2");
      var moreText3 = document.getElementById("beaulieu2020aMeta_Continual_Learning_more3");
      var btnText = document.getElementById("beaulieu2020aMeta_Continual_Learning_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function beaulieu2020aMeta_Continual_LearningFunction2() {
      var moreText = document.getElementById("beaulieu2020aMeta_Continual_Learning_more2");
      var moreText1 = document.getElementById("beaulieu2020aMeta_Continual_Learning_more");
      var moreText3 = document.getElementById("beaulieu2020aMeta_Continual_Learning_more3");
      var btnText = document.getElementById("beaulieu2020aMeta_Continual_Learning_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function beaulieu2020aMeta_Continual_LearningFunction3() {
      var moreText = document.getElementById("beaulieu2020aMeta_Continual_Learning_more3");
      var moreText1 = document.getElementById("beaulieu2020aMeta_Continual_Learning_more");
      var moreText2 = document.getElementById("beaulieu2020aMeta_Continual_Learning_more2");
      var btnText = document.getElementById("beaulieu2020aMeta_Continual_Learning_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p>Learning to learn without forgetting by maximizing transfer and minimizing interference by Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu and Gerald Tesauro. <em>ICLR</em>, 2019. <span style='background-color:MediumAquaMarine; padding: 2px; border-radius:4px; border: 1px solid black;'>[mnist]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="riemer2019aMeta_Continual_LearningFunction()" id="riemer2019aMeta_Continual_Learning_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="riemer2019aMeta_Continual_LearningFunction2()" id="riemer2019aMeta_Continual_Learning_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="riemer2019aMeta_Continual_LearningFunction3()" id="riemer2019aMeta_Continual_Learning_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="riemer2019aMeta_Continual_Learning_more" style="display: none">
    @inproceedings{riemer2019a,<br> author = {Riemer, Matthew and Cases, Ignacio and Ajemian, Robert and Liu, Miao and Rish, Irina and Tu, Yuhai and Tesauro, Gerald},<br> booktitle = {ICLR},<br> file = {:home/andrea/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Riemer et al. - 2019 - Learning to learn without forgetting by maximizing transfer and minimizing interference(2).pdf:pdf},<br> keywords = {[mnist]},<br> mendeley-tags = {[mnist]},<br> month = {sep},<br> title = {Learning to learn without forgetting by maximizing transfer and minimizing interference},<br> year = {2019}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="riemer2019aMeta_Continual_Learning_more2" style="display: none">
    Lack of performance when it comes to continual learning over non-stationary distributions of data remains a major challenge in scaling neural network learning to more human realistic settings. In this work we propose a new conceptualization of the continual learning problem in terms of a temporally symmetric trade-off between transfer and interference that can be optimized by enforcing gradient alignment across examples. We then propose a new algorithm, Meta-Experience Replay (MER), that directly exploits this view by combining experience replay with optimization based meta-learning. This method learns parameters that make interference based on future gradients less likely and transfer based on future gradients more likely. 1 We conduct experiments across continual lifelong supervised learning benchmarks and non-stationary reinforcement learning environments demonstrating that our approach consistently outperforms recently proposed baselines for continual learning. Our experiments show that the gap between the performance of MER and baseline algorithms grows both as the environment gets more non-stationary and as the fraction of the total experiences stored gets smaller.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="riemer2019aMeta_Continual_Learning_more3" style="display: none">
    N.A.
</span></p>
<script>
    function riemer2019aMeta_Continual_LearningFunction() {
      var moreText = document.getElementById("riemer2019aMeta_Continual_Learning_more");
      var moreText2 = document.getElementById("riemer2019aMeta_Continual_Learning_more2");
      var moreText3 = document.getElementById("riemer2019aMeta_Continual_Learning_more3");
      var btnText = document.getElementById("riemer2019aMeta_Continual_Learning_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function riemer2019aMeta_Continual_LearningFunction2() {
      var moreText = document.getElementById("riemer2019aMeta_Continual_Learning_more2");
      var moreText1 = document.getElementById("riemer2019aMeta_Continual_Learning_more");
      var moreText3 = document.getElementById("riemer2019aMeta_Continual_Learning_more3");
      var btnText = document.getElementById("riemer2019aMeta_Continual_Learning_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function riemer2019aMeta_Continual_LearningFunction3() {
      var moreText = document.getElementById("riemer2019aMeta_Continual_Learning_more3");
      var moreText1 = document.getElementById("riemer2019aMeta_Continual_Learning_more");
      var moreText2 = document.getElementById("riemer2019aMeta_Continual_Learning_more2");
      var btnText = document.getElementById("riemer2019aMeta_Continual_Learning_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="https://github.com/khurramjaved96/mrcl">Meta-Learning Representations for Continual Learning</a> by Khurram Javed and Martha White. <em>NeurIPS</em>, 2019. <span style='background-color:#FDD017; padding: 2px; border-radius:4px; border: 1px solid black;'>[omniglot]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="khurram2019aMeta_Continual_LearningFunction()" id="khurram2019aMeta_Continual_Learning_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="khurram2019aMeta_Continual_LearningFunction2()" id="khurram2019aMeta_Continual_Learning_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="khurram2019aMeta_Continual_LearningFunction3()" id="khurram2019aMeta_Continual_Learning_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="khurram2019aMeta_Continual_Learning_more" style="display: none">
    @inproceedings{khurram2019a,<br> author = {Javed, Khurram and White, Martha},<br> booktitle = {NeurIPS},<br> file = {:home/andrea/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Javed, White - 2019 - Meta-Learning Representations for Continual Learning(2).pdf:pdf},<br> title = {Meta-Learning Representations for Continual Learning},<br> url = {https://github.com/khurramjaved96/mrcl},<br> year = {2019}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="khurram2019aMeta_Continual_Learning_more2" style="display: none">
    A continual learning agent should be able to build on top of existing knowledge to learn on new data quickly while minimizing forgetting. Current intelligent systems based on neural network function approximators arguably do the opposite-they are highly prone to forgetting and rarely trained to facilitate future learning. One reason for this poor behavior is that they learn from a representation that is not explicitly trained for these two goals. In this paper, we propose OML, an objective that directly minimizes catastrophic interference by learning representations that accelerate future learning and are robust to forgetting under online updates in continual learning. We show that it is possible to learn naturally sparse representations that are more effective for online updating. Moreover, our algorithm is complementary to existing continual learning strategies, such as MER and GEM. Finally, we demonstrate that a basic online updating strategy on representations learned by OML is competitive with rehearsal based methods for continual learning. 1
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="khurram2019aMeta_Continual_Learning_more3" style="display: none">
    N.A.
</span></p>
<script>
    function khurram2019aMeta_Continual_LearningFunction() {
      var moreText = document.getElementById("khurram2019aMeta_Continual_Learning_more");
      var moreText2 = document.getElementById("khurram2019aMeta_Continual_Learning_more2");
      var moreText3 = document.getElementById("khurram2019aMeta_Continual_Learning_more3");
      var btnText = document.getElementById("khurram2019aMeta_Continual_Learning_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function khurram2019aMeta_Continual_LearningFunction2() {
      var moreText = document.getElementById("khurram2019aMeta_Continual_Learning_more2");
      var moreText1 = document.getElementById("khurram2019aMeta_Continual_Learning_more");
      var moreText3 = document.getElementById("khurram2019aMeta_Continual_Learning_more3");
      var btnText = document.getElementById("khurram2019aMeta_Continual_Learning_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function khurram2019aMeta_Continual_LearningFunction3() {
      var moreText = document.getElementById("khurram2019aMeta_Continual_Learning_more3");
      var moreText1 = document.getElementById("khurram2019aMeta_Continual_Learning_more");
      var moreText2 = document.getElementById("khurram2019aMeta_Continual_Learning_more2");
      var btnText = document.getElementById("khurram2019aMeta_Continual_Learning_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1806.06928">Meta continual learning</a> by Risto Vuorio, Dong-Yeon Cho, Daejoong Kim and Jiwon Kim. <em>arXiv</em>, 2018. <span style='background-color:MediumAquaMarine; padding: 2px; border-radius:4px; border: 1px solid black;'>[mnist]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="vuorio2018aMeta_Continual_LearningFunction()" id="vuorio2018aMeta_Continual_Learning_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="vuorio2018aMeta_Continual_LearningFunction2()" id="vuorio2018aMeta_Continual_Learning_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="vuorio2018aMeta_Continual_LearningFunction3()" id="vuorio2018aMeta_Continual_Learning_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="vuorio2018aMeta_Continual_Learning_more" style="display: none">
    @article{vuorio2018a,<br> archiveprefix = {arXiv},<br> arxivid = {1806.06928},<br> author = {Vuorio, Risto and Cho, Dong-Yeon and Kim, Daejoong and Kim, Jiwon},<br> eprint = {1806.06928},<br> journal = {arXiv},<br> keywords = {[mnist]},<br> mendeley-tags = {[mnist]},<br> title = {Meta continual learning},<br> url = {https://arxiv.org/abs/1806.06928},<br> year = {2018}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="vuorio2018aMeta_Continual_Learning_more2" style="display: none">
    Using neural networks in practical settings would benefit from the ability of the networks to learn new tasks throughout their lifetimes without forgetting the previous tasks. This ability is limited in the current deep neural networks by a problem called catastrophic forgetting, where training on new tasks tends to severely degrade performance on previous tasks. One way to lessen the impact of the forgetting problem is to constrain parameters that are important to previous tasks to stay close to the optimal parameters. Recently, multiple competitive approaches for computing the importance of the parameters with respect to the previous tasks have been presented. In this paper, we propose a learning to optimize algorithm for mitigating catastrophic forgetting. Instead of trying to formulate a new constraint function ourselves, we propose to train another neural network to predict parameter update steps that respect the importance of parameters to the previous tasks. In the proposed meta-training scheme, the update predictor is trained to minimize loss on a combination of current and past tasks. We show experimentally that the proposed approach works in the continual learning setting.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="vuorio2018aMeta_Continual_Learning_more3" style="display: none">
    N.A.
</span></p>
<script>
    function vuorio2018aMeta_Continual_LearningFunction() {
      var moreText = document.getElementById("vuorio2018aMeta_Continual_Learning_more");
      var moreText2 = document.getElementById("vuorio2018aMeta_Continual_Learning_more2");
      var moreText3 = document.getElementById("vuorio2018aMeta_Continual_Learning_more3");
      var btnText = document.getElementById("vuorio2018aMeta_Continual_Learning_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function vuorio2018aMeta_Continual_LearningFunction2() {
      var moreText = document.getElementById("vuorio2018aMeta_Continual_Learning_more2");
      var moreText1 = document.getElementById("vuorio2018aMeta_Continual_Learning_more");
      var moreText3 = document.getElementById("vuorio2018aMeta_Continual_Learning_more3");
      var btnText = document.getElementById("vuorio2018aMeta_Continual_Learning_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function vuorio2018aMeta_Continual_LearningFunction3() {
      var moreText = document.getElementById("vuorio2018aMeta_Continual_Learning_more3");
      var moreText1 = document.getElementById("vuorio2018aMeta_Continual_Learning_more");
      var moreText2 = document.getElementById("vuorio2018aMeta_Continual_Learning_more2");
      var btnText = document.getElementById("vuorio2018aMeta_Continual_Learning_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
</ul>
</div>
<div class="section" id="continual-meta-learning">
<h3>Continual Meta Learning<a class="headerlink" href="#continual-meta-learning" title="Permalink to this headline">¶</a></h3>
<p>In this section we list all the papers related to the continual meta-learning.</p>
<ul class="simple">
<li><p><a class="reference external" href="http://arxiv.org/abs/2003.05856">Online Fast Adaptation and Knowledge Accumulation: a New Approach to Continual Learning</a> by Massimo Caccia, Pau Rodriguez, Oleksiy Ostapenko, Fabrice Normandin, Min Lin, Lucas Caccia, Issam Laradji, Irina Rish, Alexande Lacoste, David Vazquez and Laurent Charlin. <em>arXiv</em>, 2020. <span style='background-color:LightGray; padding: 2px; border-radius:4px; border: 1px solid black;'>[fashion]</span> <span style='background-color:Tomato; padding: 2px; border-radius:4px; border: 1px solid black;'>[framework]</span> <span style='background-color:MediumAquaMarine; padding: 2px; border-radius:4px; border: 1px solid black;'>[mnist]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="caccia2020aContinual_Meta_LearningFunction()" id="caccia2020aContinual_Meta_Learning_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="caccia2020aContinual_Meta_LearningFunction2()" id="caccia2020aContinual_Meta_Learning_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="caccia2020aContinual_Meta_LearningFunction3()" id="caccia2020aContinual_Meta_Learning_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="caccia2020aContinual_Meta_Learning_more" style="display: none">
    @article{caccia2020a,<br> archiveprefix = {arXiv},<br> arxivid = {2003.05856},<br> author = {Caccia, Massimo and Rodriguez, Pau and Ostapenko, Oleksiy and Normandin, Fabrice and Lin, Min and Caccia, Lucas and Laradji, Issam and Rish, Irina and Lacoste, Alexande and Vazquez, David and Charlin, Laurent},<br> eprint = {2003.05856},<br> file = {:home/andrea/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Caccia et al. - 2020 - Online Fast Adaptation and Knowledge Accumulation a New Approach to Continual Learning(2).pdf:pdf},<br> journal = {arXiv},<br> keywords = {[fashion],[framework],[mnist]},<br> mendeley-tags = {[fashion],[framework],[mnist]},<br> month = {mar},<br> title = {Online Fast Adaptation and Knowledge Accumulation: a New Approach to Continual Learning},<br> url = {http://arxiv.org/abs/2003.05856},<br> year = {2020}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="caccia2020aContinual_Meta_Learning_more2" style="display: none">
    Learning from non-stationary data remains a great challenge for machine learning. Continual learning addresses this problem in scenarios where the learning agent faces a stream of changing tasks. In these scenarios, the agent is expected to retain its highest performance on previous tasks without revisiting them while adapting well to the new tasks. Two new recent continual-learning scenarios have been proposed. In meta-continual learning, the model is pre-trained to minimize catastrophic forgetting when trained on a sequence of tasks. In continual-meta learning, the goal is faster remembering, i.e., focusing on how quickly the agent recovers performance rather than measuring the agent's performance without any adaptation. Both scenarios have the potential to propel the field forward. Yet in their original formulations, they each have limitations. As a remedy, we propose a more general scenario where an agent must quickly solve (new) out-of-distribution tasks, while also requiring fast remembering. We show that current continual learning, meta learning, meta-continual learning, and continual-meta learning techniques fail in this new scenario. Accordingly, we propose a strong baseline: Continual-MAML, an online extension of the popular MAML algorithm. In our empirical experiments, we show that our method is better suited to the new scenario than the methodologies mentioned above, as well as standard continual learning and meta learning approaches.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="caccia2020aContinual_Meta_Learning_more3" style="display: none">
    N.A.
</span></p>
<script>
    function caccia2020aContinual_Meta_LearningFunction() {
      var moreText = document.getElementById("caccia2020aContinual_Meta_Learning_more");
      var moreText2 = document.getElementById("caccia2020aContinual_Meta_Learning_more2");
      var moreText3 = document.getElementById("caccia2020aContinual_Meta_Learning_more3");
      var btnText = document.getElementById("caccia2020aContinual_Meta_Learning_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function caccia2020aContinual_Meta_LearningFunction2() {
      var moreText = document.getElementById("caccia2020aContinual_Meta_Learning_more2");
      var moreText1 = document.getElementById("caccia2020aContinual_Meta_Learning_more");
      var moreText3 = document.getElementById("caccia2020aContinual_Meta_Learning_more3");
      var btnText = document.getElementById("caccia2020aContinual_Meta_Learning_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function caccia2020aContinual_Meta_LearningFunction3() {
      var moreText = document.getElementById("caccia2020aContinual_Meta_Learning_more3");
      var moreText1 = document.getElementById("caccia2020aContinual_Meta_Learning_more");
      var moreText2 = document.getElementById("caccia2020aContinual_Meta_Learning_more2");
      var btnText = document.getElementById("caccia2020aContinual_Meta_Learning_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1912.08866">Continuous meta-learning without tasks</a> by James Harrison, Apoorva Sharma, Chelsea Finn and Marco Pavone. <em>arXiv</em>, 2019. <span style='background-color:DeepPink; padding: 2px; border-radius:4px; border: 1px solid black;'>[imagenet]</span> <span style='background-color:MediumAquaMarine; padding: 2px; border-radius:4px; border: 1px solid black;'>[mnist]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="harrison2019aContinual_Meta_LearningFunction()" id="harrison2019aContinual_Meta_Learning_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="harrison2019aContinual_Meta_LearningFunction2()" id="harrison2019aContinual_Meta_Learning_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="harrison2019aContinual_Meta_LearningFunction3()" id="harrison2019aContinual_Meta_Learning_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="harrison2019aContinual_Meta_Learning_more" style="display: none">
    @article{harrison2019a,<br> archiveprefix = {arXiv},<br> arxivid = {1912.08866},<br> author = {Harrison, James and Sharma, Apoorva and Finn, Chelsea and Pavone, Marco},<br> eprint = {1912.08866},<br> journal = {arXiv},<br> keywords = {[imagenet],[mnist]},<br> mendeley-tags = {[imagenet],[mnist]},<br> title = {Continuous meta-learning without tasks},<br> url = {https://arxiv.org/abs/1912.08866},<br> year = {2019}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="harrison2019aContinual_Meta_Learning_more2" style="display: none">
    Meta-learning is a promising strategy for learning to efficiently learn within new tasks, using data gathered from a distribution of tasks. However, the meta-learning literature thus far has focused on the task segmented setting, where at train-time, offline data is assumed to be split according to the underlying task, and at test-time, the algorithms are optimized to learn in a single task. In this work, we enable the application of generic meta-learning algorithms to settings where this task segmentation is unavailable, such as continual online learning with a time-varying task. We present meta-learning via online changepoint analysis (MOCA), an approach which augments a meta-learning algorithm with a differentiable Bayesian changepoint detection scheme. The framework allows both training and testing directly on time series data without segmenting it into discrete tasks. We demonstrate the utility of this approach on a nonlinear meta-regression benchmark as well as two meta-image-classification benchmarks.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="harrison2019aContinual_Meta_Learning_more3" style="display: none">
    N.A.
</span></p>
<script>
    function harrison2019aContinual_Meta_LearningFunction() {
      var moreText = document.getElementById("harrison2019aContinual_Meta_Learning_more");
      var moreText2 = document.getElementById("harrison2019aContinual_Meta_Learning_more2");
      var moreText3 = document.getElementById("harrison2019aContinual_Meta_Learning_more3");
      var btnText = document.getElementById("harrison2019aContinual_Meta_Learning_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function harrison2019aContinual_Meta_LearningFunction2() {
      var moreText = document.getElementById("harrison2019aContinual_Meta_Learning_more2");
      var moreText1 = document.getElementById("harrison2019aContinual_Meta_Learning_more");
      var moreText3 = document.getElementById("harrison2019aContinual_Meta_Learning_more3");
      var btnText = document.getElementById("harrison2019aContinual_Meta_Learning_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function harrison2019aContinual_Meta_LearningFunction3() {
      var moreText = document.getElementById("harrison2019aContinual_Meta_Learning_more3");
      var moreText1 = document.getElementById("harrison2019aContinual_Meta_Learning_more");
      var moreText2 = document.getElementById("harrison2019aContinual_Meta_Learning_more2");
      var btnText = document.getElementById("harrison2019aContinual_Meta_Learning_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="http://arxiv.org/abs/1906.05201">Task Agnostic Continual Learning via Meta Learning</a> by Xu He, Jakub Sygnowski, Alexandre Galashov, Andrei A Rusu, Yee Whye Teh and Razvan Pascanu. <em>arXiv:1906.05201 [cs, stat]</em>, 2019. <span style='background-color:MediumAquaMarine; padding: 2px; border-radius:4px; border: 1px solid black;'>[mnist]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="He2019aContinual_Meta_LearningFunction()" id="He2019aContinual_Meta_Learning_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="He2019aContinual_Meta_LearningFunction2()" id="He2019aContinual_Meta_Learning_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="He2019aContinual_Meta_LearningFunction3()" id="He2019aContinual_Meta_Learning_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="He2019aContinual_Meta_Learning_more" style="display: none">
    @misc{He2019a,<br> annote = {arXiv: 1906.05201},<br> author = {He, Xu and Sygnowski, Jakub and Galashov, Alexandre and Rusu, Andrei A and Teh, Yee Whye and Pascanu, Razvan},<br> booktitle = {arXiv:1906.05201 [cs, stat]},<br> keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computi,Statistics - Machine Learning,[mnist]},<br> mendeley-tags = {[mnist]},<br> month = {jun},<br> title = {Task Agnostic Continual Learning via Meta Learning},<br> url = {http://arxiv.org/abs/1906.05201},<br> year = {2019}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="He2019aContinual_Meta_Learning_more2" style="display: none">
    While neural networks are powerful function approximators, they suffer from catastrophic forgetting when the data distribution is not stationary. One particular formalism that studies learning under non-stationary distribution is provided by continual learning, where the non-stationarity is imposed by a sequence of distinct tasks. Most methods in this space assume, however, the knowledge of task boundaries, and focus on alleviating catastrophic forgetting. In this work, we depart from this view and move the focus towards faster remembering – i.e measuring how quickly the network recovers performance rather than measuring the network's performance without any adaptation. We argue that in many settings this can be more effective and that it opens the door to combining meta-learning and continual learning techniques, leveraging their complementary advantages. We propose a framework specific for the scenario where no information about task boundaries or task identity is given. It relies on a separation of concerns into what task is being solved and how the task should be solved. This framework is implemented by differentiating task specific parameters from task agnostic parameters, where the latter are optimized in a continual meta learning fashion, without access to multiple tasks at the same time. We showcase this framework in a supervised learning scenario and discuss the implication of the proposed formalism.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="He2019aContinual_Meta_Learning_more3" style="display: none">
    arXiv: 1906.05201
</span></p>
<script>
    function He2019aContinual_Meta_LearningFunction() {
      var moreText = document.getElementById("He2019aContinual_Meta_Learning_more");
      var moreText2 = document.getElementById("He2019aContinual_Meta_Learning_more2");
      var moreText3 = document.getElementById("He2019aContinual_Meta_Learning_more3");
      var btnText = document.getElementById("He2019aContinual_Meta_Learning_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function He2019aContinual_Meta_LearningFunction2() {
      var moreText = document.getElementById("He2019aContinual_Meta_Learning_more2");
      var moreText1 = document.getElementById("He2019aContinual_Meta_Learning_more");
      var moreText3 = document.getElementById("He2019aContinual_Meta_Learning_more3");
      var btnText = document.getElementById("He2019aContinual_Meta_Learning_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function He2019aContinual_Meta_LearningFunction3() {
      var moreText = document.getElementById("He2019aContinual_Meta_Learning_more3");
      var moreText1 = document.getElementById("He2019aContinual_Meta_Learning_more");
      var moreText2 = document.getElementById("He2019aContinual_Meta_Learning_more2");
      var btnText = document.getElementById("He2019aContinual_Meta_Learning_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1902.08438">Online meta-learning</a> by Chelsea Finn, Aravind Rajeswaran, Sham Kakade and Sergey Levine. <em>arXiv</em>, 2019. <span style='background-color:DarkCyan; padding: 2px; border-radius:4px; border: 1px solid black;'>[cifar]</span> <span style='background-color:MediumAquaMarine; padding: 2px; border-radius:4px; border: 1px solid black;'>[mnist]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="finn2019aContinual_Meta_LearningFunction()" id="finn2019aContinual_Meta_Learning_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="finn2019aContinual_Meta_LearningFunction2()" id="finn2019aContinual_Meta_Learning_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="finn2019aContinual_Meta_LearningFunction3()" id="finn2019aContinual_Meta_Learning_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="finn2019aContinual_Meta_Learning_more" style="display: none">
    @article{finn2019a,<br> archiveprefix = {arXiv},<br> arxivid = {1902.08438},<br> author = {Finn, Chelsea and Rajeswaran, Aravind and Kakade, Sham and Levine, Sergey},<br> eprint = {1902.08438},<br> journal = {arXiv},<br> keywords = {[cifar],[mnist]},<br> mendeley-tags = {[cifar],[mnist]},<br> title = {Online meta-learning},<br> url = {https://arxiv.org/abs/1902.08438},<br> year = {2019}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="finn2019aContinual_Meta_Learning_more2" style="display: none">
    A central capability of intelligent systems is the ability to continuously build upon previous experiences to speed up and enhance learning of new tasks. Two distinct research paradigms have studied this question. Meta-learning views this problem as learning a prior over model parameters that is amenable for fast adaptation on a new task, but typically assumes the set of tasks are available together as a batch. In contrast, online (regret based) learning considers a sequential setting in which problems are revealed one after the other, but conventionally train only a single model without any task-specific adaptation. This work introduces an online meta-learning setting, which merges ideas from both the aforementioned paradigms to better capture the spirit and practice of continual lifelong learning. We propose the follow the meta leader (FTML) algorithm which extends the MAML algorithm to this setting. Theoretically, this work provides an O(log T) regret guarantee with one additional higher order smoothness assumption (in comparison to the standard online setting). Our experimental evaluation on three different large-scale tasks suggest that the proposed algorithm significantly outperforms alternatives based on traditional online learning approaches.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="finn2019aContinual_Meta_Learning_more3" style="display: none">
    N.A.
</span></p>
<script>
    function finn2019aContinual_Meta_LearningFunction() {
      var moreText = document.getElementById("finn2019aContinual_Meta_Learning_more");
      var moreText2 = document.getElementById("finn2019aContinual_Meta_Learning_more2");
      var moreText3 = document.getElementById("finn2019aContinual_Meta_Learning_more3");
      var btnText = document.getElementById("finn2019aContinual_Meta_Learning_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function finn2019aContinual_Meta_LearningFunction2() {
      var moreText = document.getElementById("finn2019aContinual_Meta_Learning_more2");
      var moreText1 = document.getElementById("finn2019aContinual_Meta_Learning_more");
      var moreText3 = document.getElementById("finn2019aContinual_Meta_Learning_more3");
      var btnText = document.getElementById("finn2019aContinual_Meta_Learning_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function finn2019aContinual_Meta_LearningFunction3() {
      var moreText = document.getElementById("finn2019aContinual_Meta_Learning_more3");
      var moreText1 = document.getElementById("finn2019aContinual_Meta_Learning_more");
      var moreText2 = document.getElementById("finn2019aContinual_Meta_Learning_more2");
      var btnText = document.getElementById("finn2019aContinual_Meta_Learning_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="http://papers.nips.cc/paper/9112-reconciling-meta-learning-and-continual-learning-with-online-mixtures-of-tasks">Reconciling meta-learning and continual learning with online mixtures of tasks</a> by Ghassen Jerfel, Erin Grant, Tom Griffiths and Katherine A Heller. <em>Advances in Neural Information Processing Systems</em>, 9122–9133, 2019. <span style='background-color:Violet; padding: 2px; border-radius:4px; border: 1px solid black;'>[bayes]</span> <span style='background-color:#E5E4E2; padding: 2px; border-radius:4px; border: 1px solid black;'>[vision]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="jerfel2019aContinual_Meta_LearningFunction()" id="jerfel2019aContinual_Meta_Learning_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="jerfel2019aContinual_Meta_LearningFunction2()" id="jerfel2019aContinual_Meta_Learning_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="jerfel2019aContinual_Meta_LearningFunction3()" id="jerfel2019aContinual_Meta_Learning_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="jerfel2019aContinual_Meta_Learning_more" style="display: none">
    @inproceedings{jerfel2019a,<br> author = {Jerfel, Ghassen and Grant, Erin and Griffiths, Tom and Heller, Katherine A},<br> booktitle = {Advances in Neural Information Processing Systems},<br> keywords = {[bayes],[vision]},<br> mendeley-tags = {[bayes],[vision]},<br> pages = {9122--9133},<br> title = {Reconciling meta-learning and continual learning with online mixtures of tasks},<br> url = {http://papers.nips.cc/paper/9112-reconciling-meta-learning-and-continual-learning-with-online-mixtures-of-tasks},<br> year = {2019}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="jerfel2019aContinual_Meta_Learning_more2" style="display: none">
    Learning-to-learn or meta-learning leverages data-driven inductive bias to increase the efficiency of learning on a novel task. This approach encounters difficulty when transfer is not advantageous, for instance, when tasks are considerably dissimilar or change over time. We use the connection between gradient-based meta-learning and hierarchical Bayes to propose a Dirichlet process mixture of hierarchical Bayesian models over the parameters of an arbitrary parametric model such as a neural network. In contrast to consolidating inductive biases into a single set of hyperparameters, our approach of task-dependent hyperparameter selection better handles latent distribution shift, as demonstrated on a set of evolving, image-based, few-shot learning benchmarks.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="jerfel2019aContinual_Meta_Learning_more3" style="display: none">
    N.A.
</span></p>
<script>
    function jerfel2019aContinual_Meta_LearningFunction() {
      var moreText = document.getElementById("jerfel2019aContinual_Meta_Learning_more");
      var moreText2 = document.getElementById("jerfel2019aContinual_Meta_Learning_more2");
      var moreText3 = document.getElementById("jerfel2019aContinual_Meta_Learning_more3");
      var btnText = document.getElementById("jerfel2019aContinual_Meta_Learning_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function jerfel2019aContinual_Meta_LearningFunction2() {
      var moreText = document.getElementById("jerfel2019aContinual_Meta_Learning_more2");
      var moreText1 = document.getElementById("jerfel2019aContinual_Meta_Learning_more");
      var moreText3 = document.getElementById("jerfel2019aContinual_Meta_Learning_more3");
      var btnText = document.getElementById("jerfel2019aContinual_Meta_Learning_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function jerfel2019aContinual_Meta_LearningFunction3() {
      var moreText = document.getElementById("jerfel2019aContinual_Meta_Learning_more3");
      var moreText1 = document.getElementById("jerfel2019aContinual_Meta_Learning_more");
      var moreText2 = document.getElementById("jerfel2019aContinual_Meta_Learning_more2");
      var btnText = document.getElementById("jerfel2019aContinual_Meta_Learning_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
</ul>
</div>
<div class="section" id="continual-reinforcement-learning">
<h3>Continual Reinforcement Learning<a class="headerlink" href="#continual-reinforcement-learning" title="Permalink to this headline">¶</a></h3>
<p>In this section we list all the papers related to the continual Reinforcement Learning.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/abs/1812.07671">Deep online learning via meta-learning: Continual adaptation for model-based RL</a> by Anusha Nagabandi, Chelsea Finn and Sergey Levine. <em>7th International Conference on Learning Representations, ICLR 2019</em>, 2019.  <br>
<button style="font-size:75%; line-height:15px" onclick="nagabandi2019aContinual_Reinforcement_LearningFunction()" id="nagabandi2019aContinual_Reinforcement_Learning_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="nagabandi2019aContinual_Reinforcement_LearningFunction2()" id="nagabandi2019aContinual_Reinforcement_Learning_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="nagabandi2019aContinual_Reinforcement_LearningFunction3()" id="nagabandi2019aContinual_Reinforcement_Learning_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="nagabandi2019aContinual_Reinforcement_Learning_more" style="display: none">
    @article{nagabandi2019a,<br> archiveprefix = {arXiv},<br> arxivid = {1812.07671},<br> author = {Nagabandi, Anusha and Finn, Chelsea and Levine, Sergey},<br> eprint = {1812.07671},<br> journal = {7th International Conference on Learning Representations, ICLR 2019},<br> title = {Deep online learning via meta-learning: Continual adaptation for model-based RL},<br> url = {https://arxiv.org/abs/1812.07671},<br> year = {2019}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="nagabandi2019aContinual_Reinforcement_Learning_more2" style="display: none">
    Humans and animals can learn complex predictive models that allow them to accurately and reliably reason about real-world phenomena, and they can adapt such models extremely quickly in the face of unexpected changes. Deep neural network models allow us to represent very complex functions, but lack this capacity for rapid online adaptation. The goal in this paper is to develop a method for continual online learning from an incoming stream of data, using deep neural network models. We formulate an online learning procedure that uses stochastic gradient descent to update model parameters, and an expectation maximization algorithm with a Chinese restaurant process prior to develop and maintain a mixture of models to handle non-stationary task distributions. This allows for all models to be adapted as necessary, with new models instantiated for task changes and old models recalled when previously seen tasks are encountered again. Furthermore, we observe that meta-learning can be used to meta-train a model such that this direct online adaptation with SGD is effective, which is otherwise not the case for large function approximators. In this work, we apply our meta-learning for online learning (MOLe) approach to model-based reinforcement learning, where adapting the predictive model is critical for control; we demonstrate that MOLe outperforms alternative prior methods, and enables effective continuous adaptation in non-stationary task distributions such as varying terrains, motor failures, and unexpected disturbances. Videos available at: https://sites.google.com/Berkeley.edu/onlineviameta.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="nagabandi2019aContinual_Reinforcement_Learning_more3" style="display: none">
    N.A.
</span></p>
<script>
    function nagabandi2019aContinual_Reinforcement_LearningFunction() {
      var moreText = document.getElementById("nagabandi2019aContinual_Reinforcement_Learning_more");
      var moreText2 = document.getElementById("nagabandi2019aContinual_Reinforcement_Learning_more2");
      var moreText3 = document.getElementById("nagabandi2019aContinual_Reinforcement_Learning_more3");
      var btnText = document.getElementById("nagabandi2019aContinual_Reinforcement_Learning_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function nagabandi2019aContinual_Reinforcement_LearningFunction2() {
      var moreText = document.getElementById("nagabandi2019aContinual_Reinforcement_Learning_more2");
      var moreText1 = document.getElementById("nagabandi2019aContinual_Reinforcement_Learning_more");
      var moreText3 = document.getElementById("nagabandi2019aContinual_Reinforcement_Learning_more3");
      var btnText = document.getElementById("nagabandi2019aContinual_Reinforcement_Learning_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function nagabandi2019aContinual_Reinforcement_LearningFunction3() {
      var moreText = document.getElementById("nagabandi2019aContinual_Reinforcement_Learning_more3");
      var moreText1 = document.getElementById("nagabandi2019aContinual_Reinforcement_Learning_more");
      var moreText2 = document.getElementById("nagabandi2019aContinual_Reinforcement_Learning_more2");
      var btnText = document.getElementById("nagabandi2019aContinual_Reinforcement_Learning_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p>Learning to learn without forgetting by maximizing transfer and minimizing interference by Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu and Gerald Tesauro. <em>ICLR</em>, 2019. <span style='background-color:MediumAquaMarine; padding: 2px; border-radius:4px; border: 1px solid black;'>[mnist]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="riemer2019aContinual_Reinforcement_LearningFunction()" id="riemer2019aContinual_Reinforcement_Learning_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="riemer2019aContinual_Reinforcement_LearningFunction2()" id="riemer2019aContinual_Reinforcement_Learning_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="riemer2019aContinual_Reinforcement_LearningFunction3()" id="riemer2019aContinual_Reinforcement_Learning_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="riemer2019aContinual_Reinforcement_Learning_more" style="display: none">
    @inproceedings{riemer2019a,<br> author = {Riemer, Matthew and Cases, Ignacio and Ajemian, Robert and Liu, Miao and Rish, Irina and Tu, Yuhai and Tesauro, Gerald},<br> booktitle = {ICLR},<br> file = {:home/andrea/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Riemer et al. - 2019 - Learning to learn without forgetting by maximizing transfer and minimizing interference(2).pdf:pdf},<br> keywords = {[mnist]},<br> mendeley-tags = {[mnist]},<br> month = {sep},<br> title = {Learning to learn without forgetting by maximizing transfer and minimizing interference},<br> year = {2019}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="riemer2019aContinual_Reinforcement_Learning_more2" style="display: none">
    Lack of performance when it comes to continual learning over non-stationary distributions of data remains a major challenge in scaling neural network learning to more human realistic settings. In this work we propose a new conceptualization of the continual learning problem in terms of a temporally symmetric trade-off between transfer and interference that can be optimized by enforcing gradient alignment across examples. We then propose a new algorithm, Meta-Experience Replay (MER), that directly exploits this view by combining experience replay with optimization based meta-learning. This method learns parameters that make interference based on future gradients less likely and transfer based on future gradients more likely. 1 We conduct experiments across continual lifelong supervised learning benchmarks and non-stationary reinforcement learning environments demonstrating that our approach consistently outperforms recently proposed baselines for continual learning. Our experiments show that the gap between the performance of MER and baseline algorithms grows both as the environment gets more non-stationary and as the fraction of the total experiences stored gets smaller.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="riemer2019aContinual_Reinforcement_Learning_more3" style="display: none">
    N.A.
</span></p>
<script>
    function riemer2019aContinual_Reinforcement_LearningFunction() {
      var moreText = document.getElementById("riemer2019aContinual_Reinforcement_Learning_more");
      var moreText2 = document.getElementById("riemer2019aContinual_Reinforcement_Learning_more2");
      var moreText3 = document.getElementById("riemer2019aContinual_Reinforcement_Learning_more3");
      var btnText = document.getElementById("riemer2019aContinual_Reinforcement_Learning_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function riemer2019aContinual_Reinforcement_LearningFunction2() {
      var moreText = document.getElementById("riemer2019aContinual_Reinforcement_Learning_more2");
      var moreText1 = document.getElementById("riemer2019aContinual_Reinforcement_Learning_more");
      var moreText3 = document.getElementById("riemer2019aContinual_Reinforcement_Learning_more3");
      var btnText = document.getElementById("riemer2019aContinual_Reinforcement_Learning_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function riemer2019aContinual_Reinforcement_LearningFunction3() {
      var moreText = document.getElementById("riemer2019aContinual_Reinforcement_Learning_more3");
      var moreText1 = document.getElementById("riemer2019aContinual_Reinforcement_Learning_more");
      var moreText2 = document.getElementById("riemer2019aContinual_Reinforcement_Learning_more2");
      var btnText = document.getElementById("riemer2019aContinual_Reinforcement_Learning_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p>Policy Consolidation for Continual Reinforcement Learning by Christos Kaplanis, Murray Shanahan and Claudia Clopath. <em>ICML</em>, 2019.  <br>
<button style="font-size:75%; line-height:15px" onclick="kaplanis2019aContinual_Reinforcement_LearningFunction()" id="kaplanis2019aContinual_Reinforcement_Learning_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="kaplanis2019aContinual_Reinforcement_LearningFunction2()" id="kaplanis2019aContinual_Reinforcement_Learning_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="kaplanis2019aContinual_Reinforcement_LearningFunction3()" id="kaplanis2019aContinual_Reinforcement_Learning_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="kaplanis2019aContinual_Reinforcement_Learning_more" style="display: none">
    @inproceedings{kaplanis2019a,<br> archiveprefix = {arXiv},<br> arxivid = {1902.00255v2},<br> author = {Kaplanis, Christos and Shanahan, Murray and Clopath, Claudia},<br> booktitle = {ICML},<br> eprint = {1902.00255v2},<br> file = {::},<br> title = {Policy Consolidation for Continual Reinforcement Learning},<br> year = {2019}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="kaplanis2019aContinual_Reinforcement_Learning_more2" style="display: none">
    We propose a method for tackling catastrophic forgetting in deep reinforcement learning that is agnostic to the timescale of changes in the distribution of experiences, does not require knowledge of task boundaries, and can adapt in continuously changing environments. In our policy consolidation model, the policy network interacts with a cascade of hidden networks that simultaneously remember the agent's policy at a range of timescales and regularise the current policy by its own history, thereby improving its ability to learn without forgetting. We find that the model improves continual learning relative to baselines on a number of continuous control tasks in single-task, alternating two-task, and multi-agent competitive self-play settings.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="kaplanis2019aContinual_Reinforcement_Learning_more3" style="display: none">
    N.A.
</span></p>
<script>
    function kaplanis2019aContinual_Reinforcement_LearningFunction() {
      var moreText = document.getElementById("kaplanis2019aContinual_Reinforcement_Learning_more");
      var moreText2 = document.getElementById("kaplanis2019aContinual_Reinforcement_Learning_more2");
      var moreText3 = document.getElementById("kaplanis2019aContinual_Reinforcement_Learning_more3");
      var btnText = document.getElementById("kaplanis2019aContinual_Reinforcement_Learning_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function kaplanis2019aContinual_Reinforcement_LearningFunction2() {
      var moreText = document.getElementById("kaplanis2019aContinual_Reinforcement_Learning_more2");
      var moreText1 = document.getElementById("kaplanis2019aContinual_Reinforcement_Learning_more");
      var moreText3 = document.getElementById("kaplanis2019aContinual_Reinforcement_Learning_more3");
      var btnText = document.getElementById("kaplanis2019aContinual_Reinforcement_Learning_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function kaplanis2019aContinual_Reinforcement_LearningFunction3() {
      var moreText = document.getElementById("kaplanis2019aContinual_Reinforcement_Learning_more3");
      var moreText1 = document.getElementById("kaplanis2019aContinual_Reinforcement_Learning_more");
      var moreText2 = document.getElementById("kaplanis2019aContinual_Reinforcement_Learning_more2");
      var btnText = document.getElementById("kaplanis2019aContinual_Reinforcement_Learning_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="http://arxiv.org/abs/1911.08068">Leaky Tiling Activations: A Simple Approach to Learning Sparse Representations Online</a> by Yangchen Pan, Kirby Banman and Martha White. <em>arXiv</em>, 2019. <span style='background-color:DarkSalmon; padding: 2px; border-radius:4px; border: 1px solid black;'>[sparsity]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Pan2019aContinual_Reinforcement_LearningFunction()" id="Pan2019aContinual_Reinforcement_Learning_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Pan2019aContinual_Reinforcement_LearningFunction2()" id="Pan2019aContinual_Reinforcement_Learning_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Pan2019aContinual_Reinforcement_LearningFunction3()" id="Pan2019aContinual_Reinforcement_Learning_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Pan2019aContinual_Reinforcement_Learning_more" style="display: none">
    @article{Pan2019a,<br> archiveprefix = {arXiv},<br> arxivid = {1911.08068},<br> author = {Pan, Yangchen and Banman, Kirby and White, Martha},<br> eprint = {1911.08068},<br> journal = {arXiv},<br> keywords = {[sparsity]},<br> mendeley-tags = {[sparsity]},<br> title = {Leaky Tiling Activations: A Simple Approach to Learning Sparse Representations Online},<br> url = {http://arxiv.org/abs/1911.08068},<br> year = {2019}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Pan2019aContinual_Reinforcement_Learning_more2" style="display: none">
    Interference is a known problem when learning in online settings, such as continual learning or reinforcement learning. Interference occurs when updates, to improve performance for some inputs, degrades performance for others. Recent work has shown that sparse representations---where only a small percentage of units are active---can significantly reduce interference. Those works, however, relied on relatively complex regularization or meta-learning approaches, that have only been used offline in a pre-training phase. In our approach, we design an activation function that naturally produces sparse representations, and so is much more amenable to online training. The idea relies on the simple approach of binning, but overcomes the two key limitations of binning: zero gradients for the flat regions almost everywhere, and lost precision---reduced discrimination---due to coarse aggregation. We introduce a Leaky Tiling Activation (LTA) that provides non-negligible gradients and produces overlap between bins that improves discrimination. We empirically investigate both value-based and policy gradient reinforcement learning algorithms that use neural networks with LTAs, in classic discrete-action control environments and Mujoco continuous-action environments. We show that, with LTAs, learning is faster, with more stable policies, without needing target networks.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Pan2019aContinual_Reinforcement_Learning_more3" style="display: none">
    N.A.
</span></p>
<script>
    function Pan2019aContinual_Reinforcement_LearningFunction() {
      var moreText = document.getElementById("Pan2019aContinual_Reinforcement_Learning_more");
      var moreText2 = document.getElementById("Pan2019aContinual_Reinforcement_Learning_more2");
      var moreText3 = document.getElementById("Pan2019aContinual_Reinforcement_Learning_more3");
      var btnText = document.getElementById("Pan2019aContinual_Reinforcement_Learning_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Pan2019aContinual_Reinforcement_LearningFunction2() {
      var moreText = document.getElementById("Pan2019aContinual_Reinforcement_Learning_more2");
      var moreText1 = document.getElementById("Pan2019aContinual_Reinforcement_Learning_more");
      var moreText3 = document.getElementById("Pan2019aContinual_Reinforcement_Learning_more3");
      var btnText = document.getElementById("Pan2019aContinual_Reinforcement_Learning_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Pan2019aContinual_Reinforcement_LearningFunction3() {
      var moreText = document.getElementById("Pan2019aContinual_Reinforcement_Learning_more3");
      var moreText1 = document.getElementById("Pan2019aContinual_Reinforcement_Learning_more");
      var moreText2 = document.getElementById("Pan2019aContinual_Reinforcement_Learning_more2");
      var btnText = document.getElementById("Pan2019aContinual_Reinforcement_Learning_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="http://link.springer.com/10.1007/978-3-030-30493-5_4">Continual Learning Exploiting Structure of Fractal Reservoir Computing</a> by Taisuke Kobayashi and Toshiki Sugino. <em>Artificial Neural Networks and Machine Learning – ICANN 2019: Workshop and Special Sessions</em>, 35–47, 2019. <span style='background-color:AliceBlue; padding: 2px; border-radius:4px; border: 1px solid black;'>[rnn]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Kobayashi2019aContinual_Reinforcement_LearningFunction()" id="Kobayashi2019aContinual_Reinforcement_Learning_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Kobayashi2019aContinual_Reinforcement_LearningFunction2()" id="Kobayashi2019aContinual_Reinforcement_Learning_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Kobayashi2019aContinual_Reinforcement_LearningFunction3()" id="Kobayashi2019aContinual_Reinforcement_Learning_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Kobayashi2019aContinual_Reinforcement_Learning_more" style="display: none">
    @inproceedings{Kobayashi2019a,<br> address = {Cham},<br> annote = {A reservoir computing approach with Echo State Networks is implemented in order to learn multiple tasks in reinforcement learning environments.},<br> author = {Kobayashi, Taisuke and Sugino, Toshiki},<br> booktitle = {Artificial Neural Networks and Machine Learning – ICANN 2019: Workshop and Special Sessions},<br> doi = {10.1007/978-3-030-30493-5_4},<br> editor = {Tetko, Igor V and Kůrková, Věra and Karpov, Pavel and Theis, Fabian},<br> isbn = {978-3-030-30492-8 978-3-030-30493-5},<br> keywords = {[rnn],fractals,rc,reinforcement,reservoir computing},<br> language = {en},<br> mendeley-tags = {[rnn]},<br> pages = {35--47},<br> publisher = {Springer International Publishing},<br> title = {Continual Learning Exploiting Structure of Fractal Reservoir Computing},<br> url = {http://link.springer.com/10.1007/978-3-030-30493-5_4},<br> volume = {11731},<br> year = {2019}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Kobayashi2019aContinual_Reinforcement_Learning_more2" style="display: none">
    Neural network has a critical problem, called catastrophic forgetting, where memories for tasks already learned are easily overwritten with memories for a task additionally learned. This problem interferes with continual learning required for autonomous robots, which learn many tasks incrementally from daily activities. To mitigate the catastrophic forgetting, it is important for especially reservoir computing to clarify which neurons should be ﬁred corresponding to each task, since only readout weights are updated according to the degree of ﬁring of neurons. We therefore propose the way to design reservoir computing such that the ﬁring neurons are clearly distinguished from others according to the task to be performed. As a key design feature, we employ fractal network, which has modularity and scalability, to be reservoir layer. In particular, its modularity is fully utilized by designing input layer. As a result, simulations of control tasks using reinforcement learning show that our design mitigates the catastrophic forgetting even when random actions from reinforcement learning prompt parameters to be overwritten. Furthermore, learning multiple tasks with a single network suggests that knowledge for the other tasks can facilitate to learn a new task, unlike the case using completely diﬀerent networks.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Kobayashi2019aContinual_Reinforcement_Learning_more3" style="display: none">
    A reservoir computing approach with Echo State Networks is implemented in order to learn multiple tasks in reinforcement learning environments.
</span></p>
<script>
    function Kobayashi2019aContinual_Reinforcement_LearningFunction() {
      var moreText = document.getElementById("Kobayashi2019aContinual_Reinforcement_Learning_more");
      var moreText2 = document.getElementById("Kobayashi2019aContinual_Reinforcement_Learning_more2");
      var moreText3 = document.getElementById("Kobayashi2019aContinual_Reinforcement_Learning_more3");
      var btnText = document.getElementById("Kobayashi2019aContinual_Reinforcement_Learning_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Kobayashi2019aContinual_Reinforcement_LearningFunction2() {
      var moreText = document.getElementById("Kobayashi2019aContinual_Reinforcement_Learning_more2");
      var moreText1 = document.getElementById("Kobayashi2019aContinual_Reinforcement_Learning_more");
      var moreText3 = document.getElementById("Kobayashi2019aContinual_Reinforcement_Learning_more3");
      var btnText = document.getElementById("Kobayashi2019aContinual_Reinforcement_Learning_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Kobayashi2019aContinual_Reinforcement_LearningFunction3() {
      var moreText = document.getElementById("Kobayashi2019aContinual_Reinforcement_Learning_more3");
      var moreText1 = document.getElementById("Kobayashi2019aContinual_Reinforcement_Learning_more");
      var moreText2 = document.getElementById("Kobayashi2019aContinual_Reinforcement_Learning_more2");
      var btnText = document.getElementById("Kobayashi2019aContinual_Reinforcement_Learning_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="http://proceedings.mlr.press/v80/schwarz18a.html">Progress &amp; Compress: A scalable framework for continual learning</a> by Jonathan Schwarz, Wojciech Czarnecki, Jelena Luketina, Agnieszka Grabska-Barwinska, Yee Whye Teh, Razvan Pascanu and Raia Hadsell. <em>International Conference on Machine Learning</em>, 4528–4537, 2018. <span style='background-color:#E5E4E2; padding: 2px; border-radius:4px; border: 1px solid black;'>[vision]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Schwarz2018aContinual_Reinforcement_LearningFunction()" id="Schwarz2018aContinual_Reinforcement_Learning_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Schwarz2018aContinual_Reinforcement_LearningFunction2()" id="Schwarz2018aContinual_Reinforcement_Learning_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Schwarz2018aContinual_Reinforcement_LearningFunction3()" id="Schwarz2018aContinual_Reinforcement_Learning_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Schwarz2018aContinual_Reinforcement_Learning_more" style="display: none">
    @inproceedings{Schwarz2018a,<br> author = {Schwarz, Jonathan and Czarnecki, Wojciech and Luketina, Jelena and Grabska-Barwinska, Agnieszka and Teh, Yee Whye and Pascanu, Razvan and Hadsell, Raia},<br> booktitle = {International Conference on Machine Learning},<br> keywords = {[vision],ewc,normalized ewc,online ewc},<br> language = {en},<br> mendeley-tags = {[vision]},<br> month = {jul},<br> pages = {4528--4537},<br> shorttitle = {Progress & Compress},<br> title = {Progress & Compress: A scalable framework for continual learning},<br> url = {http://proceedings.mlr.press/v80/schwarz18a.html},<br> year = {2018}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Schwarz2018aContinual_Reinforcement_Learning_more2" style="display: none">
    We introduce a conceptually simple and scalable framework for continual learning domains where tasks are learned sequentially. Our method is constant in the number of parameters and is designed to ...
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Schwarz2018aContinual_Reinforcement_Learning_more3" style="display: none">
    N.A.
</span></p>
<script>
    function Schwarz2018aContinual_Reinforcement_LearningFunction() {
      var moreText = document.getElementById("Schwarz2018aContinual_Reinforcement_Learning_more");
      var moreText2 = document.getElementById("Schwarz2018aContinual_Reinforcement_Learning_more2");
      var moreText3 = document.getElementById("Schwarz2018aContinual_Reinforcement_Learning_more3");
      var btnText = document.getElementById("Schwarz2018aContinual_Reinforcement_Learning_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Schwarz2018aContinual_Reinforcement_LearningFunction2() {
      var moreText = document.getElementById("Schwarz2018aContinual_Reinforcement_Learning_more2");
      var moreText1 = document.getElementById("Schwarz2018aContinual_Reinforcement_Learning_more");
      var moreText3 = document.getElementById("Schwarz2018aContinual_Reinforcement_Learning_more3");
      var btnText = document.getElementById("Schwarz2018aContinual_Reinforcement_Learning_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Schwarz2018aContinual_Reinforcement_LearningFunction3() {
      var moreText = document.getElementById("Schwarz2018aContinual_Reinforcement_Learning_more3");
      var moreText1 = document.getElementById("Schwarz2018aContinual_Reinforcement_Learning_more");
      var moreText2 = document.getElementById("Schwarz2018aContinual_Reinforcement_Learning_more2");
      var btnText = document.getElementById("Schwarz2018aContinual_Reinforcement_Learning_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="http://arxiv.org/abs/1612.00796">Overcoming catastrophic forgetting in neural networks</a> by James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran and Raia Hadsell. <em>PNAS</em>, 3521–3526, 2017. <span style='background-color:MediumAquaMarine; padding: 2px; border-radius:4px; border: 1px solid black;'>[mnist]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Kirkpatrick2017aContinual_Reinforcement_LearningFunction()" id="Kirkpatrick2017aContinual_Reinforcement_Learning_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Kirkpatrick2017aContinual_Reinforcement_LearningFunction2()" id="Kirkpatrick2017aContinual_Reinforcement_Learning_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Kirkpatrick2017aContinual_Reinforcement_LearningFunction3()" id="Kirkpatrick2017aContinual_Reinforcement_Learning_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Kirkpatrick2017aContinual_Reinforcement_Learning_more" style="display: none">
    @article{Kirkpatrick2017a,<br> annote = {arXiv: 1612.00796},<br> author = {Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and Hassabis, Demis and Clopath, Claudia and Kumaran, Dharshan and Hadsell, Raia},<br> journal = {PNAS},<br> keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning,[mnist],annotated,ewc},<br> mendeley-tags = {[mnist]},<br> number = {13},<br> pages = {3521--3526},<br> title = {Overcoming catastrophic forgetting in neural networks},<br> url = {http://arxiv.org/abs/1612.00796},<br> volume = {114},<br> year = {2017}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Kirkpatrick2017aContinual_Reinforcement_Learning_more2" style="display: none">
    The ability to learn tasks in a sequential fashion is crucial to the development of artificial intelligence. Neural networks are not, in general, capable of this and it has been widely thought that catastrophic forgetting is an inevitable feature of connectionist models. We show that it is possible to overcome this limitation and train networks that can maintain expertise on tasks which they have not experienced for a long time. Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks. We demonstrate our approach is scalable and effective by solving a set of classification tasks based on the MNIST hand written digit dataset and by learning several Atari 2600 games sequentially.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Kirkpatrick2017aContinual_Reinforcement_Learning_more3" style="display: none">
    arXiv: 1612.00796
</span></p>
<script>
    function Kirkpatrick2017aContinual_Reinforcement_LearningFunction() {
      var moreText = document.getElementById("Kirkpatrick2017aContinual_Reinforcement_Learning_more");
      var moreText2 = document.getElementById("Kirkpatrick2017aContinual_Reinforcement_Learning_more2");
      var moreText3 = document.getElementById("Kirkpatrick2017aContinual_Reinforcement_Learning_more3");
      var btnText = document.getElementById("Kirkpatrick2017aContinual_Reinforcement_Learning_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Kirkpatrick2017aContinual_Reinforcement_LearningFunction2() {
      var moreText = document.getElementById("Kirkpatrick2017aContinual_Reinforcement_Learning_more2");
      var moreText1 = document.getElementById("Kirkpatrick2017aContinual_Reinforcement_Learning_more");
      var moreText3 = document.getElementById("Kirkpatrick2017aContinual_Reinforcement_Learning_more3");
      var btnText = document.getElementById("Kirkpatrick2017aContinual_Reinforcement_Learning_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Kirkpatrick2017aContinual_Reinforcement_LearningFunction3() {
      var moreText = document.getElementById("Kirkpatrick2017aContinual_Reinforcement_Learning_more3");
      var moreText1 = document.getElementById("Kirkpatrick2017aContinual_Reinforcement_Learning_more");
      var moreText2 = document.getElementById("Kirkpatrick2017aContinual_Reinforcement_Learning_more2");
      var btnText = document.getElementById("Kirkpatrick2017aContinual_Reinforcement_Learning_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="https://sites.ualberta.ca/$∼$amw8/cldl.pdf">Stable predictive representations with general value functions for continual learning</a> by Matthew Schlegel, Adam White and Martha White. <em>Continual Learning and Deep Networks workshop at the Neural Information Processing System Conference</em>, 2017.  <br>
<button style="font-size:75%; line-height:15px" onclick="schlegel2017aContinual_Reinforcement_LearningFunction()" id="schlegel2017aContinual_Reinforcement_Learning_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="schlegel2017aContinual_Reinforcement_LearningFunction2()" id="schlegel2017aContinual_Reinforcement_Learning_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="schlegel2017aContinual_Reinforcement_LearningFunction3()" id="schlegel2017aContinual_Reinforcement_Learning_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="schlegel2017aContinual_Reinforcement_Learning_more" style="display: none">
    @inproceedings{schlegel2017a,<br> author = {Schlegel, Matthew and White, Adam and White, Martha},<br> booktitle = {Continual Learning and Deep Networks workshop at the Neural Information Processing System Conference},<br> title = {Stable predictive representations with general value functions for continual learning},<br> url = {https://sites.ualberta.ca/$∼$amw8/cldl.pdf},<br> year = {2017}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="schlegel2017aContinual_Reinforcement_Learning_more2" style="display: none">
    The objective of continual learning is to build agents that continually learn about their world, building on prior learning. In this paper, we explore an approach to continual learning based on making and updating many predictions formalized as general value functions (GVFs). The idea behind GVFs is simple: if we can cast the task of representing predictive knowledge as a prediction of future reward, then computationally efficient policy evaluation methods from reinforcement learning can be used to learn a large collection of predictions while the agent interacts with the world. We explore this idea further by analyzing how GVF predictions can be used as predictive features, and introduce two algorithmic techniques to ensure the stability of continual prediction learning. We illustrate these ideas with a small experiment in the cycle world domain.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="schlegel2017aContinual_Reinforcement_Learning_more3" style="display: none">
    N.A.
</span></p>
<script>
    function schlegel2017aContinual_Reinforcement_LearningFunction() {
      var moreText = document.getElementById("schlegel2017aContinual_Reinforcement_Learning_more");
      var moreText2 = document.getElementById("schlegel2017aContinual_Reinforcement_Learning_more2");
      var moreText3 = document.getElementById("schlegel2017aContinual_Reinforcement_Learning_more3");
      var btnText = document.getElementById("schlegel2017aContinual_Reinforcement_Learning_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function schlegel2017aContinual_Reinforcement_LearningFunction2() {
      var moreText = document.getElementById("schlegel2017aContinual_Reinforcement_Learning_more2");
      var moreText1 = document.getElementById("schlegel2017aContinual_Reinforcement_Learning_more");
      var moreText3 = document.getElementById("schlegel2017aContinual_Reinforcement_Learning_more3");
      var btnText = document.getElementById("schlegel2017aContinual_Reinforcement_Learning_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function schlegel2017aContinual_Reinforcement_LearningFunction3() {
      var moreText = document.getElementById("schlegel2017aContinual_Reinforcement_Learning_more3");
      var moreText1 = document.getElementById("schlegel2017aContinual_Reinforcement_Learning_more");
      var moreText2 = document.getElementById("schlegel2017aContinual_Reinforcement_Learning_more2");
      var btnText = document.getElementById("schlegel2017aContinual_Reinforcement_Learning_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="https://core.ac.uk/reader/84859350">Continual learning through evolvable neural turing machines</a> by Benno Luders, Mikkel Schlager and Sebastia Risi. <em>NIPS 2016 Workshop on Continual Learning and Deep Networks</em>, 2016.  <br>
<button style="font-size:75%; line-height:15px" onclick="Luders2016aContinual_Reinforcement_LearningFunction()" id="Luders2016aContinual_Reinforcement_Learning_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Luders2016aContinual_Reinforcement_LearningFunction2()" id="Luders2016aContinual_Reinforcement_Learning_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Luders2016aContinual_Reinforcement_LearningFunction3()" id="Luders2016aContinual_Reinforcement_Learning_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Luders2016aContinual_Reinforcement_Learning_more" style="display: none">
    @inproceedings{Luders2016a,<br> author = {Luders, Benno and Schlager, Mikkel and Risi, Sebastia},<br> booktitle = {NIPS 2016 Workshop on Continual Learning and Deep Networks},<br> title = {Continual learning through evolvable neural turing machines},<br> url = {https://core.ac.uk/reader/84859350},<br> year = {2016}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Luders2016aContinual_Reinforcement_Learning_more2" style="display: none">
    Continual learning, i.e. the ability to sequentially learn tasks without catastrophicforgetting of previously learned ones, is an important open challenge in machinelearning. In this paper we take a step in this direction by showing that the recentlyproposedEvolving Neural Turing Machine(ENTM) approach is able to performone-shot learningin a reinforcement learning task without catastrophic forgettingof previously stored associations.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Luders2016aContinual_Reinforcement_Learning_more3" style="display: none">
    N.A.
</span></p>
<script>
    function Luders2016aContinual_Reinforcement_LearningFunction() {
      var moreText = document.getElementById("Luders2016aContinual_Reinforcement_Learning_more");
      var moreText2 = document.getElementById("Luders2016aContinual_Reinforcement_Learning_more2");
      var moreText3 = document.getElementById("Luders2016aContinual_Reinforcement_Learning_more3");
      var btnText = document.getElementById("Luders2016aContinual_Reinforcement_Learning_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Luders2016aContinual_Reinforcement_LearningFunction2() {
      var moreText = document.getElementById("Luders2016aContinual_Reinforcement_Learning_more2");
      var moreText1 = document.getElementById("Luders2016aContinual_Reinforcement_Learning_more");
      var moreText3 = document.getElementById("Luders2016aContinual_Reinforcement_Learning_more3");
      var btnText = document.getElementById("Luders2016aContinual_Reinforcement_Learning_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Luders2016aContinual_Reinforcement_LearningFunction3() {
      var moreText = document.getElementById("Luders2016aContinual_Reinforcement_Learning_more3");
      var moreText1 = document.getElementById("Luders2016aContinual_Reinforcement_Learning_more");
      var moreText2 = document.getElementById("Luders2016aContinual_Reinforcement_Learning_more2");
      var btnText = document.getElementById("Luders2016aContinual_Reinforcement_Learning_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="http://arxiv.org/abs/1606.04671">Progressive Neural Networks</a> by Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu and Raia Hadsell. <em>arXiv</em>, 2016. <span style='background-color:MediumAquaMarine; padding: 2px; border-radius:4px; border: 1px solid black;'>[mnist]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Rusu2016aContinual_Reinforcement_LearningFunction()" id="Rusu2016aContinual_Reinforcement_Learning_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Rusu2016aContinual_Reinforcement_LearningFunction2()" id="Rusu2016aContinual_Reinforcement_Learning_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Rusu2016aContinual_Reinforcement_LearningFunction3()" id="Rusu2016aContinual_Reinforcement_Learning_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Rusu2016aContinual_Reinforcement_Learning_more" style="display: none">
    @article{Rusu2016a,<br> annote = {The authors rely on a separate feedforward network (column) for each task the model is trained on. Each column is connected through adaptive connections to all the previous ones. The weights of previous columns are frozen once trained. At inference time, given a known task label, the network choose the appropriate column to produce the output, thus preventing forgetting by design.},<br> author = {Rusu, Andrei A and Rabinowitz, Neil C and Desjardins, Guillaume and Soyer, Hubert and Kirkpatrick, James and Kavukcuoglu, Koray and Pascanu, Razvan and Hadsell, Raia},<br> journal = {arXiv},<br> keywords = {Computer Science - Machine Learning,[mnist],lifelong learning,modular,progressive},<br> language = {en},<br> mendeley-tags = {[mnist]},<br> month = {jun},<br> title = {Progressive Neural Networks},<br> url = {http://arxiv.org/abs/1606.04671},<br> year = {2016}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Rusu2016aContinual_Reinforcement_Learning_more2" style="display: none">
    Learning to solve complex sequences of tasks—while both leveraging transfer and avoiding catastrophic forgetting—remains a key obstacle to achieving human-level intelligence. The progressive networks approach represents a step forward in this direction: they are immune to forgetting and can leverage prior knowledge via lateral connections to previously learned features. We evaluate this architecture extensively on a wide variety of reinforcement learning tasks (Atari and 3D maze games), and show that it outperforms common baselines based on pretraining and ﬁnetuning. Using a novel sensitivity measure, we demonstrate that transfer occurs at both low-level sensory and high-level control layers of the learned policy.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Rusu2016aContinual_Reinforcement_Learning_more3" style="display: none">
    The authors rely on a separate feedforward network (column) for each task the model is trained on. Each column is connected through adaptive connections to all the previous ones. The weights of previous columns are frozen once trained. At inference time, given a known task label, the network choose the appropriate column to produce the output, thus preventing forgetting by design.
</span></p>
<script>
    function Rusu2016aContinual_Reinforcement_LearningFunction() {
      var moreText = document.getElementById("Rusu2016aContinual_Reinforcement_Learning_more");
      var moreText2 = document.getElementById("Rusu2016aContinual_Reinforcement_Learning_more2");
      var moreText3 = document.getElementById("Rusu2016aContinual_Reinforcement_Learning_more3");
      var btnText = document.getElementById("Rusu2016aContinual_Reinforcement_Learning_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Rusu2016aContinual_Reinforcement_LearningFunction2() {
      var moreText = document.getElementById("Rusu2016aContinual_Reinforcement_Learning_more2");
      var moreText1 = document.getElementById("Rusu2016aContinual_Reinforcement_Learning_more");
      var moreText3 = document.getElementById("Rusu2016aContinual_Reinforcement_Learning_more3");
      var btnText = document.getElementById("Rusu2016aContinual_Reinforcement_Learning_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Rusu2016aContinual_Reinforcement_LearningFunction3() {
      var moreText = document.getElementById("Rusu2016aContinual_Reinforcement_Learning_more3");
      var moreText1 = document.getElementById("Rusu2016aContinual_Reinforcement_Learning_more");
      var moreText2 = document.getElementById("Rusu2016aContinual_Reinforcement_Learning_more2");
      var btnText = document.getElementById("Rusu2016aContinual_Reinforcement_Learning_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="http://www.ncbi.nlm.nih.gov/pubmed/29756130http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5947972">Lifelong-RL: Lifelong Relaxation Labeling for Separating Entities and Aspects in Opinion Targets.</a> by Lei Shu, Bing Liu, Hu Xu and Annice Kim. <em>Proceedings of the Conference on Empirical Methods in Natural Language Processing. Conference on Empirical Methods in Natural Language Processing</em>, 225–235, 2016. <span style='background-color:BlueViolet; padding: 2px; border-radius:4px; border: 1px solid black;'>[nlp]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Civelek2014aContinual_Reinforcement_LearningFunction()" id="Civelek2014aContinual_Reinforcement_Learning_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Civelek2014aContinual_Reinforcement_LearningFunction2()" id="Civelek2014aContinual_Reinforcement_Learning_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Civelek2014aContinual_Reinforcement_LearningFunction3()" id="Civelek2014aContinual_Reinforcement_Learning_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Civelek2014aContinual_Reinforcement_Learning_more" style="display: none">
    @article{Civelek2014a,<br> archiveprefix = {arXiv},<br> arxivid = {15334406},<br> author = {Shu, Lei and Liu, Bing and Xu, Hu and Kim, Annice},<br> doi = {10.1038/nrg3575.Systems},<br> eprint = {15334406},<br> isbn = {9781493973712},<br> issn = {1527-5418},<br> journal = {Proceedings of the Conference on Empirical Methods in Natural Language Processing. Conference on Empirical Methods in Natural Language Processing},<br> keywords = {[nlp]},<br> mendeley-tags = {[nlp]},<br> month = {nov},<br> number = {1},<br> pages = {225--235},<br> pmid = {29756130},<br> title = {Lifelong-RL: Lifelong Relaxation Labeling for Separating Entities and Aspects in Opinion Targets.},<br> url = {http://www.ncbi.nlm.nih.gov/pubmed/29756130 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5947972},<br> volume = {2016},<br> year = {2016}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Civelek2014aContinual_Reinforcement_Learning_more2" style="display: none">
    It is well-known that opinions have targets. Extracting such targets is an important problem of opinion mining because without knowing the target of an opinion, the opinion is of limited use. So far many algorithms have been proposed to extract opinion targets. However, an opinion target can be an entity or an aspect (part or attribute) of an entity. An opinion about an entity is an opinion about the entity as a whole, while an opinion about an aspect is just an opinion about that specific attribute or aspect of an entity. Thus, opinion targets should be separated into entities and aspects before use because they represent very different things about opinions. This paper proposes a novel algorithm, called Lifelong-RL, to solve the problem based on lifelong machine learning and relaxation labeling. Extensive experiments show that the proposed algorithm Lifelong-RL outperforms baseline methods markedly.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Civelek2014aContinual_Reinforcement_Learning_more3" style="display: none">
    N.A.
</span></p>
<script>
    function Civelek2014aContinual_Reinforcement_LearningFunction() {
      var moreText = document.getElementById("Civelek2014aContinual_Reinforcement_Learning_more");
      var moreText2 = document.getElementById("Civelek2014aContinual_Reinforcement_Learning_more2");
      var moreText3 = document.getElementById("Civelek2014aContinual_Reinforcement_Learning_more3");
      var btnText = document.getElementById("Civelek2014aContinual_Reinforcement_Learning_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Civelek2014aContinual_Reinforcement_LearningFunction2() {
      var moreText = document.getElementById("Civelek2014aContinual_Reinforcement_Learning_more2");
      var moreText1 = document.getElementById("Civelek2014aContinual_Reinforcement_Learning_more");
      var moreText3 = document.getElementById("Civelek2014aContinual_Reinforcement_Learning_more3");
      var btnText = document.getElementById("Civelek2014aContinual_Reinforcement_Learning_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Civelek2014aContinual_Reinforcement_LearningFunction3() {
      var moreText = document.getElementById("Civelek2014aContinual_Reinforcement_Learning_more3");
      var moreText1 = document.getElementById("Civelek2014aContinual_Reinforcement_Learning_more");
      var moreText2 = document.getElementById("Civelek2014aContinual_Reinforcement_Learning_more2");
      var btnText = document.getElementById("Civelek2014aContinual_Reinforcement_Learning_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="https://doi.org/10.1023/A:1007331723572">CHILD: A First Step Towards Continual Learning</a> by  and Mark B Ring. <em>Machine Learning</em>, 77–104, 1997. <span style='background-color:AliceBlue; padding: 2px; border-radius:4px; border: 1px solid black;'>[rnn]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Ring1997aContinual_Reinforcement_LearningFunction()" id="Ring1997aContinual_Reinforcement_Learning_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Ring1997aContinual_Reinforcement_LearningFunction2()" id="Ring1997aContinual_Reinforcement_Learning_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Ring1997aContinual_Reinforcement_LearningFunction3()" id="Ring1997aContinual_Reinforcement_Learning_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Ring1997aContinual_Reinforcement_Learning_more" style="display: none">
    @article{Ring1997a,<br> author = {Ring, Mark B},<br> doi = {10.1023/A:1007331723572},<br> issn = {1573-0565},<br> journal = {Machine Learning},<br> keywords = {Continual learning,[rnn],cl,continual learner,definition,hierarchical neural networks,reinforcement learning,sequence learning,transfer},<br> language = {en},<br> mendeley-tags = {[rnn]},<br> month = {jul},<br> number = {1},<br> pages = {77--104},<br> shorttitle = {CHILD},<br> title = {CHILD: A First Step Towards Continual Learning},<br> url = {https://doi.org/10.1023/A:1007331723572},<br> volume = {28},<br> year = {1997}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Ring1997aContinual_Reinforcement_Learning_more2" style="display: none">
    Continual learning is the constant development of increasingly complex behaviors; the process of building more complicated skills on top of those already developed. A continual-learning agent should therefore learn incrementally and hierarchically. This paper describes CHILD, an agent capable of Continual, Hierarchical, Incremental Learning and Development. CHILD can quickly solve complicated non-Markovian reinforcement-learning tasks and can then transfer its skills to similar but even more complicated tasks, learning these faster still.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Ring1997aContinual_Reinforcement_Learning_more3" style="display: none">
    N.A.
</span></p>
<script>
    function Ring1997aContinual_Reinforcement_LearningFunction() {
      var moreText = document.getElementById("Ring1997aContinual_Reinforcement_Learning_more");
      var moreText2 = document.getElementById("Ring1997aContinual_Reinforcement_Learning_more2");
      var moreText3 = document.getElementById("Ring1997aContinual_Reinforcement_Learning_more3");
      var btnText = document.getElementById("Ring1997aContinual_Reinforcement_Learning_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Ring1997aContinual_Reinforcement_LearningFunction2() {
      var moreText = document.getElementById("Ring1997aContinual_Reinforcement_Learning_more2");
      var moreText1 = document.getElementById("Ring1997aContinual_Reinforcement_Learning_more");
      var moreText3 = document.getElementById("Ring1997aContinual_Reinforcement_Learning_more3");
      var btnText = document.getElementById("Ring1997aContinual_Reinforcement_Learning_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Ring1997aContinual_Reinforcement_LearningFunction3() {
      var moreText = document.getElementById("Ring1997aContinual_Reinforcement_Learning_more3");
      var moreText1 = document.getElementById("Ring1997aContinual_Reinforcement_Learning_more");
      var moreText2 = document.getElementById("Ring1997aContinual_Reinforcement_Learning_more2");
      var btnText = document.getElementById("Ring1997aContinual_Reinforcement_Learning_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
</ul>
</div>
<div class="section" id="continual-sequential-learning">
<h3>Continual Sequential Learning<a class="headerlink" href="#continual-sequential-learning" title="Permalink to this headline">¶</a></h3>
<p>Here we maintain a list of all the papers related to the continual learning at the intersection with sequential learning.</p>
<ul class="simple">
<li><p><a class="reference external" href="http://arxiv.org/abs/1909.01520">Lifelong Machine Learning with Deep Streaming Linear Discriminant Analysis</a> by Tyler L Hayes and Christopher Kanan. <em>CLVision Workshop at CVPR 2020</em>, 1–15, 2020. <span style='background-color:Chartreuse; padding: 2px; border-radius:4px; border: 1px solid black;'>[core50]</span> <span style='background-color:DeepPink; padding: 2px; border-radius:4px; border: 1px solid black;'>[imagenet]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Hayes2019bContinual_Sequential_LearningFunction()" id="Hayes2019bContinual_Sequential_Learning_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Hayes2019bContinual_Sequential_LearningFunction2()" id="Hayes2019bContinual_Sequential_Learning_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Hayes2019bContinual_Sequential_LearningFunction3()" id="Hayes2019bContinual_Sequential_Learning_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Hayes2019bContinual_Sequential_Learning_more" style="display: none">
    @article{Hayes2019b,<br> archiveprefix = {arXiv},<br> arxivid = {1909.01520},<br> author = {Hayes, Tyler L and Kanan, Christopher},<br> eprint = {1909.01520},<br> journal = {CLVision Workshop at CVPR 2020},<br> keywords = {[core50],[imagenet],deep learning,streaming learning},<br> mendeley-tags = {[core50],[imagenet]},<br> pages = {1--15},<br> title = {Lifelong Machine Learning with Deep Streaming Linear Discriminant Analysis},<br> url = {http://arxiv.org/abs/1909.01520},<br> year = {2020}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Hayes2019bContinual_Sequential_Learning_more2" style="display: none">
    When a robot acquires new information, ideally it would immediately be capable of using that information to understand its environment. While deep neural networks are now widely used by robots for inferring semantic information, conventional neural networks suffer from catastrophic forgetting when they are incrementally updated, with new knowledge overwriting established representations. While a variety of approaches have been developed that attempt to mitigate catastrophic forgetting in the incremental batch learning scenario, in which an agent learns a large collection of labeled samples at once, streaming learning has been much less studied in the robotics and deep learning communities. In streaming learning, an agent learns instances one-by-one and can be tested at any time. Here, we revisit streaming linear discriminant analysis, which has been widely used in the data mining research community. By combining streaming linear discriminant analysis with deep learning, we are able to outperform both incremental batch learning and streaming learning algorithms on both ImageNet-1K and CORe50.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Hayes2019bContinual_Sequential_Learning_more3" style="display: none">
    N.A.
</span></p>
<script>
    function Hayes2019bContinual_Sequential_LearningFunction() {
      var moreText = document.getElementById("Hayes2019bContinual_Sequential_Learning_more");
      var moreText2 = document.getElementById("Hayes2019bContinual_Sequential_Learning_more2");
      var moreText3 = document.getElementById("Hayes2019bContinual_Sequential_Learning_more3");
      var btnText = document.getElementById("Hayes2019bContinual_Sequential_Learning_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Hayes2019bContinual_Sequential_LearningFunction2() {
      var moreText = document.getElementById("Hayes2019bContinual_Sequential_Learning_more2");
      var moreText1 = document.getElementById("Hayes2019bContinual_Sequential_Learning_more");
      var moreText3 = document.getElementById("Hayes2019bContinual_Sequential_Learning_more3");
      var btnText = document.getElementById("Hayes2019bContinual_Sequential_Learning_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Hayes2019bContinual_Sequential_LearningFunction3() {
      var moreText = document.getElementById("Hayes2019bContinual_Sequential_Learning_more3");
      var moreText1 = document.getElementById("Hayes2019bContinual_Sequential_Learning_more");
      var moreText2 = document.getElementById("Hayes2019bContinual_Sequential_Learning_more2");
      var btnText = document.getElementById("Hayes2019bContinual_Sequential_Learning_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="http://arxiv.org/abs/2003.09114">Online Continual Learning on Sequences</a> by German I Parisi and Vincenzo Lomonaco. <em>arXiv</em>, 2020. <span style='background-color:Tomato; padding: 2px; border-radius:4px; border: 1px solid black;'>[framework]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Parisi2020aContinual_Sequential_LearningFunction()" id="Parisi2020aContinual_Sequential_Learning_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Parisi2020aContinual_Sequential_LearningFunction2()" id="Parisi2020aContinual_Sequential_Learning_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Parisi2020aContinual_Sequential_LearningFunction3()" id="Parisi2020aContinual_Sequential_Learning_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Parisi2020aContinual_Sequential_Learning_more" style="display: none">
    @article{Parisi2020a,<br> annote = {Comment: L. Oneto et al. (eds.), Recent Trends in Learning From Data, Studies in Computational Intelligence 896<br>arXiv: 2003.09114},<br> author = {Parisi, German I and Lomonaco, Vincenzo},<br> doi = {10.1007/978-3-030-43883-8_8},<br> journal = {arXiv},<br> keywords = {Computer Science - Computer Vision and Pattern Rec,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computi,[framework]},<br> mendeley-tags = {[framework]},<br> month = {mar},<br> title = {Online Continual Learning on Sequences},<br> url = {http://arxiv.org/abs/2003.09114},<br> year = {2020}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Parisi2020aContinual_Sequential_Learning_more2" style="display: none">
    Online continual learning (OCL) refers to the ability of a system to learn over time from a continuous stream of data without having to revisit previously encountered training samples. Learning continually in a single data pass is crucial for agents and robots operating in changing environments and required to acquire, fine-tune, and transfer increasingly complex representations from non-i.i.d. input distributions. Machine learning models that address OCL must alleviate \textitcatastrophic forgetting in which hidden representations are disrupted or completely overwritten when learning from streams of novel input. In this chapter, we summarize and discuss recent deep learning models that address OCL on sequential input through the use (and combination) of synaptic regularization, structural plasticity, and experience replay. Different implementations of replay have been proposed that alleviate catastrophic forgetting in connectionists architectures via the re-occurrence of (latent representations of) input sequences and that functionally resemble mechanisms of hippocampal replay in the mammalian brain. Empirical evidence shows that architectures endowed with experience replay typically outperform architectures without in (online) incremental learning tasks.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Parisi2020aContinual_Sequential_Learning_more3" style="display: none">
    Comment: L. Oneto et al. (eds.), Recent Trends in Learning From Data, Studies in Computational Intelligence 896
            arXiv: 2003.09114
</span></p>
<script>
    function Parisi2020aContinual_Sequential_LearningFunction() {
      var moreText = document.getElementById("Parisi2020aContinual_Sequential_Learning_more");
      var moreText2 = document.getElementById("Parisi2020aContinual_Sequential_Learning_more2");
      var moreText3 = document.getElementById("Parisi2020aContinual_Sequential_Learning_more3");
      var btnText = document.getElementById("Parisi2020aContinual_Sequential_Learning_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Parisi2020aContinual_Sequential_LearningFunction2() {
      var moreText = document.getElementById("Parisi2020aContinual_Sequential_Learning_more2");
      var moreText1 = document.getElementById("Parisi2020aContinual_Sequential_Learning_more");
      var moreText3 = document.getElementById("Parisi2020aContinual_Sequential_Learning_more3");
      var btnText = document.getElementById("Parisi2020aContinual_Sequential_Learning_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Parisi2020aContinual_Sequential_LearningFunction3() {
      var moreText = document.getElementById("Parisi2020aContinual_Sequential_Learning_more3");
      var moreText1 = document.getElementById("Parisi2020aContinual_Sequential_Learning_more");
      var moreText2 = document.getElementById("Parisi2020aContinual_Sequential_Learning_more2");
      var btnText = document.getElementById("Parisi2020aContinual_Sequential_Learning_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="http://arxiv.org/abs/1812.00420">Efficient Lifelong Learning with A-GEM</a> by Arslan Chaudhry, Marc’Aurelio Ranzato, Marcus Rohrbach and Mohamed Elhoseiny. <em>ICLR</em>, 2019. <span style='background-color:DarkCyan; padding: 2px; border-radius:4px; border: 1px solid black;'>[cifar]</span> <span style='background-color:MediumAquaMarine; padding: 2px; border-radius:4px; border: 1px solid black;'>[mnist]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Chaudhry2019aContinual_Sequential_LearningFunction()" id="Chaudhry2019aContinual_Sequential_Learning_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Chaudhry2019aContinual_Sequential_LearningFunction2()" id="Chaudhry2019aContinual_Sequential_Learning_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Chaudhry2019aContinual_Sequential_LearningFunction3()" id="Chaudhry2019aContinual_Sequential_Learning_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Chaudhry2019aContinual_Sequential_Learning_more" style="display: none">
    @inproceedings{Chaudhry2019a,<br> annote = {Comment: Published as a conference paper at ICLR 2019<br>arXiv: 1812.00420},<br> author = {Chaudhry, Arslan and Ranzato, Marc'Aurelio and Rohrbach, Marcus and Elhoseiny, Mohamed},<br> booktitle = {ICLR},<br> keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,[cifar],[mnist]},<br> language = {en},<br> mendeley-tags = {[cifar],[mnist]},<br> title = {Efficient Lifelong Learning with A-GEM},<br> url = {http://arxiv.org/abs/1812.00420},<br> year = {2019}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Chaudhry2019aContinual_Sequential_Learning_more2" style="display: none">
    In lifelong learning, the learner is presented with a sequence of tasks, incrementally building a data-driven prior which may be leveraged to speed up learning of a new task. In this work, we investigate the efficiency of current lifelong approaches, in terms of sample complexity, computational and memory cost. Towards this end, we first introduce a new and a more realistic evaluation protocol, whereby learners observe each example only once and hyper-parameter selection is done on a small and disjoint set of tasks, which is not used for the actual learning experience and evaluation. Second, we introduce a new metric measuring how quickly a learner acquires a new skill. Third, we propose an improved version of GEM (Lopez-Paz & Ranzato, 2017), dubbed Averaged GEM (A-GEM), which enjoys the same or even better performance as GEM, while being almost as computationally and memory efficient as EWC (Kirkpatrick et al., 2016) and other regularization-based methods. Finally, we show that all algorithms including A-GEM can learn even more quickly if they are provided with task descriptors specifying the classification tasks under consideration. Our experiments on several standard lifelong learning benchmarks demonstrate that A-GEM has the best trade-off between accuracy and efficiency.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Chaudhry2019aContinual_Sequential_Learning_more3" style="display: none">
    Comment: Published as a conference paper at ICLR 2019
            arXiv: 1812.00420
</span></p>
<script>
    function Chaudhry2019aContinual_Sequential_LearningFunction() {
      var moreText = document.getElementById("Chaudhry2019aContinual_Sequential_Learning_more");
      var moreText2 = document.getElementById("Chaudhry2019aContinual_Sequential_Learning_more2");
      var moreText3 = document.getElementById("Chaudhry2019aContinual_Sequential_Learning_more3");
      var btnText = document.getElementById("Chaudhry2019aContinual_Sequential_Learning_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Chaudhry2019aContinual_Sequential_LearningFunction2() {
      var moreText = document.getElementById("Chaudhry2019aContinual_Sequential_Learning_more2");
      var moreText1 = document.getElementById("Chaudhry2019aContinual_Sequential_Learning_more");
      var moreText3 = document.getElementById("Chaudhry2019aContinual_Sequential_Learning_more3");
      var btnText = document.getElementById("Chaudhry2019aContinual_Sequential_Learning_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Chaudhry2019aContinual_Sequential_LearningFunction3() {
      var moreText = document.getElementById("Chaudhry2019aContinual_Sequential_Learning_more3");
      var moreText1 = document.getElementById("Chaudhry2019aContinual_Sequential_Learning_more");
      var moreText2 = document.getElementById("Chaudhry2019aContinual_Sequential_Learning_more2");
      var btnText = document.getElementById("Chaudhry2019aContinual_Sequential_Learning_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p>Task-Free Continual Learning by Rahaf Aljundi, Klaas Kelchtermans and Tinne Tuytelaars. <em>The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2019. <span style='background-color:#E5E4E2; padding: 2px; border-radius:4px; border: 1px solid black;'>[vision]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Aljundi2019Continual_Sequential_LearningFunction()" id="Aljundi2019Continual_Sequential_Learning_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Aljundi2019Continual_Sequential_LearningFunction2()" id="Aljundi2019Continual_Sequential_Learning_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Aljundi2019Continual_Sequential_LearningFunction3()" id="Aljundi2019Continual_Sequential_Learning_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Aljundi2019Continual_Sequential_Learning_more" style="display: none">
    @inproceedings{Aljundi2019,<br> author = {Aljundi, Rahaf and Kelchtermans, Klaas and Tuytelaars, Tinne},<br> booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},<br> keywords = {[vision]},<br> mendeley-tags = {[vision]},<br> month = {jun},<br> publisher = {Aljundi2019b},<br> title = {Task-Free Continual Learning},<br> year = {2019}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Aljundi2019Continual_Sequential_Learning_more2" style="display: none">
    N.A.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Aljundi2019Continual_Sequential_Learning_more3" style="display: none">
    N.A.
</span></p>
<script>
    function Aljundi2019Continual_Sequential_LearningFunction() {
      var moreText = document.getElementById("Aljundi2019Continual_Sequential_Learning_more");
      var moreText2 = document.getElementById("Aljundi2019Continual_Sequential_Learning_more2");
      var moreText3 = document.getElementById("Aljundi2019Continual_Sequential_Learning_more3");
      var btnText = document.getElementById("Aljundi2019Continual_Sequential_Learning_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Aljundi2019Continual_Sequential_LearningFunction2() {
      var moreText = document.getElementById("Aljundi2019Continual_Sequential_Learning_more2");
      var moreText1 = document.getElementById("Aljundi2019Continual_Sequential_Learning_more");
      var moreText3 = document.getElementById("Aljundi2019Continual_Sequential_Learning_more3");
      var btnText = document.getElementById("Aljundi2019Continual_Sequential_Learning_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Aljundi2019Continual_Sequential_LearningFunction3() {
      var moreText = document.getElementById("Aljundi2019Continual_Sequential_Learning_more3");
      var moreText1 = document.getElementById("Aljundi2019Continual_Sequential_Learning_more");
      var moreText2 = document.getElementById("Aljundi2019Continual_Sequential_Learning_more2");
      var btnText = document.getElementById("Aljundi2019Continual_Sequential_Learning_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="http://papers.nips.cc/paper/9357-online-continual-learning-with-maximal-interfered-retrieval.pdf">Online Continual Learning with Maximal Interfered Retrieval</a> by Rahaf Aljundi, Eugene Belilovsky, Tinne Tuytelaars, Laurent Charlin, Massimo Caccia, Min Lin and Lucas Page-Caccia. <em>Advances in Neural Information Processing Systems 32</em>, 11849–11860, 2019. <span style='background-color:DarkCyan; padding: 2px; border-radius:4px; border: 1px solid black;'>[cifar]</span> <span style='background-color:MediumAquaMarine; padding: 2px; border-radius:4px; border: 1px solid black;'>[mnist]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Aljundi2019aContinual_Sequential_LearningFunction()" id="Aljundi2019aContinual_Sequential_Learning_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Aljundi2019aContinual_Sequential_LearningFunction2()" id="Aljundi2019aContinual_Sequential_Learning_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Aljundi2019aContinual_Sequential_LearningFunction3()" id="Aljundi2019aContinual_Sequential_Learning_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Aljundi2019aContinual_Sequential_Learning_more" style="display: none">
    @inproceedings{Aljundi2019a,<br> author = {Aljundi, Rahaf and Belilovsky, Eugene and Tuytelaars, Tinne and Charlin, Laurent and Caccia, Massimo and Lin, Min and Page-Caccia, Lucas},<br> booktitle = {Advances in Neural Information Processing Systems 32},<br> editor = {Wallach, H and Larochelle, H and Beygelzimer, A and d' Alché-Buc, F and Fox, E and Garnett, R},<br> keywords = {[cifar],[mnist]},<br> mendeley-tags = {[cifar],[mnist]},<br> pages = {11849--11860},<br> publisher = {Curran Associates, Inc.},<br> title = {Online Continual Learning with Maximal Interfered Retrieval},<br> url = {http://papers.nips.cc/paper/9357-online-continual-learning-with-maximal-interfered-retrieval.pdf},<br> year = {2019}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Aljundi2019aContinual_Sequential_Learning_more2" style="display: none">
    N.A.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Aljundi2019aContinual_Sequential_Learning_more3" style="display: none">
    N.A.
</span></p>
<script>
    function Aljundi2019aContinual_Sequential_LearningFunction() {
      var moreText = document.getElementById("Aljundi2019aContinual_Sequential_Learning_more");
      var moreText2 = document.getElementById("Aljundi2019aContinual_Sequential_Learning_more2");
      var moreText3 = document.getElementById("Aljundi2019aContinual_Sequential_Learning_more3");
      var btnText = document.getElementById("Aljundi2019aContinual_Sequential_Learning_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Aljundi2019aContinual_Sequential_LearningFunction2() {
      var moreText = document.getElementById("Aljundi2019aContinual_Sequential_Learning_more2");
      var moreText1 = document.getElementById("Aljundi2019aContinual_Sequential_Learning_more");
      var moreText3 = document.getElementById("Aljundi2019aContinual_Sequential_Learning_more3");
      var btnText = document.getElementById("Aljundi2019aContinual_Sequential_Learning_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Aljundi2019aContinual_Sequential_LearningFunction3() {
      var moreText = document.getElementById("Aljundi2019aContinual_Sequential_Learning_more3");
      var moreText1 = document.getElementById("Aljundi2019aContinual_Sequential_Learning_more");
      var moreText2 = document.getElementById("Aljundi2019aContinual_Sequential_Learning_more2");
      var btnText = document.getElementById("Aljundi2019aContinual_Sequential_Learning_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p>A Study on Catastrophic Forgetting in Deep LSTM Networks by Monika Schak and Alexander Gepperth. <em>Artificial Neural Networks and Machine Learning – ICANN 2019: Deep Learning</em>, 714–728, 2019. <span style='background-color:AliceBlue; padding: 2px; border-radius:4px; border: 1px solid black;'>[rnn]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Schak2019aContinual_Sequential_LearningFunction()" id="Schak2019aContinual_Sequential_Learning_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Schak2019aContinual_Sequential_LearningFunction2()" id="Schak2019aContinual_Sequential_Learning_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Schak2019aContinual_Sequential_LearningFunction3()" id="Schak2019aContinual_Sequential_Learning_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Schak2019aContinual_Sequential_Learning_more" style="display: none">
    @inproceedings{Schak2019a,<br> address = {Cham},<br> author = {Schak, Monika and Gepperth, Alexander},<br> booktitle = {Artificial Neural Networks and Machine Learning – ICANN 2019: Deep Learning},<br> doi = {10.1007/978-3-030-30484-3_56},<br> editor = {Tetko, Igor V and Kůrková, Věra and Karpov, Pavel and Theis, Fabian},<br> isbn = {978-3-030-30484-3},<br> keywords = {Catastrophic Forgetting,LSTM,[rnn],sequential},<br> language = {en},<br> mendeley-tags = {[rnn]},<br> pages = {714--728},<br> publisher = {Springer International Publishing},<br> series = {Lecture Notes in Computer Science},<br> title = {A Study on Catastrophic Forgetting in Deep LSTM Networks},<br> year = {2019}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Schak2019aContinual_Sequential_Learning_more2" style="display: none">
    We present a systematic study of Catastrophic Forgetting (CF), i.e., the abrupt loss of previously acquired knowledge, when retraining deep recurrent LSTM networks with new samples. CF has recently received renewed attention in the case of feed-forward DNNs, and this article is the first work that aims to rigorously establish whether deep LSTM networks are afflicted by CF as well, and to what degree. In order to test this fully, training is conducted using a wide variety of high-dimensional image-based sequence classification tasks derived from established visual classification benchmarks (MNIST, Devanagari, FashionMNIST and EMNIST). We find that the CF effect occurs universally, without exception, for deep LSTM-based sequence classifiers, regardless of the construction and provenance of sequences. This leads us to conclude that LSTMs, just like DNNs, are fully affected by CF, and that further research work needs to be conducted in order to determine how to avoid this effect (which is not a goal of this study).
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Schak2019aContinual_Sequential_Learning_more3" style="display: none">
    N.A.
</span></p>
<script>
    function Schak2019aContinual_Sequential_LearningFunction() {
      var moreText = document.getElementById("Schak2019aContinual_Sequential_Learning_more");
      var moreText2 = document.getElementById("Schak2019aContinual_Sequential_Learning_more2");
      var moreText3 = document.getElementById("Schak2019aContinual_Sequential_Learning_more3");
      var btnText = document.getElementById("Schak2019aContinual_Sequential_Learning_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Schak2019aContinual_Sequential_LearningFunction2() {
      var moreText = document.getElementById("Schak2019aContinual_Sequential_Learning_more2");
      var moreText1 = document.getElementById("Schak2019aContinual_Sequential_Learning_more");
      var moreText3 = document.getElementById("Schak2019aContinual_Sequential_Learning_more3");
      var btnText = document.getElementById("Schak2019aContinual_Sequential_Learning_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Schak2019aContinual_Sequential_LearningFunction3() {
      var moreText = document.getElementById("Schak2019aContinual_Sequential_Learning_more3");
      var moreText1 = document.getElementById("Schak2019aContinual_Sequential_Learning_more");
      var moreText2 = document.getElementById("Schak2019aContinual_Sequential_Learning_more2");
      var btnText = document.getElementById("Schak2019aContinual_Sequential_Learning_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="http://arxiv.org/abs/1906.05201">Task Agnostic Continual Learning via Meta Learning</a> by Xu He, Jakub Sygnowski, Alexandre Galashov, Andrei A Rusu, Yee Whye Teh and Razvan Pascanu. <em>arXiv:1906.05201 [cs, stat]</em>, 2019. <span style='background-color:MediumAquaMarine; padding: 2px; border-radius:4px; border: 1px solid black;'>[mnist]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="He2019aContinual_Sequential_LearningFunction()" id="He2019aContinual_Sequential_Learning_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="He2019aContinual_Sequential_LearningFunction2()" id="He2019aContinual_Sequential_Learning_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="He2019aContinual_Sequential_LearningFunction3()" id="He2019aContinual_Sequential_Learning_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="He2019aContinual_Sequential_Learning_more" style="display: none">
    @misc{He2019a,<br> annote = {arXiv: 1906.05201},<br> author = {He, Xu and Sygnowski, Jakub and Galashov, Alexandre and Rusu, Andrei A and Teh, Yee Whye and Pascanu, Razvan},<br> booktitle = {arXiv:1906.05201 [cs, stat]},<br> keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computi,Statistics - Machine Learning,[mnist]},<br> mendeley-tags = {[mnist]},<br> month = {jun},<br> title = {Task Agnostic Continual Learning via Meta Learning},<br> url = {http://arxiv.org/abs/1906.05201},<br> year = {2019}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="He2019aContinual_Sequential_Learning_more2" style="display: none">
    While neural networks are powerful function approximators, they suffer from catastrophic forgetting when the data distribution is not stationary. One particular formalism that studies learning under non-stationary distribution is provided by continual learning, where the non-stationarity is imposed by a sequence of distinct tasks. Most methods in this space assume, however, the knowledge of task boundaries, and focus on alleviating catastrophic forgetting. In this work, we depart from this view and move the focus towards faster remembering – i.e measuring how quickly the network recovers performance rather than measuring the network's performance without any adaptation. We argue that in many settings this can be more effective and that it opens the door to combining meta-learning and continual learning techniques, leveraging their complementary advantages. We propose a framework specific for the scenario where no information about task boundaries or task identity is given. It relies on a separation of concerns into what task is being solved and how the task should be solved. This framework is implemented by differentiating task specific parameters from task agnostic parameters, where the latter are optimized in a continual meta learning fashion, without access to multiple tasks at the same time. We showcase this framework in a supervised learning scenario and discuss the implication of the proposed formalism.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="He2019aContinual_Sequential_Learning_more3" style="display: none">
    arXiv: 1906.05201
</span></p>
<script>
    function He2019aContinual_Sequential_LearningFunction() {
      var moreText = document.getElementById("He2019aContinual_Sequential_Learning_more");
      var moreText2 = document.getElementById("He2019aContinual_Sequential_Learning_more2");
      var moreText3 = document.getElementById("He2019aContinual_Sequential_Learning_more3");
      var btnText = document.getElementById("He2019aContinual_Sequential_Learning_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function He2019aContinual_Sequential_LearningFunction2() {
      var moreText = document.getElementById("He2019aContinual_Sequential_Learning_more2");
      var moreText1 = document.getElementById("He2019aContinual_Sequential_Learning_more");
      var moreText3 = document.getElementById("He2019aContinual_Sequential_Learning_more3");
      var btnText = document.getElementById("He2019aContinual_Sequential_Learning_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function He2019aContinual_Sequential_LearningFunction3() {
      var moreText = document.getElementById("He2019aContinual_Sequential_Learning_more3");
      var moreText1 = document.getElementById("He2019aContinual_Sequential_Learning_more");
      var moreText2 = document.getElementById("He2019aContinual_Sequential_Learning_more2");
      var btnText = document.getElementById("He2019aContinual_Sequential_Learning_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="http://papers.nips.cc/paper/9354-gradient-based-sample-selection-for-online-continual-learning.pdf">Gradient based sample selection for online continual learning</a> by Rahaf Aljundi, Min Lin, Baptiste Goujaud and Yoshua Bengio. <em>Advances in Neural Information Processing Systems 32</em>, 11816–11825, 2019. <span style='background-color:DarkCyan; padding: 2px; border-radius:4px; border: 1px solid black;'>[cifar]</span> <span style='background-color:MediumAquaMarine; padding: 2px; border-radius:4px; border: 1px solid black;'>[mnist]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Aljundi2019dContinual_Sequential_LearningFunction()" id="Aljundi2019dContinual_Sequential_Learning_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Aljundi2019dContinual_Sequential_LearningFunction2()" id="Aljundi2019dContinual_Sequential_Learning_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Aljundi2019dContinual_Sequential_LearningFunction3()" id="Aljundi2019dContinual_Sequential_Learning_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Aljundi2019dContinual_Sequential_Learning_more" style="display: none">
    @inproceedings{Aljundi2019d,<br> author = {Aljundi, Rahaf and Lin, Min and Goujaud, Baptiste and Bengio, Yoshua},<br> booktitle = {Advances in Neural Information Processing Systems 32},<br> editor = {Wallach, H and Larochelle, H and Beygelzimer, A and d' Alché-Buc, F and Fox, E and Garnett, R},<br> keywords = {[cifar],[mnist]},<br> mendeley-tags = {[cifar],[mnist]},<br> pages = {11816--11825},<br> publisher = {Curran Associates, Inc.},<br> title = {Gradient based sample selection for online continual learning},<br> url = {http://papers.nips.cc/paper/9354-gradient-based-sample-selection-for-online-continual-learning.pdf},<br> year = {2019}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Aljundi2019dContinual_Sequential_Learning_more2" style="display: none">
    N.A.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Aljundi2019dContinual_Sequential_Learning_more3" style="display: none">
    N.A.
</span></p>
<script>
    function Aljundi2019dContinual_Sequential_LearningFunction() {
      var moreText = document.getElementById("Aljundi2019dContinual_Sequential_Learning_more");
      var moreText2 = document.getElementById("Aljundi2019dContinual_Sequential_Learning_more2");
      var moreText3 = document.getElementById("Aljundi2019dContinual_Sequential_Learning_more3");
      var btnText = document.getElementById("Aljundi2019dContinual_Sequential_Learning_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Aljundi2019dContinual_Sequential_LearningFunction2() {
      var moreText = document.getElementById("Aljundi2019dContinual_Sequential_Learning_more2");
      var moreText1 = document.getElementById("Aljundi2019dContinual_Sequential_Learning_more");
      var moreText3 = document.getElementById("Aljundi2019dContinual_Sequential_Learning_more3");
      var btnText = document.getElementById("Aljundi2019dContinual_Sequential_Learning_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Aljundi2019dContinual_Sequential_LearningFunction3() {
      var moreText = document.getElementById("Aljundi2019dContinual_Sequential_Learning_more3");
      var moreText1 = document.getElementById("Aljundi2019dContinual_Sequential_Learning_more");
      var moreText2 = document.getElementById("Aljundi2019dContinual_Sequential_Learning_more2");
      var btnText = document.getElementById("Aljundi2019dContinual_Sequential_Learning_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="http://arxiv.org/abs/1904.02021">Unsupervised Progressive Learning and the STAM Architecture</a> by James Smith, Seth Baer, Cameron Taylor and Constantine Dovrolis. <em>arXiv</em>, 2019. <span style='background-color:MediumAquaMarine; padding: 2px; border-radius:4px; border: 1px solid black;'>[mnist]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="smith2020aContinual_Sequential_LearningFunction()" id="smith2020aContinual_Sequential_Learning_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="smith2020aContinual_Sequential_LearningFunction2()" id="smith2020aContinual_Sequential_Learning_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="smith2020aContinual_Sequential_LearningFunction3()" id="smith2020aContinual_Sequential_Learning_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="smith2020aContinual_Sequential_Learning_more" style="display: none">
    @article{smith2020a,<br> archiveprefix = {arXiv},<br> arxivid = {1904.02021},<br> author = {Smith, James and Baer, Seth and Taylor, Cameron and Dovrolis, Constantine},<br> eprint = {1904.02021},<br> file = {::},<br> journal = {arXiv},<br> keywords = {[mnist]},<br> mendeley-tags = {[mnist]},<br> month = {apr},<br> title = {Unsupervised Progressive Learning and the STAM Architecture},<br> url = {http://arxiv.org/abs/1904.02021},<br> year = {2019}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="smith2020aContinual_Sequential_Learning_more2" style="display: none">
    We first pose the Unsupervised Progressive Learning (UPL) problem: an online representation learning problem in which the learner observes a non-stationary and unlabeled data stream, and identifies a growing number of features that persist over time even though the data is not stored or replayed. To solve the UPL problem we propose the Self-Taught Associative Memory (STAM) architecture. Layered hierarchies of STAM modules learn based on a combination of online clustering, novelty detection, forgetting outliers, and storing only prototypical features rather than specific examples. We evaluate STAM representations using classification and clustering tasks. Even though there are no prior approaches that are directly applicable to the UPL problem, we evaluate the STAM architecture in comparison to some unsupervised and self-supervised deep learning approaches adapted in the UPL context.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="smith2020aContinual_Sequential_Learning_more3" style="display: none">
    N.A.
</span></p>
<script>
    function smith2020aContinual_Sequential_LearningFunction() {
      var moreText = document.getElementById("smith2020aContinual_Sequential_Learning_more");
      var moreText2 = document.getElementById("smith2020aContinual_Sequential_Learning_more2");
      var moreText3 = document.getElementById("smith2020aContinual_Sequential_Learning_more3");
      var btnText = document.getElementById("smith2020aContinual_Sequential_Learning_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function smith2020aContinual_Sequential_LearningFunction2() {
      var moreText = document.getElementById("smith2020aContinual_Sequential_Learning_more2");
      var moreText1 = document.getElementById("smith2020aContinual_Sequential_Learning_more");
      var moreText3 = document.getElementById("smith2020aContinual_Sequential_Learning_more3");
      var btnText = document.getElementById("smith2020aContinual_Sequential_Learning_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function smith2020aContinual_Sequential_LearningFunction3() {
      var moreText = document.getElementById("smith2020aContinual_Sequential_Learning_more3");
      var moreText1 = document.getElementById("smith2020aContinual_Sequential_Learning_more");
      var moreText2 = document.getElementById("smith2020aContinual_Sequential_Learning_more2");
      var btnText = document.getElementById("smith2020aContinual_Sequential_Learning_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="https://openreview.net/pdf?id=SJ1Xmf-Rb">FearNet: Brain-Inspired Model for Incremental Learning</a> by Ronald Kemker and Christopher Kanan. <em>ICLR</em>, 2018. <span style='background-color:#50EBEC; padding: 2px; border-radius:4px; border: 1px solid black;'>[audio]</span> <span style='background-color:DarkCyan; padding: 2px; border-radius:4px; border: 1px solid black;'>[cifar]</span> <span style='background-color:Maroon; padding: 2px; border-radius:4px; border: 1px solid black;'>[generative]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="kemker2018bContinual_Sequential_LearningFunction()" id="kemker2018bContinual_Sequential_Learning_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="kemker2018bContinual_Sequential_LearningFunction2()" id="kemker2018bContinual_Sequential_Learning_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="kemker2018bContinual_Sequential_LearningFunction3()" id="kemker2018bContinual_Sequential_Learning_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="kemker2018bContinual_Sequential_Learning_more" style="display: none">
    @inproceedings{kemker2018b,<br> author = {Kemker, Ronald and Kanan, Christopher},<br> booktitle = {ICLR},<br> file = {::},<br> keywords = {[audio],[cifar],[generative]},<br> mendeley-tags = {[audio],[cifar],[generative]},<br> month = {feb},<br> title = {FearNet: Brain-Inspired Model for Incremental Learning},<br> url = {https://openreview.net/pdf?id=SJ1Xmf-Rb},<br> year = {2018}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="kemker2018bContinual_Sequential_Learning_more2" style="display: none">
    Incremental class learning involves sequentially learning classes in bursts of examples from the same class. This violates the assumptions that underlie methods for training standard deep neural networks, and will cause them to suffer from catastrophic forgetting. Arguably, the best method for incremental class learning is iCaRL, but it requires storing training examples for each class, making it challenging to scale. Here, we propose FearNet for incremental class learning. FearNet is a generative model that does not store previous examples, making it memory efficient. FearNet uses a brain-inspired dual-memory system in which new memories are consolidated from a network for recent memories inspired by the mammalian hippocampal complex to a network for long-term storage inspired by medial prefrontal cortex. Memory consolidation is inspired by mechanisms that occur during sleep. FearNet also uses a module inspired by the basolateral amygdala for determining which memory system to use for recall. FearNet achieves state-of-the-art performance at incremental class learning on image (CIFAR-100, CUB-200) and audio classification (AudioSet) benchmarks.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="kemker2018bContinual_Sequential_Learning_more3" style="display: none">
    N.A.
</span></p>
<script>
    function kemker2018bContinual_Sequential_LearningFunction() {
      var moreText = document.getElementById("kemker2018bContinual_Sequential_Learning_more");
      var moreText2 = document.getElementById("kemker2018bContinual_Sequential_Learning_more2");
      var moreText3 = document.getElementById("kemker2018bContinual_Sequential_Learning_more3");
      var btnText = document.getElementById("kemker2018bContinual_Sequential_Learning_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function kemker2018bContinual_Sequential_LearningFunction2() {
      var moreText = document.getElementById("kemker2018bContinual_Sequential_Learning_more2");
      var moreText1 = document.getElementById("kemker2018bContinual_Sequential_Learning_more");
      var moreText3 = document.getElementById("kemker2018bContinual_Sequential_Learning_more3");
      var btnText = document.getElementById("kemker2018bContinual_Sequential_Learning_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function kemker2018bContinual_Sequential_LearningFunction3() {
      var moreText = document.getElementById("kemker2018bContinual_Sequential_Learning_more3");
      var moreText1 = document.getElementById("kemker2018bContinual_Sequential_Learning_more");
      var moreText2 = document.getElementById("kemker2018bContinual_Sequential_Learning_more2");
      var btnText = document.getElementById("kemker2018bContinual_Sequential_Learning_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p>Overcoming Catastrophic interference using conceptor-aided backpropagation by Xu He and Herbert Jaeger. <em>ICLR</em>, 2018. <span style='background-color:MediumAquaMarine; padding: 2px; border-radius:4px; border: 1px solid black;'>[mnist]</span> <span style='background-color:AliceBlue; padding: 2px; border-radius:4px; border: 1px solid black;'>[rnn]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="xu2018aContinual_Sequential_LearningFunction()" id="xu2018aContinual_Sequential_Learning_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="xu2018aContinual_Sequential_LearningFunction2()" id="xu2018aContinual_Sequential_Learning_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="xu2018aContinual_Sequential_LearningFunction3()" id="xu2018aContinual_Sequential_Learning_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="xu2018aContinual_Sequential_Learning_more" style="display: none">
    @inproceedings{xu2018a,<br> author = {Xu, Ju and Zhu, Zhanxing},<br> booktitle = {Advances in Neural Information Processing Systems},<br> keywords = {[cifar],[mnist]},<br> mendeley-tags = {[cifar],[mnist]},<br> pages = {899--908},<br> title = {Reinforced continual learning},<br> url = {http://papers.nips.cc/paper/7369-reinforced-continual-learning},<br> year = {2018}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="xu2018aContinual_Sequential_Learning_more2" style="display: none">
    Catastrophic interference has been a major roadblock in the research of continual learning. Here we propose a variant of the back-propagation algorithm, "conceptor-aided backprop" (CAB), in which gradients are shielded by concep-tors against degradation of previously learned tasks. Conceptors have their origin in reservoir computing, where they have been previously shown to overcome catastrophic forgetting. CAB extends these results to deep feedforward networks. On the disjoint and permuted MNIST tasks, CAB outperforms two other methods for coping with catastrophic interference that have recently been proposed.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="xu2018aContinual_Sequential_Learning_more3" style="display: none">
    N.A.
</span></p>
<script>
    function xu2018aContinual_Sequential_LearningFunction() {
      var moreText = document.getElementById("xu2018aContinual_Sequential_Learning_more");
      var moreText2 = document.getElementById("xu2018aContinual_Sequential_Learning_more2");
      var moreText3 = document.getElementById("xu2018aContinual_Sequential_Learning_more3");
      var btnText = document.getElementById("xu2018aContinual_Sequential_Learning_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function xu2018aContinual_Sequential_LearningFunction2() {
      var moreText = document.getElementById("xu2018aContinual_Sequential_Learning_more2");
      var moreText1 = document.getElementById("xu2018aContinual_Sequential_Learning_more");
      var moreText3 = document.getElementById("xu2018aContinual_Sequential_Learning_more3");
      var btnText = document.getElementById("xu2018aContinual_Sequential_Learning_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function xu2018aContinual_Sequential_LearningFunction3() {
      var moreText = document.getElementById("xu2018aContinual_Sequential_Learning_more3");
      var moreText1 = document.getElementById("xu2018aContinual_Sequential_Learning_more");
      var moreText2 = document.getElementById("xu2018aContinual_Sequential_Learning_more2");
      var btnText = document.getElementById("xu2018aContinual_Sequential_Learning_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="http://openaccess.thecvf.com/content_ECCV_2018/html/Arslan_Chaudhry__Riemannian_Walk_ECCV_2018_paper.html">Riemannian Walk for Incremental Learning: Understanding Forgetting and Intransigence</a> by Arslan Chaudhry, Puneet K Dokania, Thalaiyasingam Ajanthan and Philip H.S. Torr. <em>Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</em>, 556–572, 2018. <span style='background-color:DarkCyan; padding: 2px; border-radius:4px; border: 1px solid black;'>[cifar]</span> <span style='background-color:MediumAquaMarine; padding: 2px; border-radius:4px; border: 1px solid black;'>[mnist]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Chaudhry2018aContinual_Sequential_LearningFunction()" id="Chaudhry2018aContinual_Sequential_Learning_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Chaudhry2018aContinual_Sequential_LearningFunction2()" id="Chaudhry2018aContinual_Sequential_Learning_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Chaudhry2018aContinual_Sequential_LearningFunction3()" id="Chaudhry2018aContinual_Sequential_Learning_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Chaudhry2018aContinual_Sequential_Learning_more" style="display: none">
    @unpublished{Chaudhry2018a,<br> archiveprefix = {arXiv},<br> arxivid = {1801.10112},<br> author = {Chaudhry, Arslan and Dokania, Puneet K and Ajanthan, Thalaiyasingam and Torr, Philip H.S.},<br> booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},<br> doi = {10.1007/978-3-030-01252-6_33},<br> eprint = {1801.10112},<br> isbn = {9783030012519},<br> issn = {16113349},<br> keywords = {[cifar],[mnist],ewc,ewc++,fisher,forgetting},<br> mendeley-tags = {[cifar],[mnist]},<br> pages = {556--572},<br> shorttitle = {Riemannian Walk for Incremental Learning},<br> title = {Riemannian Walk for Incremental Learning: Understanding Forgetting and Intransigence},<br> url = {http://openaccess.thecvf.com/content_ECCV_2018/html/Arslan_Chaudhry__Riemannian_Walk_ECCV_2018_paper.html},<br> volume = {11215 LNCS},<br> year = {2018}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Chaudhry2018aContinual_Sequential_Learning_more2" style="display: none">
    Incremental learning (il) has received a lot of attention recently, however, the literature lacks a precise problem definition, proper evaluation settings, and metrics tailored specifically for the il problem. One of the main objectives of this work is to fill these gaps so as to provide a common ground for better understanding of il. The main challenge for an il algorithm is to update the classifier whilst preserving existing knowledge. We observe that, in addition to forgetting, a known issue while preserving knowledge, il also suffers from a problem we call intransigence, its inability to update knowledge. We introduce two metrics to quantify forgetting and intransigence that allow us to understand, analyse, and gain better insights into the behaviour of il algorithms. Furthermore, we present RWalk, a generalization of ewc++ (our efficient version of ewc [6]) and Path Integral [25] with a theoretically grounded KL-divergence based perspective. We provide a thorough analysis of various il algorithms on MNIST and CIFAR-100 datasets. In these experiments, RWalk obtains superior results in terms of accuracy, and also provides a better trade-off for forgetting and intransigence.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Chaudhry2018aContinual_Sequential_Learning_more3" style="display: none">
    N.A.
</span></p>
<script>
    function Chaudhry2018aContinual_Sequential_LearningFunction() {
      var moreText = document.getElementById("Chaudhry2018aContinual_Sequential_Learning_more");
      var moreText2 = document.getElementById("Chaudhry2018aContinual_Sequential_Learning_more2");
      var moreText3 = document.getElementById("Chaudhry2018aContinual_Sequential_Learning_more3");
      var btnText = document.getElementById("Chaudhry2018aContinual_Sequential_Learning_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Chaudhry2018aContinual_Sequential_LearningFunction2() {
      var moreText = document.getElementById("Chaudhry2018aContinual_Sequential_Learning_more2");
      var moreText1 = document.getElementById("Chaudhry2018aContinual_Sequential_Learning_more");
      var moreText3 = document.getElementById("Chaudhry2018aContinual_Sequential_Learning_more3");
      var btnText = document.getElementById("Chaudhry2018aContinual_Sequential_Learning_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Chaudhry2018aContinual_Sequential_LearningFunction3() {
      var moreText = document.getElementById("Chaudhry2018aContinual_Sequential_Learning_more3");
      var moreText1 = document.getElementById("Chaudhry2018aContinual_Sequential_Learning_more");
      var moreText2 = document.getElementById("Chaudhry2018aContinual_Sequential_Learning_more2");
      var btnText = document.getElementById("Chaudhry2018aContinual_Sequential_Learning_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="http://arxiv.org/abs/1706.08840">Gradient Episodic Memory for Continual Learning</a> by David Lopez-Paz and Marc’Aurelio Ranzato. <em>NIPS</em>, 2017. <span style='background-color:DarkCyan; padding: 2px; border-radius:4px; border: 1px solid black;'>[cifar]</span> <span style='background-color:MediumAquaMarine; padding: 2px; border-radius:4px; border: 1px solid black;'>[mnist]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="LopezPaz2017aContinual_Sequential_LearningFunction()" id="LopezPaz2017aContinual_Sequential_Learning_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="LopezPaz2017aContinual_Sequential_LearningFunction2()" id="LopezPaz2017aContinual_Sequential_Learning_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="LopezPaz2017aContinual_Sequential_LearningFunction3()" id="LopezPaz2017aContinual_Sequential_Learning_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="LopezPaz2017aContinual_Sequential_Learning_more" style="display: none">
    @inproceedings{Lopez-Paz2017a,<br> annote = {Comment: Published at NIPS 2017<br>arXiv: 1706.08840},<br> author = {Lopez-Paz, David and Ranzato, Marc'Aurelio},<br> booktitle = {NIPS},<br> keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,[cifar],[mnist],gem},<br> mendeley-tags = {[cifar],[mnist]},<br> title = {Gradient Episodic Memory for Continual Learning},<br> url = {http://arxiv.org/abs/1706.08840},<br> year = {2017}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="LopezPaz2017aContinual_Sequential_Learning_more2" style="display: none">
    One major obstacle towards AI is the poor ability of models to solve new problems quicker, and without forgetting previously acquired knowledge. To better understand this issue, we study the problem of continual learning, where the model observes, once and one by one, examples concerning a sequence of tasks. First, we propose a set of metrics to evaluate models learning over a continuum of data. These metrics characterize models not only by their test accuracy, but also in terms of their ability to transfer knowledge across tasks. Second, we propose a model for continual learning, called Gradient Episodic Memory (GEM) that alleviates forgetting, while allowing beneficial transfer of knowledge to previous tasks. Our experiments on variants of the MNIST and CIFAR-100 datasets demonstrate the strong performance of GEM when compared to the state-of-the-art.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="LopezPaz2017aContinual_Sequential_Learning_more3" style="display: none">
    Comment: Published at NIPS 2017
            arXiv: 1706.08840
</span></p>
<script>
    function LopezPaz2017aContinual_Sequential_LearningFunction() {
      var moreText = document.getElementById("LopezPaz2017aContinual_Sequential_Learning_more");
      var moreText2 = document.getElementById("LopezPaz2017aContinual_Sequential_Learning_more2");
      var moreText3 = document.getElementById("LopezPaz2017aContinual_Sequential_Learning_more3");
      var btnText = document.getElementById("LopezPaz2017aContinual_Sequential_Learning_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function LopezPaz2017aContinual_Sequential_LearningFunction2() {
      var moreText = document.getElementById("LopezPaz2017aContinual_Sequential_Learning_more2");
      var moreText1 = document.getElementById("LopezPaz2017aContinual_Sequential_Learning_more");
      var moreText3 = document.getElementById("LopezPaz2017aContinual_Sequential_Learning_more3");
      var btnText = document.getElementById("LopezPaz2017aContinual_Sequential_Learning_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function LopezPaz2017aContinual_Sequential_LearningFunction3() {
      var moreText = document.getElementById("LopezPaz2017aContinual_Sequential_Learning_more3");
      var moreText1 = document.getElementById("LopezPaz2017aContinual_Sequential_Learning_more");
      var moreText2 = document.getElementById("LopezPaz2017aContinual_Sequential_Learning_more2");
      var btnText = document.getElementById("LopezPaz2017aContinual_Sequential_Learning_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p>iCaRL: Incremental Classifier and Representation Learning by Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl and Christoph H Lampert. <em>The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2017. <span style='background-color:DarkCyan; padding: 2px; border-radius:4px; border: 1px solid black;'>[cifar]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Rebuffi2017aContinual_Sequential_LearningFunction()" id="Rebuffi2017aContinual_Sequential_Learning_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Rebuffi2017aContinual_Sequential_LearningFunction2()" id="Rebuffi2017aContinual_Sequential_Learning_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Rebuffi2017aContinual_Sequential_LearningFunction3()" id="Rebuffi2017aContinual_Sequential_Learning_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Rebuffi2017aContinual_Sequential_Learning_more" style="display: none">
    @inproceedings{Rebuffi2017a,<br> author = {Rebuffi, Sylvestre-Alvise and Kolesnikov, Alexander and Sperl, Georg and Lampert, Christoph H},<br> booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},<br> keywords = {[cifar]},<br> mendeley-tags = {[cifar]},<br> month = {jul},<br> title = {iCaRL: Incremental Classifier and Representation Learning},<br> year = {2017}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Rebuffi2017aContinual_Sequential_Learning_more2" style="display: none">
    N.A.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Rebuffi2017aContinual_Sequential_Learning_more3" style="display: none">
    N.A.
</span></p>
<script>
    function Rebuffi2017aContinual_Sequential_LearningFunction() {
      var moreText = document.getElementById("Rebuffi2017aContinual_Sequential_Learning_more");
      var moreText2 = document.getElementById("Rebuffi2017aContinual_Sequential_Learning_more2");
      var moreText3 = document.getElementById("Rebuffi2017aContinual_Sequential_Learning_more3");
      var btnText = document.getElementById("Rebuffi2017aContinual_Sequential_Learning_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Rebuffi2017aContinual_Sequential_LearningFunction2() {
      var moreText = document.getElementById("Rebuffi2017aContinual_Sequential_Learning_more2");
      var moreText1 = document.getElementById("Rebuffi2017aContinual_Sequential_Learning_more");
      var moreText3 = document.getElementById("Rebuffi2017aContinual_Sequential_Learning_more3");
      var btnText = document.getElementById("Rebuffi2017aContinual_Sequential_Learning_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Rebuffi2017aContinual_Sequential_LearningFunction3() {
      var moreText = document.getElementById("Rebuffi2017aContinual_Sequential_Learning_more3");
      var moreText1 = document.getElementById("Rebuffi2017aContinual_Sequential_Learning_more");
      var moreText2 = document.getElementById("Rebuffi2017aContinual_Sequential_Learning_more2");
      var btnText = document.getElementById("Rebuffi2017aContinual_Sequential_Learning_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
</ul>
</div>
<div class="section" id="dissertation-and-theses">
<h3>Dissertation and Theses<a class="headerlink" href="#dissertation-and-theses" title="Permalink to this headline">¶</a></h3>
<p>In this section we maintain a list of all the dissertation and thesis produced on continual learning and related topics.</p>
<ul class="simple">
<li><p><a class="reference external" href="http://amsdottorato.unibo.it/9073/">Continual Learning with Deep Architectures</a> by  and Vincenzo Lomonaco. <em>University of Bologna</em>, 2019. <span style='background-color:Tomato; padding: 2px; border-radius:4px; border: 1px solid black;'>[framework]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="lomonaco2019aDissertation_and_ThesesFunction()" id="lomonaco2019aDissertation_and_Theses_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="lomonaco2019aDissertation_and_ThesesFunction2()" id="lomonaco2019aDissertation_and_Theses_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="lomonaco2019aDissertation_and_ThesesFunction3()" id="lomonaco2019aDissertation_and_Theses_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="lomonaco2019aDissertation_and_Theses_more" style="display: none">
    @phdthesis{lomonaco2019a,<br> author = {Lomonaco, Vincenzo},<br> booktitle = {University of Bologna},<br> doi = {Lomonaco, Vincenzo (2019) Continual Learning with Deep Architectures, [Dissertation thesis], Alma Mater Studiorum Università di Bologna. Dottorato di ricerca in Computer science and engineering <http://amsdottorato.unibo.it/view/dottorati/DOT536/>, 31 Ciclo. DOI 10.6092/unibo/amsdottorato/9073.},<br> keywords = {[framework]},<br> language = {it},<br> mendeley-tags = {[framework]},<br> month = {apr},<br> school = {alma},<br> title = {Continual Learning with Deep Architectures},<br> url = {http://amsdottorato.unibo.it/9073/},<br> year = {2019}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="lomonaco2019aDissertation_and_Theses_more2" style="display: none">
    Humans have the extraordinary ability to learn continually from experience. Not only we can apply previously learned knowledge and skills to new situations, we can also use these as the foundation for later learning. One of the grand goals of Artificial Intelligence (AI) is building an artificial “continual learning” agent that constructs a sophisticated understanding of the world from its own experience through the autonomous incremental development of ever more complex knowledge and skills. However, despite early speculations and few pioneering works, very little research and effort has been devoted to address this vision. Current AI systems greatly suffer from the exposure to new data or environments which even slightly differ from the ones for which they have been trained for. Moreover, the learning process is usually constrained on fixed datasets within narrow and isolated tasks which may hardly lead to the emergence of more complex and autonomous intelligent behaviors. In essence, continual learning and adaptation capabilities, while more than often thought as fundamental pillars of every intelligent agent, have been mostly left out of the main AI research focus. In this dissertation, we study the application of these ideas in light of the more recent advances in machine learning research and in the context of deep architectures for AI. We propose a comprehensive and unifying framework for continual learning, new metrics, benchmarks and algorithms, as well as providing substantial experimental evaluations in different supervised, unsupervised and reinforcement learning tasks.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="lomonaco2019aDissertation_and_Theses_more3" style="display: none">
    N.A.
</span></p>
<script>
    function lomonaco2019aDissertation_and_ThesesFunction() {
      var moreText = document.getElementById("lomonaco2019aDissertation_and_Theses_more");
      var moreText2 = document.getElementById("lomonaco2019aDissertation_and_Theses_more2");
      var moreText3 = document.getElementById("lomonaco2019aDissertation_and_Theses_more3");
      var btnText = document.getElementById("lomonaco2019aDissertation_and_Theses_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function lomonaco2019aDissertation_and_ThesesFunction2() {
      var moreText = document.getElementById("lomonaco2019aDissertation_and_Theses_more2");
      var moreText1 = document.getElementById("lomonaco2019aDissertation_and_Theses_more");
      var moreText3 = document.getElementById("lomonaco2019aDissertation_and_Theses_more3");
      var btnText = document.getElementById("lomonaco2019aDissertation_and_Theses_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function lomonaco2019aDissertation_and_ThesesFunction3() {
      var moreText = document.getElementById("lomonaco2019aDissertation_and_Theses_more3");
      var moreText1 = document.getElementById("lomonaco2019aDissertation_and_Theses_more");
      var moreText2 = document.getElementById("lomonaco2019aDissertation_and_Theses_more2");
      var btnText = document.getElementById("lomonaco2019aDissertation_and_Theses_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="http://researchbank.rmit.edu.au/eserv/rmit:162646/Fayek.pdf">Continual Deep Learning via Progressive Learning</a> by  and Haytham M. Fayek. <em>RMIT University</em>, 2019. <span style='background-color:#50EBEC; padding: 2px; border-radius:4px; border: 1px solid black;'>[audio]</span> <span style='background-color:DarkCyan; padding: 2px; border-radius:4px; border: 1px solid black;'>[cifar]</span> <span style='background-color:DeepPink; padding: 2px; border-radius:4px; border: 1px solid black;'>[imagenet]</span> <span style='background-color:DarkSalmon; padding: 2px; border-radius:4px; border: 1px solid black;'>[sparsity]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="fayek2019aDissertation_and_ThesesFunction()" id="fayek2019aDissertation_and_Theses_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="fayek2019aDissertation_and_ThesesFunction2()" id="fayek2019aDissertation_and_Theses_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="fayek2019aDissertation_and_ThesesFunction3()" id="fayek2019aDissertation_and_Theses_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="fayek2019aDissertation_and_Theses_more" style="display: none">
    @phdthesis{fayek2019a,<br> author = {Fayek, Haytham M.},<br> booktitle = {RMIT University},<br> keywords = {[audio],[cifar],[imagenet],[sparsity]},<br> mendeley-tags = {[audio],[cifar],[imagenet],[sparsity]},<br> school = {RMIT University},<br> title = {Continual Deep Learning via Progressive Learning},<br> url = {http://researchbank.rmit.edu.au/eserv/rmit:162646/Fayek.pdf},<br> year = {2019}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="fayek2019aDissertation_and_Theses_more2" style="display: none">
    Machine learning is one of several approaches to artificial intelligence. It allows us to build machines that can learn from experience as opposed to being explicitly programmed. Current machine learning formulations are mostly designed for learning and performing a particular task from a tabula rasa using data available for that task. For machine learning to converge to artificial intelligence, in addition to other desiderata, it must be in a state of continual learning, i.e., have the ability to be in a continuous learning process, such that when a new task is presented, the system can leverage prior knowledge from prior tasks, in learning and performing this new task, and augment the prior knowledge with the newly acquired knowledge without having a significant adverse effect on the prior knowledge. Continual learning is key to advancing machine learning and artificial intelligence. Deep learning is a powerful general-purpose approach to machine learning that is able to solve numerous and various tasks with minimal modification. Deep learning extends machine learning, and specially neural networks, to learn multiple levels of distributed representations together with the required mapping function into a single composite function. The emergence of deep learning and neural networks as a generic approach to machine learning, coupled with their ability to learn versatile hierarchical representations, has paved the way for continual learning. The main aim of this thesis is the study and development of a structured approach to continual learning, leveraging the success of deep learning and neural networks. This thesis studies the application of deep learning to a number of supervised learning tasks, and in particular, classification tasks in machine perception, e.g., image recognition, automatic speech recognition, and speech emotion recognition. The relation between the systems developed for these tasks is investigated to illuminate the layer-wise relevance of features in deep networks trained for these tasks via transfer learning, and these independent systems are unified into continual learning systems. The main contribution of this thesis is the construction and formulation of a deep learning framework, denoted progressive learning, that allows a holistic and systematic approach to continual learning. Progressive learning comprises a number of procedures that address the continual learning desiderata. It is shown that, when tasks are related, progressive learning leads to faster learning that converges to better generalization performance using less amounts of data and a smaller number of dedicated parameters, for the tasks studied in this thesis, by accumulating and leveraging knowledge learned across tasks in a continuous manner. It is envisioned that progressive learning is a step towards a fully general continual learning framework.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="fayek2019aDissertation_and_Theses_more3" style="display: none">
    N.A.
</span></p>
<script>
    function fayek2019aDissertation_and_ThesesFunction() {
      var moreText = document.getElementById("fayek2019aDissertation_and_Theses_more");
      var moreText2 = document.getElementById("fayek2019aDissertation_and_Theses_more2");
      var moreText3 = document.getElementById("fayek2019aDissertation_and_Theses_more3");
      var btnText = document.getElementById("fayek2019aDissertation_and_Theses_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function fayek2019aDissertation_and_ThesesFunction2() {
      var moreText = document.getElementById("fayek2019aDissertation_and_Theses_more2");
      var moreText1 = document.getElementById("fayek2019aDissertation_and_Theses_more");
      var moreText3 = document.getElementById("fayek2019aDissertation_and_Theses_more3");
      var btnText = document.getElementById("fayek2019aDissertation_and_Theses_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function fayek2019aDissertation_and_ThesesFunction3() {
      var moreText = document.getElementById("fayek2019aDissertation_and_Theses_more3");
      var moreText1 = document.getElementById("fayek2019aDissertation_and_Theses_more");
      var moreText2 = document.getElementById("fayek2019aDissertation_and_Theses_more2");
      var btnText = document.getElementById("fayek2019aDissertation_and_Theses_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
</ul>
</div>
<div class="section" id="applications">
<h3>Applications<a class="headerlink" href="#applications" title="Permalink to this headline">¶</a></h3>
<p>In this section we maintain a list of all applicative papers produced on continual learning and related topics.</p>
<ul class="simple">
<li><p><a class="reference external" href="http://arxiv.org/abs/2006.01945">Continual Learning of Predictive Models in Video Sequences via Variational Autoencoders</a> by Damian Campo, Giulia Slavic, Mohamad Baydoun, Lucio Marcenaro and Carlo Regazzoni. <em>arXiv</em>, 2020. <span style='background-color:#E5E4E2; padding: 2px; border-radius:4px; border: 1px solid black;'>[vision]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Campo2020aApplicationsFunction()" id="Campo2020aApplications_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Campo2020aApplicationsFunction2()" id="Campo2020aApplications_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Campo2020aApplicationsFunction3()" id="Campo2020aApplications_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Campo2020aApplications_more" style="display: none">
    @article{Campo2020a,<br> archiveprefix = {arXiv},<br> arxivid = {2006.01945},<br> author = {Campo, Damian and Slavic, Giulia and Baydoun, Mohamad and Marcenaro, Lucio and Regazzoni, Carlo},<br> eprint = {2006.01945},<br> file = {::},<br> journal = {arXiv},<br> keywords = {[vision]},<br> mendeley-tags = {[vision]},<br> month = {jun},<br> title = {Continual Learning of Predictive Models in Video Sequences via Variational Autoencoders},<br> url = {http://arxiv.org/abs/2006.01945},<br> year = {2020}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Campo2020aApplications_more2" style="display: none">
    This paper proposes a method for performing continual learning of predictive models that facilitate the inference of future frames in video sequences. For a first given experience, an initial Variational Autoencoder, together with a set of fully connected neural networks are utilized to respectively learn the appearance of video frames and their dynamics at the latent space level. By employing an adapted Markov Jump Particle Filter, the proposed method recognizes new situations and integrates them as predictive models avoiding catastrophic forgetting of previously learned tasks. For evaluating the proposed method, this article uses video sequences from a vehicle that performs different tasks in a controlled environment.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Campo2020aApplications_more3" style="display: none">
    N.A.
</span></p>
<script>
    function Campo2020aApplicationsFunction() {
      var moreText = document.getElementById("Campo2020aApplications_more");
      var moreText2 = document.getElementById("Campo2020aApplications_more2");
      var moreText3 = document.getElementById("Campo2020aApplications_more3");
      var btnText = document.getElementById("Campo2020aApplications_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Campo2020aApplicationsFunction2() {
      var moreText = document.getElementById("Campo2020aApplications_more2");
      var moreText1 = document.getElementById("Campo2020aApplications_more");
      var moreText3 = document.getElementById("Campo2020aApplications_more3");
      var btnText = document.getElementById("Campo2020aApplications_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Campo2020aApplicationsFunction3() {
      var moreText = document.getElementById("Campo2020aApplications_more3");
      var moreText1 = document.getElementById("Campo2020aApplications_more");
      var moreText2 = document.getElementById("Campo2020aApplications_more2");
      var btnText = document.getElementById("Campo2020aApplications_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="https://openreview.net/forum?id=Skgxcn4YDS">LAMOL: LAnguage MOdeling for Lifelong Language Learning</a> by Fan-Keng Sun, Cheng-Hao Ho and Hung-Yi Lee. <em>ICLR</em>, 2020. <span style='background-color:BlueViolet; padding: 2px; border-radius:4px; border: 1px solid black;'>[nlp]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="sun2019aApplicationsFunction()" id="sun2019aApplications_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="sun2019aApplicationsFunction2()" id="sun2019aApplications_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="sun2019aApplicationsFunction3()" id="sun2019aApplications_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="sun2019aApplications_more" style="display: none">
    @inproceedings{sun2019a,<br> author = {Sun, Fan-Keng and Ho, Cheng-Hao and Lee, Hung-Yi},<br> booktitle = {ICLR},<br> keywords = {[nlp]},<br> mendeley-tags = {[nlp]},<br> month = {sep},<br> shorttitle = {LAMOL},<br> title = {LAMOL: LAnguage MOdeling for Lifelong Language Learning},<br> url = {https://openreview.net/forum?id=Skgxcn4YDS},<br> year = {2020}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="sun2019aApplications_more2" style="display: none">
    Most research on lifelong learning applies to images or games, but not language. We present LAMOL, a simple yet effective method for lifelong language learning (LLL) based on language...
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="sun2019aApplications_more3" style="display: none">
    N.A.
</span></p>
<script>
    function sun2019aApplicationsFunction() {
      var moreText = document.getElementById("sun2019aApplications_more");
      var moreText2 = document.getElementById("sun2019aApplications_more2");
      var moreText3 = document.getElementById("sun2019aApplications_more3");
      var btnText = document.getElementById("sun2019aApplications_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function sun2019aApplicationsFunction2() {
      var moreText = document.getElementById("sun2019aApplications_more2");
      var moreText1 = document.getElementById("sun2019aApplications_more");
      var moreText3 = document.getElementById("sun2019aApplications_more3");
      var btnText = document.getElementById("sun2019aApplications_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function sun2019aApplicationsFunction3() {
      var moreText = document.getElementById("sun2019aApplications_more3");
      var moreText1 = document.getElementById("sun2019aApplications_more");
      var moreText2 = document.getElementById("sun2019aApplications_more2");
      var btnText = document.getElementById("sun2019aApplications_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2005.04288">Incremental Learning for End-to-End Automatic Speech Recognition</a> by Li Fu, Xiaoxiao Li and Libo Zi. <em>arXiv</em>, 2020. <span style='background-color:#50EBEC; padding: 2px; border-radius:4px; border: 1px solid black;'>[audio]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Fu2020aApplicationsFunction()" id="Fu2020aApplications_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Fu2020aApplicationsFunction2()" id="Fu2020aApplications_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Fu2020aApplicationsFunction3()" id="Fu2020aApplications_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Fu2020aApplications_more" style="display: none">
    @article{Fu2020a,<br> author = {Fu, Li and Li, Xiaoxiao and Zi, Libo},<br> file = {::},<br> journal = {arXiv},<br> keywords = {Index Terms: automatic speech recognition,[audio],end-to-end,incremental learning,knowledge distillation},<br> mendeley-tags = {[audio]},<br> title = {Incremental Learning for End-to-End Automatic Speech Recognition},<br> url = {https://arxiv.org/abs/2005.04288},<br> year = {2020}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Fu2020aApplications_more2" style="display: none">
    † We propose an incremental learning for end-to-end Automatic Speech Recognition (ASR) to extend the model's capacity on a new task while retaining the performance on existing ones. The proposed method is effective without accessing to the old dataset to address the issues of high training cost and old dataset unavailability. To achieve this, knowledge distillation is applied as a guidance to retain the recognition ability from the previous model, which is then combined with the new ASR task for model optimization. With an ASR model pre-trained on 12,000h Mandarin speech, we test our proposed method on 300h new scenario task and 1h new named entities task. Experiments show that our method yields 3.25% and 0.88% absolute Character Error Rate (CER) reduction on the new scenario, when compared with the pre-trained model and the full-data retraining baseline, respectively. It even yields a surprising 0.37% absolute CER reduction on the new scenario than the fine-tuning. For the new named entities task, our method significantly improves the accuracy compared with the pre-trained model, i.e. 16.95% absolute CER reduction. For both of the new task adaptions, the new models still maintain a same accuracy with the baseline on the old tasks.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Fu2020aApplications_more3" style="display: none">
    N.A.
</span></p>
<script>
    function Fu2020aApplicationsFunction() {
      var moreText = document.getElementById("Fu2020aApplications_more");
      var moreText2 = document.getElementById("Fu2020aApplications_more2");
      var moreText3 = document.getElementById("Fu2020aApplications_more3");
      var btnText = document.getElementById("Fu2020aApplications_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Fu2020aApplicationsFunction2() {
      var moreText = document.getElementById("Fu2020aApplications_more2");
      var moreText1 = document.getElementById("Fu2020aApplications_more");
      var moreText3 = document.getElementById("Fu2020aApplications_more3");
      var btnText = document.getElementById("Fu2020aApplications_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Fu2020aApplicationsFunction3() {
      var moreText = document.getElementById("Fu2020aApplications_more3");
      var moreText1 = document.getElementById("Fu2020aApplications_more");
      var moreText2 = document.getElementById("Fu2020aApplications_more2");
      var btnText = document.getElementById("Fu2020aApplications_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="http://arxiv.org/abs/2004.03340">Class-Agnostic Continual Learning of Alternating Languages and Domains</a> by Germán Kruszewski, Ionut-Teodor Sorodoc and Tomas Mikolov. <em>arXiv</em>, 2020. <span style='background-color:BlueViolet; padding: 2px; border-radius:4px; border: 1px solid black;'>[nlp]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="kruszewski2020aApplicationsFunction()" id="kruszewski2020aApplications_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="kruszewski2020aApplicationsFunction2()" id="kruszewski2020aApplications_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="kruszewski2020aApplicationsFunction3()" id="kruszewski2020aApplications_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="kruszewski2020aApplications_more" style="display: none">
    @article{kruszewski2020a,<br> annote = {arXiv: 2004.03340},<br> author = {Kruszewski, Germán and Sorodoc, Ionut-Teodor and Mikolov, Tomas},<br> journal = {arXiv},<br> keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,[nlp],expert,mixture},<br> mendeley-tags = {[nlp]},<br> month = {apr},<br> title = {Class-Agnostic Continual Learning of Alternating Languages and Domains},<br> url = {http://arxiv.org/abs/2004.03340},<br> year = {2020}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="kruszewski2020aApplications_more2" style="display: none">
    Continual Learning has been often framed as the problem of training a model in a sequence of tasks. In this regard, Neural Networks have been attested to forget the solutions to previous task as they learn new ones. Yet, modelling human life-long learning does not necessarily require any crisp notion of tasks. In this work, we propose a benchmark based on language modelling in a multilingual and multidomain setting that prescinds of any explicit delimitation of training examples into distinct tasks, and propose metrics to study continual learning and catastrophic forgetting in this setting. Then, we introduce a simple Product of Experts learning system that performs strongly on this problem while displaying interesting properties, and investigate its merits for avoiding forgetting.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="kruszewski2020aApplications_more3" style="display: none">
    arXiv: 2004.03340
</span></p>
<script>
    function kruszewski2020aApplicationsFunction() {
      var moreText = document.getElementById("kruszewski2020aApplications_more");
      var moreText2 = document.getElementById("kruszewski2020aApplications_more2");
      var moreText3 = document.getElementById("kruszewski2020aApplications_more3");
      var btnText = document.getElementById("kruszewski2020aApplications_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function kruszewski2020aApplicationsFunction2() {
      var moreText = document.getElementById("kruszewski2020aApplications_more2");
      var moreText1 = document.getElementById("kruszewski2020aApplications_more");
      var moreText3 = document.getElementById("kruszewski2020aApplications_more3");
      var btnText = document.getElementById("kruszewski2020aApplications_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function kruszewski2020aApplicationsFunction3() {
      var moreText = document.getElementById("kruszewski2020aApplications_more3");
      var moreText1 = document.getElementById("kruszewski2020aApplications_more");
      var moreText2 = document.getElementById("kruszewski2020aApplications_more2");
      var btnText = document.getElementById("kruszewski2020aApplications_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="http://arxiv.org/abs/2004.09578">CLOPS: Continual Learning of Physiological Signals</a> by Dani Kiyasseh, Tingting Zhu and David A Clifton. <em>arXiv</em>, 2020.  <br>
<button style="font-size:75%; line-height:15px" onclick="Kiyasseh2020ApplicationsFunction()" id="Kiyasseh2020Applications_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Kiyasseh2020ApplicationsFunction2()" id="Kiyasseh2020Applications_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Kiyasseh2020ApplicationsFunction3()" id="Kiyasseh2020Applications_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Kiyasseh2020Applications_more" style="display: none">
    @article{Kiyasseh2020,<br> archiveprefix = {arXiv},<br> arxivid = {2004.09578},<br> author = {Kiyasseh, Dani and Zhu, Tingting and Clifton, David A},<br> eprint = {2004.09578},<br> journal = {arXiv},<br> title = {CLOPS: Continual Learning of Physiological Signals},<br> url = {http://arxiv.org/abs/2004.09578},<br> year = {2020}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Kiyasseh2020Applications_more2" style="display: none">
    Deep learning algorithms are known to experience destructive interference when instances violate the assumption of being independent and identically distributed (i.i.d). This violation, however, is ubiquitous in clinical settings where data are streamed temporally and from a multitude of physiological sensors. To overcome this obstacle, we propose CLOPS, a healthcare-specific replay-based continual learning strategy. In three continual learning scenarios based on three publically-available datasets, we show that CLOPS can outperform its multi-task learning counterpart. Moreover, we propose end-to-end trainable parameters, which we term task-instance parameters, that can be used to quantify task difficulty and similarity. This quantification yields insights into both network interpretability and clinical applications, where task difficulty is poorly quantified.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Kiyasseh2020Applications_more3" style="display: none">
    N.A.
</span></p>
<script>
    function Kiyasseh2020ApplicationsFunction() {
      var moreText = document.getElementById("Kiyasseh2020Applications_more");
      var moreText2 = document.getElementById("Kiyasseh2020Applications_more2");
      var moreText3 = document.getElementById("Kiyasseh2020Applications_more3");
      var btnText = document.getElementById("Kiyasseh2020Applications_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Kiyasseh2020ApplicationsFunction2() {
      var moreText = document.getElementById("Kiyasseh2020Applications_more2");
      var moreText1 = document.getElementById("Kiyasseh2020Applications_more");
      var moreText3 = document.getElementById("Kiyasseh2020Applications_more3");
      var btnText = document.getElementById("Kiyasseh2020Applications_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Kiyasseh2020ApplicationsFunction3() {
      var moreText = document.getElementById("Kiyasseh2020Applications_more3");
      var moreText1 = document.getElementById("Kiyasseh2020Applications_more");
      var moreText2 = document.getElementById("Kiyasseh2020Applications_more2");
      var btnText = document.getElementById("Kiyasseh2020Applications_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="http://arxiv.org/abs/2006.10909">Neural Topic Modeling with Continual Lifelong Learning</a> by Pankaj Gupta, Yatin Chaudhary, Thomas Runkler and Hinrich Schütze. <em>ICML</em>, 2020. <span style='background-color:BlueViolet; padding: 2px; border-radius:4px; border: 1px solid black;'>[nlp]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Pankaj2020aApplicationsFunction()" id="Pankaj2020aApplications_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Pankaj2020aApplicationsFunction2()" id="Pankaj2020aApplications_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Pankaj2020aApplicationsFunction3()" id="Pankaj2020aApplications_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Pankaj2020aApplications_more" style="display: none">
    @inproceedings{Pankaj2020a,<br> archiveprefix = {arXiv},<br> arxivid = {2006.10909},<br> author = {Gupta, Pankaj and Chaudhary, Yatin and Runkler, Thomas and Schütze, Hinrich},<br> booktitle = {ICML},<br> eprint = {2006.10909},<br> file = {::},<br> keywords = {[nlp]},<br> mendeley-tags = {[nlp]},<br> month = {jun},<br> title = {Neural Topic Modeling with Continual Lifelong Learning},<br> url = {http://arxiv.org/abs/2006.10909},<br> year = {2020}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Pankaj2020aApplications_more2" style="display: none">
    Lifelong learning has recently attracted attention in building machine learning systems that continually accumulate and transfer knowledge to help future learning. Unsupervised topic modeling has been popularly used to discover topics from document collections. However, the application of topic modeling is challenging due to data sparsity, e.g., in a small collection of (short) documents and thus, generate incoherent topics and sub-optimal document representations. To address the problem, we propose a lifelong learning framework for neural topic modeling that can continuously process streams of document collections, accumulate topics and guide future topic modeling tasks by knowledge transfer from several sources to better deal with the sparse data. In the lifelong process, we particularly investigate jointly: (1) sharing generative homologies (latent topics) over lifetime to transfer prior knowledge, and (2) minimizing catastrophic forgetting to retain the past learning via novel selective data augmentation, co-training and topic regularization approaches. Given a stream of document collections, we apply the proposed Lifelong Neural Topic Modeling (LNTM) framework in modeling three sparse document collections as future tasks and demonstrate improved performance quantified by perplexity, topic coherence and information retrieval task.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Pankaj2020aApplications_more3" style="display: none">
    N.A.
</span></p>
<script>
    function Pankaj2020aApplicationsFunction() {
      var moreText = document.getElementById("Pankaj2020aApplications_more");
      var moreText2 = document.getElementById("Pankaj2020aApplications_more2");
      var moreText3 = document.getElementById("Pankaj2020aApplications_more3");
      var btnText = document.getElementById("Pankaj2020aApplications_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Pankaj2020aApplicationsFunction2() {
      var moreText = document.getElementById("Pankaj2020aApplications_more2");
      var moreText1 = document.getElementById("Pankaj2020aApplications_more");
      var moreText3 = document.getElementById("Pankaj2020aApplications_more3");
      var btnText = document.getElementById("Pankaj2020aApplications_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Pankaj2020aApplicationsFunction3() {
      var moreText = document.getElementById("Pankaj2020aApplications_more3");
      var moreText1 = document.getElementById("Pankaj2020aApplications_more");
      var moreText2 = document.getElementById("Pankaj2020aApplications_more2");
      var btnText = document.getElementById("Pankaj2020aApplications_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="http://arxiv.org/abs/2001.05922">Continual Learning for Domain Adaptation in Chest X-ray Classification</a> by Matthias Lenga, Heinrich Schulz and Axel Saalbach. <em>arXiv</em>, 1–11, 2020.  <br>
<button style="font-size:75%; line-height:15px" onclick="Lenga2020aApplicationsFunction()" id="Lenga2020aApplications_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Lenga2020aApplicationsFunction2()" id="Lenga2020aApplications_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Lenga2020aApplicationsFunction3()" id="Lenga2020aApplications_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Lenga2020aApplications_more" style="display: none">
    @article{Lenga2020a,<br> archiveprefix = {arXiv},<br> arxivid = {2001.05922},<br> author = {Lenga, Matthias and Schulz, Heinrich and Saalbach, Axel},<br> eprint = {2001.05922},<br> journal = {arXiv},<br> keywords = {catastrophic forgetting,chest x-ray,chestx-ray14,continual learning,convolutional neural networks,elastic weight consolidation,joint training,learning without forgetting,mimic-cxr},<br> pages = {1--11},<br> title = {Continual Learning for Domain Adaptation in Chest X-ray Classification},<br> url = {http://arxiv.org/abs/2001.05922},<br> year = {2020}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Lenga2020aApplications_more2" style="display: none">
    Over the last years, Deep Learning has been successfully applied to a broad range of medical applications. Especially in the context of chest X-ray classification, results have been reported which are on par, or even superior to experienced radiologists. Despite this success in controlled experimental environments, it has been noted that the ability of Deep Learning models to generalize to data from a new domain (with potentially different tasks) is often limited. In order to address this challenge, we investigate techniques from the field of Continual Learning (CL) including Joint Training (JT), Elastic Weight Consolidation (EWC) and Learning Without Forgetting (LWF). Using the ChestX-ray14 and the MIMIC-CXR datasets, we demonstrate empirically that these methods provide promising options to improve the performance of Deep Learning models on a target domain and to mitigate effectively catastrophic forgetting for the source domain. To this end, the best overall performance was obtained using JT, while for LWF competitive results could be achieved - even without accessing data from the source domain.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Lenga2020aApplications_more3" style="display: none">
    N.A.
</span></p>
<script>
    function Lenga2020aApplicationsFunction() {
      var moreText = document.getElementById("Lenga2020aApplications_more");
      var moreText2 = document.getElementById("Lenga2020aApplications_more2");
      var moreText3 = document.getElementById("Lenga2020aApplications_more3");
      var btnText = document.getElementById("Lenga2020aApplications_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Lenga2020aApplicationsFunction2() {
      var moreText = document.getElementById("Lenga2020aApplications_more2");
      var moreText1 = document.getElementById("Lenga2020aApplications_more");
      var moreText3 = document.getElementById("Lenga2020aApplications_more3");
      var btnText = document.getElementById("Lenga2020aApplications_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Lenga2020aApplicationsFunction3() {
      var moreText = document.getElementById("Lenga2020aApplications_more3");
      var moreText1 = document.getElementById("Lenga2020aApplications_more");
      var moreText2 = document.getElementById("Lenga2020aApplications_more2");
      var btnText = document.getElementById("Lenga2020aApplications_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="http://arxiv.org/abs/1903.00058">Non-Parametric Adaptation for Neural Machine Translation</a> by Ankur Bapna and Orhan Firat. <em>arXiv</em>, 2019. <span style='background-color:BlueViolet; padding: 2px; border-radius:4px; border: 1px solid black;'>[nlp]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Bapna2019aApplicationsFunction()" id="Bapna2019aApplications_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Bapna2019aApplicationsFunction2()" id="Bapna2019aApplications_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Bapna2019aApplicationsFunction3()" id="Bapna2019aApplications_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Bapna2019aApplications_more" style="display: none">
    @article{Bapna2019a,<br> archiveprefix = {arXiv},<br> arxivid = {1903.00058},<br> author = {Bapna, Ankur and Firat, Orhan},<br> eprint = {1903.00058},<br> journal = {arXiv},<br> keywords = {[nlp]},<br> mendeley-tags = {[nlp]},<br> title = {Non-Parametric Adaptation for Neural Machine Translation},<br> url = {http://arxiv.org/abs/1903.00058},<br> year = {2019}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Bapna2019aApplications_more2" style="display: none">
    Neural Networks trained with gradient descent are known to be susceptible to catastrophic forgetting caused by parameter shift during the training process. In the context of Neural Machine Translation (NMT) this results in poor performance on heterogeneous datasets and on sub-tasks like rare phrase translation. On the other hand, non-parametric approaches are immune to forgetting, perfectly complementing the generalization ability of NMT. However, attempts to combine non-parametric or retrieval based approaches with NMT have only been successful on narrow domains, possibly due to over-reliance on sentence level retrieval. We propose a novel n-gram level retrieval approach that relies on local phrase level similarities, allowing us to retrieve neighbors that are useful for translation even when overall sentence similarity is low. We complement this with an expressive neural network, allowing our model to extract information from the noisy retrieved context. We evaluate our semi-parametric NMT approach on a heterogeneous dataset composed of WMT, IWSLT, JRC-Acquis and OpenSubtitles, and demonstrate gains on all 4 evaluation sets. The semi-parametric nature of our approach opens the door for non-parametric domain adaptation, demonstrating strong inference-time adaptation performance on new domains without the need for any parameter updates.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Bapna2019aApplications_more3" style="display: none">
    N.A.
</span></p>
<script>
    function Bapna2019aApplicationsFunction() {
      var moreText = document.getElementById("Bapna2019aApplications_more");
      var moreText2 = document.getElementById("Bapna2019aApplications_more2");
      var moreText3 = document.getElementById("Bapna2019aApplications_more3");
      var btnText = document.getElementById("Bapna2019aApplications_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Bapna2019aApplicationsFunction2() {
      var moreText = document.getElementById("Bapna2019aApplications_more2");
      var moreText1 = document.getElementById("Bapna2019aApplications_more");
      var moreText3 = document.getElementById("Bapna2019aApplications_more3");
      var btnText = document.getElementById("Bapna2019aApplications_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Bapna2019aApplicationsFunction3() {
      var moreText = document.getElementById("Bapna2019aApplications_more3");
      var moreText1 = document.getElementById("Bapna2019aApplications_more");
      var moreText2 = document.getElementById("Bapna2019aApplications_more2");
      var btnText = document.getElementById("Bapna2019aApplications_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="http://arxiv.org/abs/1904.08039">A Multi-Task Learning Framework for Overcoming the Catastrophic Forgetting in Automatic Speech Recognition</a> by Jiabin Xue, Jiqing Han, Tieran Zheng, Xiang Gao and Jiaxing Guo. <em>arXiv</em>, 2019. <span style='background-color:#50EBEC; padding: 2px; border-radius:4px; border: 1px solid black;'>[audio]</span> <span style='background-color:AliceBlue; padding: 2px; border-radius:4px; border: 1px solid black;'>[rnn]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Xue2019aApplicationsFunction()" id="Xue2019aApplications_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Xue2019aApplicationsFunction2()" id="Xue2019aApplications_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Xue2019aApplicationsFunction3()" id="Xue2019aApplications_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Xue2019aApplications_more" style="display: none">
    @article{Xue2019a,<br> archiveprefix = {arXiv},<br> arxivid = {1904.08039},<br> author = {Xue, Jiabin and Han, Jiqing and Zheng, Tieran and Gao, Xiang and Guo, Jiaxing},<br> eprint = {1904.08039},<br> journal = {arXiv},<br> keywords = {[audio],[rnn]},<br> mendeley-tags = {[audio],[rnn]},<br> title = {A Multi-Task Learning Framework for Overcoming the Catastrophic Forgetting in Automatic Speech Recognition},<br> url = {http://arxiv.org/abs/1904.08039},<br> year = {2019}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Xue2019aApplications_more2" style="display: none">
    Recently, data-driven based Automatic Speech Recognition (ASR) systems have achieved state-of-the-art results. And transfer learning is often used when those existing systems are adapted to the target domain, e.g., fine-tuning, retraining. However, in the processes, the system parameters may well deviate too much from the previously learned parameters. Thus, it is difficult for the system training process to learn knowledge from target domains meanwhile not forgetting knowledge from the previous learning process, which is called as catastrophic forgetting (CF). In this paper, we attempt to solve the CF problem with the lifelong learning and propose a novel multi-task learning (MTL) training framework for ASR. It considers reserving original knowledge and learning new knowledge as two independent tasks, respectively. On the one hand, we constrain the new parameters not to deviate too far from the original parameters and punish the new system when forgetting original knowledge. On the other hand, we force the new system to solve new knowledge quickly. Then, a MTL mechanism is employed to get the balance between the two tasks. We applied our method to an End2End ASR task and obtained the best performance in both target and original datasets.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Xue2019aApplications_more3" style="display: none">
    N.A.
</span></p>
<script>
    function Xue2019aApplicationsFunction() {
      var moreText = document.getElementById("Xue2019aApplications_more");
      var moreText2 = document.getElementById("Xue2019aApplications_more2");
      var moreText3 = document.getElementById("Xue2019aApplications_more3");
      var btnText = document.getElementById("Xue2019aApplications_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Xue2019aApplicationsFunction2() {
      var moreText = document.getElementById("Xue2019aApplications_more2");
      var moreText1 = document.getElementById("Xue2019aApplications_more");
      var moreText3 = document.getElementById("Xue2019aApplications_more3");
      var btnText = document.getElementById("Xue2019aApplications_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Xue2019aApplicationsFunction3() {
      var moreText = document.getElementById("Xue2019aApplications_more3");
      var moreText1 = document.getElementById("Xue2019aApplications_more");
      var moreText2 = document.getElementById("Xue2019aApplications_more2");
      var btnText = document.getElementById("Xue2019aApplications_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="http://arxiv.org/abs/1904.09187">Continual Learning for Sentence Representations Using Conceptors</a> by Tianlin Liu, Lyle Ungar and João Sedoc. <em>NAACL</em>, 2019. <span style='background-color:BlueViolet; padding: 2px; border-radius:4px; border: 1px solid black;'>[nlp]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Liu2019aApplicationsFunction()" id="Liu2019aApplications_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Liu2019aApplicationsFunction2()" id="Liu2019aApplications_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Liu2019aApplicationsFunction3()" id="Liu2019aApplications_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Liu2019aApplications_more" style="display: none">
    @article{Liu2019a,<br> archiveprefix = {arXiv},<br> arxivid = {1904.09187},<br> author = {Liu, Tianlin and Ungar, Lyle and Sedoc, João},<br> eprint = {1904.09187},<br> journal = {NAACL},<br> keywords = {[nlp]},<br> mendeley-tags = {[nlp]},<br> title = {Continual Learning for Sentence Representations Using Conceptors},<br> url = {http://arxiv.org/abs/1904.09187},<br> year = {2019}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Liu2019aApplications_more2" style="display: none">
    Distributed representations of sentences have become ubiquitous in natural language processing tasks. In this paper, we consider a continual learning scenario for sentence representations: Given a sequence of corpora, we aim to optimize the sentence encoder with respect to the new corpus while maintaining its accuracy on the old corpora. To address this problem, we propose to initialize sentence encoders with the help of corpus-independent features, and then sequentially update sentence encoders using Boolean operations of conceptor matrices to learn corpus-dependent features. We evaluate our approach on semantic textual similarity tasks and show that our proposed sentence encoder can continually learn features from new corpora while retaining its competence on previously encountered corpora.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Liu2019aApplications_more3" style="display: none">
    N.A.
</span></p>
<script>
    function Liu2019aApplicationsFunction() {
      var moreText = document.getElementById("Liu2019aApplications_more");
      var moreText2 = document.getElementById("Liu2019aApplications_more2");
      var moreText3 = document.getElementById("Liu2019aApplications_more3");
      var btnText = document.getElementById("Liu2019aApplications_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Liu2019aApplicationsFunction2() {
      var moreText = document.getElementById("Liu2019aApplications_more2");
      var moreText1 = document.getElementById("Liu2019aApplications_more");
      var moreText3 = document.getElementById("Liu2019aApplications_more3");
      var btnText = document.getElementById("Liu2019aApplications_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Liu2019aApplicationsFunction3() {
      var moreText = document.getElementById("Liu2019aApplications_more3");
      var moreText1 = document.getElementById("Liu2019aApplications_more");
      var moreText2 = document.getElementById("Liu2019aApplications_more2");
      var btnText = document.getElementById("Liu2019aApplications_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="https://www.aclweb.org/anthology/N19-1209.pdf">Overcoming Catastrophic Forgetting During Domain Adaptation of Neural Machine Translation</a> by Brian Thompson, Jeremy Gwinnup, Huda Khayrallah, Kevin Duh and Philipp Koehn. <em>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics</em>, 2062–2068, 2019. <span style='background-color:BlueViolet; padding: 2px; border-radius:4px; border: 1px solid black;'>[nlp]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Thompson2019aApplicationsFunction()" id="Thompson2019aApplications_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Thompson2019aApplicationsFunction2()" id="Thompson2019aApplications_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Thompson2019aApplicationsFunction3()" id="Thompson2019aApplications_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Thompson2019aApplications_more" style="display: none">
    @inproceedings{Thompson2019a,<br> author = {Thompson, Brian and Gwinnup, Jeremy and Khayrallah, Huda and Duh, Kevin and Koehn, Philipp},<br> booktitle = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics},<br> file = {::},<br> keywords = {[nlp]},<br> mendeley-tags = {[nlp]},<br> pages = {2062--2068},<br> title = {Overcoming Catastrophic Forgetting During Domain Adaptation of Neural Machine Translation},<br> url = {https://www.aclweb.org/anthology/N19-1209.pdf},<br> year = {2019}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Thompson2019aApplications_more2" style="display: none">
    Continued training is an effective method for domain adaptation in neural machine translation. However, in-domain gains from adaptation come at the expense of general-domain performance. In this work, we interpret the drop in general-domain performance as catastrophic forgetting of general-domain knowledge. To mitigate it, we adapt Elastic Weight Consolidation (EWC)-a machine learning method for learning a new task without forgetting previous tasks. Our method retains the majority of general-domain performance lost in continued training without degrading in-domain performance, outperforming the previous state-of-the-art. We also explore the full range of general-domain performance available when some in-domain degradation is acceptable .
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Thompson2019aApplications_more3" style="display: none">
    N.A.
</span></p>
<script>
    function Thompson2019aApplicationsFunction() {
      var moreText = document.getElementById("Thompson2019aApplications_more");
      var moreText2 = document.getElementById("Thompson2019aApplications_more2");
      var moreText3 = document.getElementById("Thompson2019aApplications_more3");
      var btnText = document.getElementById("Thompson2019aApplications_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Thompson2019aApplicationsFunction2() {
      var moreText = document.getElementById("Thompson2019aApplications_more2");
      var moreText1 = document.getElementById("Thompson2019aApplications_more");
      var moreText3 = document.getElementById("Thompson2019aApplications_more3");
      var btnText = document.getElementById("Thompson2019aApplications_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Thompson2019aApplicationsFunction3() {
      var moreText = document.getElementById("Thompson2019aApplications_more3");
      var moreText1 = document.getElementById("Thompson2019aApplications_more");
      var moreText2 = document.getElementById("Thompson2019aApplications_more2");
      var btnText = document.getElementById("Thompson2019aApplications_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="http://arxiv.org/abs/1906.01076">Episodic Memory in Lifelong Language Learning</a> by Cyprien de Masson D’Autume, Sebastian Ruder, Lingpeng Kong and Dani Yogatama. <em>NeurIPS</em>, 2019. <span style='background-color:BlueViolet; padding: 2px; border-radius:4px; border: 1px solid black;'>[nlp]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="DAutume2019aApplicationsFunction()" id="DAutume2019aApplications_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="DAutume2019aApplicationsFunction2()" id="DAutume2019aApplications_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="DAutume2019aApplicationsFunction3()" id="DAutume2019aApplications_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="DAutume2019aApplications_more" style="display: none">
    @article{DAutume2019a,<br> archiveprefix = {arXiv},<br> arxivid = {1906.01076},<br> author = {D'Autume, Cyprien de Masson and Ruder, Sebastian and Kong, Lingpeng and Yogatama, Dani},<br> eprint = {1906.01076},<br> journal = {NeurIPS},<br> keywords = {[nlp]},<br> mendeley-tags = {[nlp]},<br> month = {jun},<br> title = {Episodic Memory in Lifelong Language Learning},<br> url = {http://arxiv.org/abs/1906.01076},<br> year = {2019}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="DAutume2019aApplications_more2" style="display: none">
    We introduce a lifelong language learning setup where a model needs to learn from a stream of text examples without any dataset identifier. We propose an episodic memory model that performs sparse experience replay and local adaptation to mitigate catastrophic forgetting in this setup. Experiments on text classification and question answering demonstrate the complementary benefits of sparse experience replay and local adaptation to allow the model to continuously learn from new datasets. We also show that the space complexity of the episodic memory module can be reduced significantly ($∼$50-90%) by randomly choosing which examples to store in memory with a minimal decrease in performance. We consider an episodic memory component as a crucial building block of general linguistic intelligence and see our model as a first step in that direction.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="DAutume2019aApplications_more3" style="display: none">
    N.A.
</span></p>
<script>
    function DAutume2019aApplicationsFunction() {
      var moreText = document.getElementById("DAutume2019aApplications_more");
      var moreText2 = document.getElementById("DAutume2019aApplications_more2");
      var moreText3 = document.getElementById("DAutume2019aApplications_more3");
      var btnText = document.getElementById("DAutume2019aApplications_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function DAutume2019aApplicationsFunction2() {
      var moreText = document.getElementById("DAutume2019aApplications_more2");
      var moreText1 = document.getElementById("DAutume2019aApplications_more");
      var moreText3 = document.getElementById("DAutume2019aApplications_more3");
      var btnText = document.getElementById("DAutume2019aApplications_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function DAutume2019aApplicationsFunction3() {
      var moreText = document.getElementById("DAutume2019aApplications_more3");
      var moreText1 = document.getElementById("DAutume2019aApplications_more");
      var moreText2 = document.getElementById("DAutume2019aApplications_more2");
      var btnText = document.getElementById("DAutume2019aApplications_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="http://arxiv.org/abs/1712.09943">Toward Continual Learning for Conversational Agents</a> by  and Sungjin Lee. <em>arXiv</em>, 2018. <span style='background-color:BlueViolet; padding: 2px; border-radius:4px; border: 1px solid black;'>[nlp]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="lee2018aApplicationsFunction()" id="lee2018aApplications_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="lee2018aApplicationsFunction2()" id="lee2018aApplications_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="lee2018aApplicationsFunction3()" id="lee2018aApplications_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="lee2018aApplications_more" style="display: none">
    @article{lee2018a,<br> annote = {arXiv: 1712.09943},<br> author = {Lee, Sungjin},<br> journal = {arXiv},<br> keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction,[nlp],chatbot,conversation,conversational agent,ewc,lstm},<br> mendeley-tags = {[nlp]},<br> month = {jan},<br> title = {Toward Continual Learning for Conversational Agents},<br> url = {http://arxiv.org/abs/1712.09943},<br> year = {2018}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="lee2018aApplications_more2" style="display: none">
    While end-to-end neural conversation models have led to promising advances in reducing hand-crafted features and errors induced by the traditional complex system architecture, they typically require an enormous amount of data due to the lack of modularity. Previous studies adopted a hybrid approach with knowledge-based components either to abstract out domain-specific information or to augment data to cover more diverse patterns. On the contrary, we propose to directly address the problem using recent developments in the space of continual learning for neural models. Specifically, we adopt a domain-independent neural conversational model and introduce a novel neural continual learning algorithm that allows a conversational agent to accumulate skills across different tasks in a data-efficient way. To the best of our knowledge, this is the first work that applies continual learning to conversation systems. We verified the efficacy of our method through a conversational skill transfer from either synthetic dialogs or human-human dialogs to human-computer conversations in a customer support domain.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="lee2018aApplications_more3" style="display: none">
    arXiv: 1712.09943
</span></p>
<script>
    function lee2018aApplicationsFunction() {
      var moreText = document.getElementById("lee2018aApplications_more");
      var moreText2 = document.getElementById("lee2018aApplications_more2");
      var moreText3 = document.getElementById("lee2018aApplications_more3");
      var btnText = document.getElementById("lee2018aApplications_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function lee2018aApplicationsFunction2() {
      var moreText = document.getElementById("lee2018aApplications_more2");
      var moreText1 = document.getElementById("lee2018aApplications_more");
      var moreText3 = document.getElementById("lee2018aApplications_more3");
      var btnText = document.getElementById("lee2018aApplications_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function lee2018aApplicationsFunction3() {
      var moreText = document.getElementById("lee2018aApplications_more3");
      var moreText1 = document.getElementById("lee2018aApplications_more");
      var moreText2 = document.getElementById("lee2018aApplications_more2");
      var btnText = document.getElementById("lee2018aApplications_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
</ul>
</div>
<div class="section" id="neuroscience">
<h3>Neuroscience<a class="headerlink" href="#neuroscience" title="Permalink to this headline">¶</a></h3>
<p>In this section we maintain a list of all Neuroscience papers that can be related (and useful) for continual machine learning.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://www.biorxiv.org/content/10.1101/569038v1">Can sleep protect memories from catastrophic forgetting?</a> by Oscar C Gonzalez, Yury Sokolov, Giri Krishnan and Maxim Bazhenov. <em>bioRxiv</em>, 569038, 2019.  <br>
<button style="font-size:75%; line-height:15px" onclick="Gonzalez2019aNeuroscienceFunction()" id="Gonzalez2019aNeuroscience_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Gonzalez2019aNeuroscienceFunction2()" id="Gonzalez2019aNeuroscience_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Gonzalez2019aNeuroscienceFunction3()" id="Gonzalez2019aNeuroscience_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Gonzalez2019aNeuroscience_more" style="display: none">
    @article{Gonzalez2019a,<br> author = {Gonzalez, Oscar C and Sokolov, Yury and Krishnan, Giri and Bazhenov, Maxim},<br> doi = {10.1101/569038},<br> journal = {bioRxiv},<br> keywords = {catastrophic,continual learning,memory consolidation,neural network,sleep},<br> pages = {569038},<br> title = {Can sleep protect memories from catastrophic forgetting?},<br> url = {https://www.biorxiv.org/content/10.1101/569038v1},<br> year = {2019}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Gonzalez2019aNeuroscience_more2" style="display: none">
    Previously encoded memories can be damaged by encoding of new memories, especially when they are relevant to the new data and hence can be disrupted by new training - a phenomenon called "catastrophic forgetting." Human and animal brains are capable of continual learning, allowing them to learn from past experience and to integrate newly acquired information with previously stored memories. A range of empirical data suggest important role of sleep in consolidation of recent memories and protection of the past knowledge from catastrophic forgetting. To explore potential mechanisms of how sleep can enable continual learning in neuronal networks, we developed a biophysically-realistic thalamocortical network model where we could train multiple memories with different degree of interference. We found that in a wake-like state of the model, training of a "new" memory that overlaps with previously stored "old" memory results in degradation of the old memory. Simulating NREM sleep state immediately after new learning led to replay of both old and new memories - this protected old memory from forgetting and ultimately enhanced both memories. The effect of sleep was similar to the interleaved training of the old and new memories. The study revealed that the network slow-wave oscillatory activity during simulated deep sleep leads to a complex reorganization of the synaptic connectivity matrix that maximizes separation between groups of synapses responsible for conflicting memories in the overlapping population of neurons. The study predicts that sleep may play a protective role against catastrophic forgetting and enables brain networks to undergo continual learning.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Gonzalez2019aNeuroscience_more3" style="display: none">
    N.A.
</span></p>
<script>
    function Gonzalez2019aNeuroscienceFunction() {
      var moreText = document.getElementById("Gonzalez2019aNeuroscience_more");
      var moreText2 = document.getElementById("Gonzalez2019aNeuroscience_more2");
      var moreText3 = document.getElementById("Gonzalez2019aNeuroscience_more3");
      var btnText = document.getElementById("Gonzalez2019aNeuroscience_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Gonzalez2019aNeuroscienceFunction2() {
      var moreText = document.getElementById("Gonzalez2019aNeuroscience_more2");
      var moreText1 = document.getElementById("Gonzalez2019aNeuroscience_more");
      var moreText3 = document.getElementById("Gonzalez2019aNeuroscience_more3");
      var btnText = document.getElementById("Gonzalez2019aNeuroscience_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Gonzalez2019aNeuroscienceFunction3() {
      var moreText = document.getElementById("Gonzalez2019aNeuroscience_more3");
      var moreText1 = document.getElementById("Gonzalez2019aNeuroscience_more");
      var moreText2 = document.getElementById("Gonzalez2019aNeuroscience_more2");
      var btnText = document.getElementById("Gonzalez2019aNeuroscience_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p>Avoiding Catastrophic Forgetting by  and Michael E. Hasselmo. <em>Trends in Cognitive Sciences</em>, 407–408, 2017.  <br>
<button style="font-size:75%; line-height:15px" onclick="Hasselmo2017NeuroscienceFunction()" id="Hasselmo2017Neuroscience_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Hasselmo2017NeuroscienceFunction2()" id="Hasselmo2017Neuroscience_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Hasselmo2017NeuroscienceFunction3()" id="Hasselmo2017Neuroscience_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Hasselmo2017Neuroscience_more" style="display: none">
    @article{Hasselmo2017,<br> author = {Hasselmo, Michael E.},<br> doi = {10.1016/j.tics.2017.04.001},<br> file = {::},<br> issn = {1879307X},<br> journal = {Trends in Cognitive Sciences},<br> keywords = {hasselmo2017a},<br> month = {jun},<br> number = {6},<br> pages = {407--408},<br> publisher = {Elsevier Ltd},<br> title = {Avoiding Catastrophic Forgetting},<br> volume = {21},<br> year = {2017}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Hasselmo2017Neuroscience_more2" style="display: none">
    Humans regularly perform new learning without losing memory for previous information, but neural network models suffer from the phenomenon of catastrophic forgetting in which new learning impairs prior function. A recent article presents an algorithm that spares learning at synapses important for previously learned function, reducing catastrophic forgetting.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Hasselmo2017Neuroscience_more3" style="display: none">
    N.A.
</span></p>
<script>
    function Hasselmo2017NeuroscienceFunction() {
      var moreText = document.getElementById("Hasselmo2017Neuroscience_more");
      var moreText2 = document.getElementById("Hasselmo2017Neuroscience_more2");
      var moreText3 = document.getElementById("Hasselmo2017Neuroscience_more3");
      var btnText = document.getElementById("Hasselmo2017Neuroscience_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Hasselmo2017NeuroscienceFunction2() {
      var moreText = document.getElementById("Hasselmo2017Neuroscience_more2");
      var moreText1 = document.getElementById("Hasselmo2017Neuroscience_more");
      var moreText3 = document.getElementById("Hasselmo2017Neuroscience_more3");
      var btnText = document.getElementById("Hasselmo2017Neuroscience_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Hasselmo2017NeuroscienceFunction3() {
      var moreText = document.getElementById("Hasselmo2017Neuroscience_more3");
      var moreText1 = document.getElementById("Hasselmo2017Neuroscience_more");
      var moreText2 = document.getElementById("Hasselmo2017Neuroscience_more2");
      var btnText = document.getElementById("Hasselmo2017Neuroscience_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p>The Organization of Behavior: A Neuropsychological Theory by  and D O Hebb. <em>Lawrence Erlbaum</em>, 2002. <span style='background-color:#99C68E; padding: 2px; border-radius:4px; border: 1px solid black;'>[hebbian]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Hebb2005aNeuroscienceFunction()" id="Hebb2005aNeuroscience_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Hebb2005aNeuroscienceFunction2()" id="Hebb2005aNeuroscience_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Hebb2005aNeuroscienceFunction3()" id="Hebb2005aNeuroscience_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Hebb2005aNeuroscience_more" style="display: none">
    @book{Hebb2005a,<br> author = {Hebb, D O},<br> booktitle = {Lawrence Erlbaum},<br> isbn = {978-1-135-63191-8},<br> keywords = {Psychology / Cognitive Psychology & Cognition,Psychology / General,Psychology / Neuropsychology,Psychology / Physiological Psychology,[hebbian]},<br> language = {en},<br> mendeley-tags = {[hebbian]},<br> month = {apr},<br> publisher = {Psychology Press},<br> shorttitle = {The Organization of Behavior},<br> title = {The Organization of Behavior: A Neuropsychological Theory},<br> year = {2002}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Hebb2005aNeuroscience_more2" style="display: none">
    Since its publication in 1949, D.O. Hebb's, The Organization of Behavior has been one of the most influential books in the fields of psychology and neuroscience. However, the original edition has been unavailable since 1966, ensuring that Hebb's comment that a classic normally means "cited but not read" is true in his case. This new edition rectifies a long-standing problem for behavioral neuroscientists–the inability to obtain one of the most cited publications in the field. The Organization of Behavior played a significant part in stimulating the investigation of the neural foundations of behavior and continues to be inspiring because it provides a general framework for relating behavior to synaptic organization through the dynamics of neural networks. D.O. Hebb was also the first to examine the mechanisms by which environment and experience can influence brain structure and function, and his ideas formed the basis for work on enriched environments as stimulants for behavioral development. References to Hebb, the Hebbian cell assembly, the Hebb synapse, and the Hebb rule increase each year. These forceful ideas of 1949 are now applied in engineering, robotics, and computer science, as well as neurophysiology, neuroscience, and psychology–a tribute to Hebb's foresight in developing a foundational neuropsychological theory of the organization of behavior.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Hebb2005aNeuroscience_more3" style="display: none">
    N.A.
</span></p>
<script>
    function Hebb2005aNeuroscienceFunction() {
      var moreText = document.getElementById("Hebb2005aNeuroscience_more");
      var moreText2 = document.getElementById("Hebb2005aNeuroscience_more2");
      var moreText3 = document.getElementById("Hebb2005aNeuroscience_more3");
      var btnText = document.getElementById("Hebb2005aNeuroscience_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Hebb2005aNeuroscienceFunction2() {
      var moreText = document.getElementById("Hebb2005aNeuroscience_more2");
      var moreText1 = document.getElementById("Hebb2005aNeuroscience_more");
      var moreText3 = document.getElementById("Hebb2005aNeuroscience_more3");
      var btnText = document.getElementById("Hebb2005aNeuroscience_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Hebb2005aNeuroscienceFunction3() {
      var moreText = document.getElementById("Hebb2005aNeuroscience_more3");
      var moreText1 = document.getElementById("Hebb2005aNeuroscience_more");
      var moreText2 = document.getElementById("Hebb2005aNeuroscience_more2");
      var btnText = document.getElementById("Hebb2005aNeuroscience_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="https://www.nature.com/articles/nn1100_1178">Synaptic plasticity: taming the beast</a> by L F Abbott and Sacha B Nelson. <em>Nature Neuroscience</em>, 1178–1183, 2000. <span style='background-color:#99C68E; padding: 2px; border-radius:4px; border: 1px solid black;'>[hebbian]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Abbott2000aNeuroscienceFunction()" id="Abbott2000aNeuroscience_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Abbott2000aNeuroscienceFunction2()" id="Abbott2000aNeuroscience_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Abbott2000aNeuroscienceFunction3()" id="Abbott2000aNeuroscience_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Abbott2000aNeuroscience_more" style="display: none">
    @article{Abbott2000a,<br> author = {Abbott, L F and Nelson, Sacha B},<br> doi = {10.1038/81453},<br> issn = {1546-1726},<br> journal = {Nature Neuroscience},<br> keywords = {[hebbian]},<br> language = {en},<br> mendeley-tags = {[hebbian]},<br> month = {nov},<br> number = {11},<br> pages = {1178--1183},<br> shorttitle = {Synaptic plasticity},<br> title = {Synaptic plasticity: taming the beast},<br> url = {https://www.nature.com/articles/nn1100_1178},<br> volume = {3},<br> year = {2000}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Abbott2000aNeuroscience_more2" style="display: none">
    Synaptic plasticity provides the basis for most models of learning, memory and development in neural circuits. To generate realistic results, synapse-specific Hebbian forms of plasticity, such as long-term potentiation and depression, must be augmented by global processes that regulate overall levels of neuronal and network activity. Regulatory processes are often as important as the more intensively studied Hebbian processes in determining the consequences of synaptic plasticity for network function. Recent experimental results suggest several novel mechanisms for regulating levels of activity in conjunction with Hebbian synaptic modification. We review three of them—synaptic scaling, spike-timing dependent plasticity and synaptic redistribution—and discuss their functional implications.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Abbott2000aNeuroscience_more3" style="display: none">
    N.A.
</span></p>
<script>
    function Abbott2000aNeuroscienceFunction() {
      var moreText = document.getElementById("Abbott2000aNeuroscience_more");
      var moreText2 = document.getElementById("Abbott2000aNeuroscience_more2");
      var moreText3 = document.getElementById("Abbott2000aNeuroscience_more3");
      var btnText = document.getElementById("Abbott2000aNeuroscience_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Abbott2000aNeuroscienceFunction2() {
      var moreText = document.getElementById("Abbott2000aNeuroscience_more2");
      var moreText1 = document.getElementById("Abbott2000aNeuroscience_more");
      var moreText3 = document.getElementById("Abbott2000aNeuroscience_more3");
      var btnText = document.getElementById("Abbott2000aNeuroscience_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Abbott2000aNeuroscienceFunction3() {
      var moreText = document.getElementById("Abbott2000aNeuroscience_more3");
      var moreText1 = document.getElementById("Abbott2000aNeuroscience_more");
      var moreText2 = document.getElementById("Abbott2000aNeuroscience_more2");
      var btnText = document.getElementById("Abbott2000aNeuroscience_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p>Negative transfer errors in sequential cognitive skills: strong-but-wrong sequence application by Woltz Dan, Gardner Michael and Bell Brian. <em>Journal of Experimental Psychology. Learning, Memory, and Cognition</em>, 601–625, 2000.  <br>
<button style="font-size:75%; line-height:15px" onclick="Dan2000aNeuroscienceFunction()" id="Dan2000aNeuroscience_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Dan2000aNeuroscienceFunction2()" id="Dan2000aNeuroscience_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Dan2000aNeuroscienceFunction3()" id="Dan2000aNeuroscience_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Dan2000aNeuroscience_more" style="display: none">
    @article{Dan2000a,<br> author = {Dan, Woltz and Michael, Gardner and Brian, Bell},<br> issn = {0278-7393},<br> journal = {Journal of Experimental Psychology. Learning, Memory, and Cognition},<br> language = {eng},<br> month = {may},<br> number = {3},<br> pages = {601--625},<br> shorttitle = {Negative transfer errors in sequential cognitive s},<br> title = {Negative transfer errors in sequential cognitive skills: strong-but-wrong sequence application},<br> volume = {26},<br> year = {2000}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Dan2000aNeuroscience_more2" style="display: none">
    Three experiments investigated the role of processing sequence knowledge in negative transfer within multistep cognitive skills. In Experiments 1 and 2, more training resulted in higher error rates when new processing sequences that resembled familiar ones were introduced in transfer. Transfer error responses were executed with the same speed as correct responses to familiar sequence trials, and the errors appeared to be undetected by the performers. Experiment 3 tested whether the effects of sequence learning were attributable to explicit or implicit knowledge of processing sequences. Evidence favored the implicit learning interpretation. Findings are discussed in relationship to earlier demonstrations of the einstellung effect and to current taxonomic theories of human error.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Dan2000aNeuroscience_more3" style="display: none">
    N.A.
</span></p>
<script>
    function Dan2000aNeuroscienceFunction() {
      var moreText = document.getElementById("Dan2000aNeuroscience_more");
      var moreText2 = document.getElementById("Dan2000aNeuroscience_more2");
      var moreText3 = document.getElementById("Dan2000aNeuroscience_more3");
      var btnText = document.getElementById("Dan2000aNeuroscience_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Dan2000aNeuroscienceFunction2() {
      var moreText = document.getElementById("Dan2000aNeuroscience_more2");
      var moreText1 = document.getElementById("Dan2000aNeuroscience_more");
      var moreText3 = document.getElementById("Dan2000aNeuroscience_more3");
      var btnText = document.getElementById("Dan2000aNeuroscience_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Dan2000aNeuroscienceFunction3() {
      var moreText = document.getElementById("Dan2000aNeuroscience_more3");
      var moreText1 = document.getElementById("Dan2000aNeuroscience_more");
      var moreText2 = document.getElementById("Dan2000aNeuroscience_more2");
      var btnText = document.getElementById("Dan2000aNeuroscience_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="http://www.ncbi.nlm.nih.gov/pubmed/2186426">Connectionist models of recognition memory: constraints imposed by learning and forgetting functions.</a> by  and R Ratcliff. <em>Psychological review</em>, 285–308, 1990.  <br>
<button style="font-size:75%; line-height:15px" onclick="Ratcliff1990aNeuroscienceFunction()" id="Ratcliff1990aNeuroscience_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Ratcliff1990aNeuroscienceFunction2()" id="Ratcliff1990aNeuroscience_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Ratcliff1990aNeuroscienceFunction3()" id="Ratcliff1990aNeuroscience_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Ratcliff1990aNeuroscience_more" style="display: none">
    @article{Ratcliff1990a,<br> author = {Ratcliff, R},<br> issn = {0033-295X},<br> journal = {Psychological review},<br> month = {apr},<br> number = {2},<br> pages = {285--308},<br> pmid = {2186426},<br> title = {Connectionist models of recognition memory: constraints imposed by learning and forgetting functions.},<br> url = {http://www.ncbi.nlm.nih.gov/pubmed/2186426},<br> volume = {97},<br> year = {1990}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Ratcliff1990aNeuroscience_more2" style="display: none">
    Multilayer connectionist models of memory based on the encoder model using the backpropagation learning rule are evaluated. The models are applied to standard recognition memory procedures in which items are studied sequentially and then tested for retention. Sequential learning in these models leads to 2 major problems. First, well-learned information is forgotten rapidly as new information is learned. Second, discrimination between studied items and new items either decreases or is nonmonotonic as a function of learning. To address these problems, manipulations of the network within the multilayer model and several variants of the multilayer model were examined, including a model with prelearned memory and a context model, but none solved the problems. The problems discussed provide limitations on connectionist models applied to human memory and in tasks where information to be learned is not all available during learning.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Ratcliff1990aNeuroscience_more3" style="display: none">
    N.A.
</span></p>
<script>
    function Ratcliff1990aNeuroscienceFunction() {
      var moreText = document.getElementById("Ratcliff1990aNeuroscience_more");
      var moreText2 = document.getElementById("Ratcliff1990aNeuroscience_more2");
      var moreText3 = document.getElementById("Ratcliff1990aNeuroscience_more3");
      var btnText = document.getElementById("Ratcliff1990aNeuroscience_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Ratcliff1990aNeuroscienceFunction2() {
      var moreText = document.getElementById("Ratcliff1990aNeuroscience_more2");
      var moreText1 = document.getElementById("Ratcliff1990aNeuroscience_more");
      var moreText3 = document.getElementById("Ratcliff1990aNeuroscience_more3");
      var btnText = document.getElementById("Ratcliff1990aNeuroscience_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Ratcliff1990aNeuroscienceFunction3() {
      var moreText = document.getElementById("Ratcliff1990aNeuroscience_more3");
      var moreText1 = document.getElementById("Ratcliff1990aNeuroscience_more");
      var moreText2 = document.getElementById("Ratcliff1990aNeuroscience_more2");
      var btnText = document.getElementById("Ratcliff1990aNeuroscience_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
</ul>
</div>
<div class="section" id="robotics">
<h3>Robotics<a class="headerlink" href="#robotics" title="Permalink to this headline">¶</a></h3>
<p>In this section we maintain a list of all Robotics papers that can be related to continual learning.</p>
<ul class="simple">
<li><p><a class="reference external" href="http://www.sciencedirect.com/science/article/pii/S1566253519307377">Continual learning for robotics: Definition, framework, learning strategies, opportunities and challenges</a> by Timothée Lesort, Vincenzo Lomonaco, Andrei Stoian, Davide Maltoni, David Filliat and Natalia Díaz-Rodrǵuez. <em>Information Fusion</em>, 52–68, 2020. <span style='background-color:Tomato; padding: 2px; border-radius:4px; border: 1px solid black;'>[framework]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Lesort2020aRoboticsFunction()" id="Lesort2020aRobotics_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Lesort2020aRoboticsFunction2()" id="Lesort2020aRobotics_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Lesort2020aRoboticsFunction3()" id="Lesort2020aRobotics_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Lesort2020aRobotics_more" style="display: none">
    @article{Lesort2020a,<br> annote = {Overview of a CL framework for applications in robotic, together with discussion of existing CL strategies and techniques.},<br> author = {Lesort, Timothée and Lomonaco, Vincenzo and Stoian, Andrei and Maltoni, Davide and Filliat, David and Díaz-Rodr\ǵuez, Natalia},<br> doi = {10.1016/j.inffus.2019.12.004},<br> issn = {1566-2535},<br> journal = {Information Fusion},<br> keywords = {Catastrophic Forgetting,Continual Learning,Deep Learning,Lifelong Learning,Reinforcement Learning,Robotics,[framework]},<br> language = {en},<br> mendeley-tags = {[framework]},<br> month = {jun},<br> pages = {52--68},<br> shorttitle = {Continual learning for robotics},<br> title = {Continual learning for robotics: Definition, framework, learning strategies, opportunities and challenges},<br> url = {http://www.sciencedirect.com/science/article/pii/S1566253519307377},<br> volume = {58},<br> year = {2020}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Lesort2020aRobotics_more2" style="display: none">
    Continual learning (CL) is a particular machine learning paradigm where the data distribution and learning objective change through time, or where all the training data and objective criteria are never available at once. The evolution of the learning process is modeled by a sequence of learning experiences where the goal is to be able to learn new skills all along the sequence without forgetting what has been previously learned. CL can be seen as an online learning where knowledge fusion needs to take place in order to learn from streams of data presented sequentially in time. Continual learning also aims at the same time at optimizing the memory, the computation power and the speed during the learning process. An important challenge for machine learning is not necessarily finding solutions that work in the real world but rather finding stable algorithms that can learn in real world. Hence, the ideal approach would be tackling the real world in a embodied platform: an autonomous agent. Continual learning would then be effective in an autonomous agent or robot, which would learn autonomously through time about the external world, and incrementally develop a set of complex skills and knowledge.Robotic agents have to learn to adapt and interact with their environment using a continuous stream of observations. Some recent approaches aim at tackling continual learning for robotics, but most recent papers on continual learning only experiment approaches in simulation or with static datasets. Unfortunately, the evaluation of those algorithms does not provide insights on whether their solutions may help continual learning in the context of robotics. This paper aims at reviewing the existing state of the art of continual learning, summarizing existing benchmarks and metrics, and proposing a framework for presenting and evaluating both robotics and non robotics approaches in a way that makes transfer between both fields easier. We put light on continual learning in the context of robotics to create connections between fields and normalize approaches.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Lesort2020aRobotics_more3" style="display: none">
    Overview of a CL framework for applications in robotic, together with discussion of existing CL strategies and techniques.
</span></p>
<script>
    function Lesort2020aRoboticsFunction() {
      var moreText = document.getElementById("Lesort2020aRobotics_more");
      var moreText2 = document.getElementById("Lesort2020aRobotics_more2");
      var moreText3 = document.getElementById("Lesort2020aRobotics_more3");
      var btnText = document.getElementById("Lesort2020aRobotics_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Lesort2020aRoboticsFunction2() {
      var moreText = document.getElementById("Lesort2020aRobotics_more2");
      var moreText1 = document.getElementById("Lesort2020aRobotics_more");
      var moreText3 = document.getElementById("Lesort2020aRobotics_more3");
      var btnText = document.getElementById("Lesort2020aRobotics_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Lesort2020aRoboticsFunction3() {
      var moreText = document.getElementById("Lesort2020aRobotics_more3");
      var moreText1 = document.getElementById("Lesort2020aRobotics_more");
      var moreText2 = document.getElementById("Lesort2020aRobotics_more2");
      var btnText = document.getElementById("Lesort2020aRobotics_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
</ul>
</div>
<div class="section" id="others">
<h3>Others<a class="headerlink" href="#others" title="Permalink to this headline">¶</a></h3>
<p>In this section we list all the other papers not appearing in at least one of the above sections.</p>
<ul class="simple">
<li><p><a class="reference external" href="http://arxiv.org/abs/2001.08650">Structured Compression and Sharing of Representational Space for Continual Learning</a> by Gobinda Saha, Isha Garg, Aayush Ankit and Kaushik Roy. <em>arXiv</em>, 2020. <span style='background-color:DarkCyan; padding: 2px; border-radius:4px; border: 1px solid black;'>[cifar]</span> <span style='background-color:MediumAquaMarine; padding: 2px; border-radius:4px; border: 1px solid black;'>[mnist]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Saha2020aOthersFunction()" id="Saha2020aOthers_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Saha2020aOthersFunction2()" id="Saha2020aOthers_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Saha2020aOthersFunction3()" id="Saha2020aOthers_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Saha2020aOthers_more" style="display: none">
    @article{Saha2020a,<br> archiveprefix = {arXiv},<br> arxivid = {2001.08650},<br> author = {Saha, Gobinda and Garg, Isha and Ankit, Aayush and Roy, Kaushik},<br> eprint = {2001.08650},<br> journal = {arXiv},<br> keywords = {[cifar],[mnist]},<br> mendeley-tags = {[cifar],[mnist]},<br> title = {Structured Compression and Sharing of Representational Space for Continual Learning},<br> url = {http://arxiv.org/abs/2001.08650},<br> year = {2020}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Saha2020aOthers_more2" style="display: none">
    Humans are skilled at learning adaptively and efficiently throughout their lives, but learning tasks incrementally causes artificial neural networks to overwrite relevant information learned about older tasks, resulting in 'Catastrophic Forgetting'. Efforts to overcome this phenomenon suffer from poor utilization of resources in many ways, such as through the need to save older data or parametric importance scores, or to grow the network architecture. We propose an algorithm that enables a network to learn continually and efficiently by partitioning the representational space into a Core space, that contains the condensed information from previously learned tasks, and a Residual space, which is akin to a scratch space for learning the current task. The information in the Residual space is then compressed using Principal Component Analysis and added to the Core space, freeing up parameters for the next task. We evaluate our algorithm on P-MNIST, CIFAR-10 and CIFAR-100 datasets. We achieve comparable accuracy to state-of-the-art methods while overcoming the problem of catastrophic forgetting completely. Additionally, we get up to 4.5x improvement in energy efficiency during inference due to the structured nature of the resulting architecture.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Saha2020aOthers_more3" style="display: none">
    N.A.
</span></p>
<script>
    function Saha2020aOthersFunction() {
      var moreText = document.getElementById("Saha2020aOthers_more");
      var moreText2 = document.getElementById("Saha2020aOthers_more2");
      var moreText3 = document.getElementById("Saha2020aOthers_more3");
      var btnText = document.getElementById("Saha2020aOthers_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Saha2020aOthersFunction2() {
      var moreText = document.getElementById("Saha2020aOthers_more2");
      var moreText1 = document.getElementById("Saha2020aOthers_more");
      var moreText3 = document.getElementById("Saha2020aOthers_more3");
      var btnText = document.getElementById("Saha2020aOthers_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Saha2020aOthersFunction3() {
      var moreText = document.getElementById("Saha2020aOthers_more3");
      var moreText1 = document.getElementById("Saha2020aOthers_more");
      var moreText2 = document.getElementById("Saha2020aOthers_more2");
      var btnText = document.getElementById("Saha2020aOthers_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="http://arxiv.org/abs/2005.05080">Continual Learning Using Task Conditional Neural Networks</a> by Honglin Li, Payam Barnaghi, Shirin Enshaeifar and Frieder Ganz. <em>arXiv</em>, 2020. <span style='background-color:DarkCyan; padding: 2px; border-radius:4px; border: 1px solid black;'>[cifar]</span> <span style='background-color:MediumAquaMarine; padding: 2px; border-radius:4px; border: 1px solid black;'>[mnist]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Li2020aOthersFunction()" id="Li2020aOthers_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Li2020aOthersFunction2()" id="Li2020aOthers_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Li2020aOthersFunction3()" id="Li2020aOthers_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Li2020aOthers_more" style="display: none">
    @article{Li2020a,<br> archiveprefix = {arXiv},<br> arxivid = {2005.05080},<br> author = {Li, Honglin and Barnaghi, Payam and Enshaeifar, Shirin and Ganz, Frieder},<br> eprint = {2005.05080},<br> file = {::},<br> journal = {arXiv},<br> keywords = {[cifar],[mnist]},<br> mendeley-tags = {[cifar],[mnist]},<br> month = {may},<br> title = {Continual Learning Using Task Conditional Neural Networks},<br> url = {http://arxiv.org/abs/2005.05080},<br> year = {2020}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Li2020aOthers_more2" style="display: none">
    Conventional deep learning models have limited capacity in learning multiple tasks sequentially. The issue of forgetting the previously learned tasks in continual learning is known as catastrophic forgetting or interference. When the input data or the goal of learning change, a continual model will learn and adapt to the new status. However, the model will not remember or recognise any revisits to the previous states. This causes performance reduction and re-training curves in dealing with periodic or irregularly reoccurring changes in the data or goals. The changes in goals or data are referred to as new tasks in a continual learning model. Most of the continual learning methods have a task-known setup in which the task identities are known in advance to the learning model. We propose Task Conditional Neural Networks (TCNN) that does not require to known the reoccurring tasks in advance. We evaluate our model on standard datasets using MNIST and CIFAR10, and also a real-world dataset that we have collected in a remote healthcare monitoring study (i.e. TIHM dataset). The proposed model outperforms the state-of-the-art solutions in continual learning and adapting to new tasks that are not defined in advance.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Li2020aOthers_more3" style="display: none">
    N.A.
</span></p>
<script>
    function Li2020aOthersFunction() {
      var moreText = document.getElementById("Li2020aOthers_more");
      var moreText2 = document.getElementById("Li2020aOthers_more2");
      var moreText3 = document.getElementById("Li2020aOthers_more3");
      var btnText = document.getElementById("Li2020aOthers_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Li2020aOthersFunction2() {
      var moreText = document.getElementById("Li2020aOthers_more2");
      var moreText1 = document.getElementById("Li2020aOthers_more");
      var moreText3 = document.getElementById("Li2020aOthers_more3");
      var btnText = document.getElementById("Li2020aOthers_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Li2020aOthersFunction3() {
      var moreText = document.getElementById("Li2020aOthers_more3");
      var moreText1 = document.getElementById("Li2020aOthers_more");
      var moreText2 = document.getElementById("Li2020aOthers_more2");
      var btnText = document.getElementById("Li2020aOthers_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p>Class-Incremental Learning Based on Feature Extraction of CNN with Optimized Softmax and One-Class Classifiers by Xin Ye and Qiuyu Zhu. <em>IEEE Access</em>, 42024–42031, 2019. <span style='background-color:DarkCyan; padding: 2px; border-radius:4px; border: 1px solid black;'>[cifar]</span> <span style='background-color:MediumAquaMarine; padding: 2px; border-radius:4px; border: 1px solid black;'>[mnist]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Ye2019aOthersFunction()" id="Ye2019aOthers_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Ye2019aOthersFunction2()" id="Ye2019aOthers_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Ye2019aOthersFunction3()" id="Ye2019aOthers_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Ye2019aOthers_more" style="display: none">
    @article{Ye2019a,<br> author = {Ye, Xin and Zhu, Qiuyu},<br> doi = {10.1109/ACCESS.2019.2904614},<br> file = {::},<br> issn = {21693536},<br> journal = {IEEE Access},<br> keywords = {One-class classifier,[cifar],[mnist],feature extraction,incremental learning,loss function},<br> mendeley-tags = {[cifar],[mnist]},<br> pages = {42024--42031},<br> publisher = {Institute of Electrical and Electronics Engineers Inc.},<br> title = {Class-Incremental Learning Based on Feature Extraction of CNN with Optimized Softmax and One-Class Classifiers},<br> volume = {7},<br> year = {2019}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Ye2019aOthers_more2" style="display: none">
    With the development of deep convolutional neural networks in recent years, the network structure has become more and more complicated and varied, and there are very good results in pattern recognition, image classification, scene classification, and target tracking. This end-to-end learning model relies on the initial large dataset. However, many data are gradually obtained in practical situations, which contradict the deep learning of one-time batch learning. There is an urgent need for an incremental learning approach that can continuously learn new knowledge from new data while retaining what has already been learned. This paper proposes an incremental learning algorithm based on convolutional neural network and support vector data description. CNN and AM-Softmax loss function are used to represent and continuously learn image features. Support vector data description is used to construct multiple hyperspheres for new and old classes of images. Class-incremental learning is achieved by the increment of hyperspheres. The experimental results show that the incremental learning method proposed in this paper can effectively extract the latent features of the image and adapt it to the learning situation of the class-increment. The recognition accuracy is close to batch learning.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Ye2019aOthers_more3" style="display: none">
    N.A.
</span></p>
<script>
    function Ye2019aOthersFunction() {
      var moreText = document.getElementById("Ye2019aOthers_more");
      var moreText2 = document.getElementById("Ye2019aOthers_more2");
      var moreText3 = document.getElementById("Ye2019aOthers_more3");
      var btnText = document.getElementById("Ye2019aOthers_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Ye2019aOthersFunction2() {
      var moreText = document.getElementById("Ye2019aOthers_more2");
      var moreText1 = document.getElementById("Ye2019aOthers_more");
      var moreText3 = document.getElementById("Ye2019aOthers_more3");
      var btnText = document.getElementById("Ye2019aOthers_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Ye2019aOthersFunction3() {
      var moreText = document.getElementById("Ye2019aOthers_more3");
      var moreText1 = document.getElementById("Ye2019aOthers_more");
      var moreText2 = document.getElementById("Ye2019aOthers_more2");
      var btnText = document.getElementById("Ye2019aOthers_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
<li><p><a class="reference external" href="https://marcpickett.com/cl2018/CL-2018%7B%5C_%7Dpaper%7B%5C_%7D56.pdf">Improving and Understanding Variational Continual Learning</a> by Siddharth Swaroop, Cuong V Nguyen, Thang D Bui and Richard E Turner. <em>Continual Learning Workshop NeurIPS</em>, 1–17, 2018. <span style='background-color:Violet; padding: 2px; border-radius:4px; border: 1px solid black;'>[bayes]</span> <span style='background-color:MediumAquaMarine; padding: 2px; border-radius:4px; border: 1px solid black;'>[mnist]</span>  <br>
<button style="font-size:75%; line-height:15px" onclick="Swaroop2018aOthersFunction()" id="Swaroop2018aOthers_btt">Bib</button>
<button style="font-size:75%; line-height:15px" onclick="Swaroop2018aOthersFunction2()" id="Swaroop2018aOthers_btt2">Abstract</button>
<button style="font-size:75%; line-height:15px" onclick="Swaroop2018aOthersFunction3()" id="Swaroop2018aOthers_btt3">Notes</button>

<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Swaroop2018aOthers_more" style="display: none">
    @article{Swaroop2018a,<br> archiveprefix = {arXiv},<br> arxivid = {arXiv:1905.02099v1},<br> author = {Swaroop, Siddharth and Nguyen, Cuong V and Bui, Thang D and Turner, Richard E},<br> eprint = {arXiv:1905.02099v1},<br> journal = {Continual Learning Workshop NeurIPS},<br> keywords = {[bayes],[mnist]},<br> mendeley-tags = {[bayes],[mnist]},<br> pages = {1--17},<br> title = {Improving and Understanding Variational Continual Learning},<br> url = {https://marcpickett.com/cl2018/CL-2018%7B%5C_%7Dpaper%7B%5C_%7D56.pdf},<br> year = {2018}<br>}<br>
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Swaroop2018aOthers_more2" style="display: none">
    In the continual learning setting, tasks are encountered sequentially. The goal is to learn whilst i) avoiding catastrophic forgetting, ii) efficiently using model capacity, and iii) employing forward and backward transfer. In this paper, we explore how the Variational Continual Learning (VCL) framework achieves these desiderata on two benchmarks in continual learning: split MNIST and permuted MNIST. We first report significantly improved results on what was already a competitive approach, achieved by establishing a new best practice approach to mean-field variational Bayesian neural networks. We then look at the solutions in detail. This allows us to obtain an understanding of why VCL performs as it does, and we compare the solution to what an 'ideal' continual learning solution might be.
</span></p>
<p style="background-color: #2980b929; font-size:75%; line-height:15px"><span id="Swaroop2018aOthers_more3" style="display: none">
    N.A.
</span></p>
<script>
    function Swaroop2018aOthersFunction() {
      var moreText = document.getElementById("Swaroop2018aOthers_more");
      var moreText2 = document.getElementById("Swaroop2018aOthers_more2");
      var moreText3 = document.getElementById("Swaroop2018aOthers_more3");
      var btnText = document.getElementById("Swaroop2018aOthers_btt");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Bib";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Bib";
        moreText.style.display = "none";
      }
      moreText2.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Swaroop2018aOthersFunction2() {
      var moreText = document.getElementById("Swaroop2018aOthers_more2");
      var moreText1 = document.getElementById("Swaroop2018aOthers_more");
      var moreText3 = document.getElementById("Swaroop2018aOthers_more3");
      var btnText = document.getElementById("Swaroop2018aOthers_btt2");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Abstract";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText3.style.display = "none";
    }
</script>
<script>
    function Swaroop2018aOthersFunction3() {
      var moreText = document.getElementById("Swaroop2018aOthers_more3");
      var moreText1 = document.getElementById("Swaroop2018aOthers_more");
      var moreText2 = document.getElementById("Swaroop2018aOthers_more2");
      var btnText = document.getElementById("Swaroop2018aOthers_btt3");

      if (moreText.style.display === "none") {
        btnText.innerHTML = "Notes";
        moreText.style.display = "inline";
      } else {
        btnText.innerHTML = "Notes";
        moreText.style.display = "none";
      }
      moreText1.style.display = "none";
      moreText2.style.display = "none";
    }
</script></p></li>
</ul>
</div>
</div>
<div class="section" id="conference-workshops">
<h2>Conference Workshops<a class="headerlink" href="#conference-workshops" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://sites.google.com/view/ll4lhri2020/objectives-and-challenges">Ro-man 2020 Workshop on Lifelong Learning for Long-term Human-Robot Interaction (LL4LHRI)</a></p></li>
<li><p><a class="reference external" href="https://lifelongml.github.io/">ICML 2020 Workshop on Lifelong Learning</a></p></li>
<li><p><a class="reference external" href="https://sites.google.com/view/clvision2020">CVPR 2020 Workshop on Continual Learning in Computer Vision</a></p></li>
<li><p><a class="reference external" href="http://www.cosyne.org/c/index.php?title=Workshops2019_learning">Cosyne 2019 Continual Learning</a></p></li>
<li><p><a class="reference external" href="https://sites.google.com/corp/view/mtlrl/home">ICML 2019 Workshop on Multi-Task and Lifelong Reinforcement Learning</a></p></li>
<li><p><a class="reference external" href="https://www.amtl-workshop.org/">ICML 2019 Adaptive and Multitask Learning: Algorithms &amp; Systems</a></p></li>
<li><p><a class="reference external" href="https://sites.google.com/corp/view/llarla2018/home">ICML 2018 Lifelong RL workshop</a></p></li>
<li><p><a class="reference external" href="http://metalearning.ml/2018/">NeurIPS 2018 Workshop on Meta-Learning</a></p></li>
<li><p><a class="reference external" href="https://sites.google.com/view/continual2018/home">NeurIPS 2018 Workshop on Continual Learning</a></p></li>
</ul>
</div>
<div class="section" id="research-programs">
<h2>Research Programs<a class="headerlink" href="#research-programs" title="Permalink to this headline">¶</a></h2>
<p>In this section we keep track of all the current and past projects on
Lifelong/Continual Learning.:</p>
<ul class="simple">
<li><p><a class="reference external" href="http://www.darpa.mil/news-events/2017-03-16">DARPA Lifelong Learning Machines (L2M) program</a></p></li>
<li><p><a class="reference external" href="http://www.robotsthatdream.eu/">European H2020 DREAM project</a></p></li>
</ul>
<script>
    function keyword_filter() {
      // Declare variables
      var input, filter, ul, li, a, i, txtValue;
      input = document.getElementById('myInput');
      filter = input.value.toUpperCase();
      sec = document.getElementById("publications");
      li = sec.getElementsByTagName('li');

      // Loop through all list items, and hide those who don't match the search query
      for (i = 0; i < li.length; i++) {
        txtValue = li[i].textContent || li[i].innerText;
        if (txtValue.toUpperCase().indexOf(filter) > -1) {
          li[i].style.display = "";
        } else {
          li[i].style.display = "none";
        }
      }
    }
</script>

<script>
    function regex_filter() {
      // Declare variables
      var input, filter, ul, li, a, i, txtValue;
      input = document.getElementById('myInputreg');
      // filter = input.value.toUpperCase();
      filter = input.value;
      sec = document.getElementById("publications");
      li = sec.getElementsByTagName('li');

      // Loop through all list items, and hide those who don't match the search query
      for (i = 0; i < li.length; i++) {
        txtValue = li[i].textContent || li[i].innerText;
        if (txtValue.match(filter)) {
          li[i].style.display = "";
        } else {
          li[i].style.display = "none";
        }
      }
    }
</script></div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="industry.html" class="btn btn-neutral float-right" title="Industry" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="introduction.html" class="btn btn-neutral float-left" title="Introduction" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2018-2021, ContinualAI

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-165355374-1', 'auto');
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>